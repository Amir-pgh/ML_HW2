{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ChC3RF8meAlK"
      },
      "source": [
        "<h1 align=\"center\">Introduction to Machine Learning - 25737-2</h1>\n",
        "<h4 align=\"center\">Dr. R. Amiri</h4>\n",
        "<h4 align=\"center\">Sharif University of Technology, Spring 2024</h4>\n",
        "\n",
        "\n",
        "**<font color='red'>Plagiarism is strongly prohibited!</font>**\n",
        "\n",
        "\n",
        "**Student Name**: Amirali Pourdehghan\n",
        "\n",
        "**Student ID**: 400107553\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IraiR0SbeDi_"
      },
      "source": [
        "# Logistic Regression"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nRQjwWC3eDnc"
      },
      "source": [
        "**Task:** Implement your own Logistic Regression model, and test it on the given dataset of Logistic_question.csv!"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Write your code here ##\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ClQ_M9qOKDub",
        "outputId": "6a12378b-b822-4cf7-c695-cf0cf0321c5a"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "id": "NXNNWAdTKQsj",
        "outputId": "05abf8b5-2ab8-4685-c7bc-765580941caa"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-7df198a7-2f7e-4dcd-85d1-0ff13d2db25e\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-7df198a7-2f7e-4dcd-85d1-0ff13d2db25e\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving Logistic_question.csv to Logistic_question.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "shutil.move(\"Logistic_question.csv\", \"/content/drive/My Drive/Logistic_question.csv\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "IPP0kma-KYB4",
        "outputId": "d583cd69-8b04-475d-ea0b-141a02c11eec"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/drive/My Drive/Logistic_question.csv'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the dataset from the CSV file\n",
        "Logistic_question_data = pd.read_csv(\"/content/drive/My Drive/Logistic_question.csv\")\n",
        "\n",
        "# Display the first few rows of the dataset to understand its structure and features\n",
        "print(\"First few rows of the Logistic_question dataset:\")\n",
        "print(Logistic_question_data.head())\n",
        "\n",
        "# Display the shape of the dataset\n",
        "print(\"\\nShape of the Logistic_question dataset:\")\n",
        "print(Logistic_question_data.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nDHAuPEqKjgX",
        "outputId": "6c3e58af-837b-44ef-fefa-76544db29ff5"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First few rows of the Logistic_question dataset:\n",
            "   Feature 1  Feature 2  Feature 3  Feature 4  Feature 5  Feature 6  \\\n",
            "0         47       11.8          4        4.5        4.5       9.65   \n",
            "1         34       10.7          4        4.0        4.5       8.87   \n",
            "2         26       10.4          3        3.0        3.5       8.00   \n",
            "3         32       11.0          3        3.5        2.5       8.67   \n",
            "4         24       10.3          2        2.0        3.0       8.21   \n",
            "\n",
            "   Feature 7  Target  \n",
            "0          1    0.92  \n",
            "1          1    0.76  \n",
            "2          1    0.72  \n",
            "3          1    0.80  \n",
            "4          0    0.65  \n",
            "\n",
            "Shape of the Logistic_question dataset:\n",
            "(400, 8)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "class MyLogisticRegression:\n",
        "    def __init__(self, input_dim):\n",
        "        self.model = nn.Sequential(\n",
        "            nn.Linear(input_dim, 1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def loss_fn(self, y_pred, y_true):\n",
        "        return nn.BCELoss()(y_pred, y_true)\n",
        "\n",
        "    def fit(self, X_train, y_train, epochs=100, lr=0.01):\n",
        "        optimizer = optim.SGD(self.model.parameters(), lr=lr)\n",
        "\n",
        "        for epoch in range(epochs):\n",
        "            optimizer.zero_grad()\n",
        "            y_pred = self.model(X_train)\n",
        "            loss = self.loss_fn(y_pred, y_train)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            if epoch % 10 == 0:\n",
        "                print(f\"Epoch {epoch+1}/{epochs}, Loss: {loss.item()}\")\n",
        "\n",
        "    def predict(self, X):\n",
        "        with torch.no_grad():\n",
        "            self.model.eval()\n",
        "            y_pred = self.model(X)\n",
        "            return (y_pred >= 0.5).float()\n",
        "\n",
        "# Load dataset\n",
        "data = pd.read_csv(\"/content/drive/My Drive/Logistic_question.csv\")\n",
        "\n",
        "# Preprocessing\n",
        "X = data.iloc[:, :-1].values\n",
        "y = data.iloc[:, -1].values.reshape(-1, 1)\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X = scaler.fit_transform(X)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Convert to PyTorch tensors\n",
        "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
        "y_train_tensor = torch.tensor(y_train, dtype=torch.float32)\n",
        "\n",
        "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
        "\n",
        "# Initialize and train the model\n",
        "model = MyLogisticRegression(input_dim=X_train.shape[1])\n",
        "model.fit(X_train_tensor, y_train_tensor)\n",
        "\n",
        "# Predictions\n",
        "y_pred = model.predict(X_test_tensor)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zcOKM-evtV94",
        "outputId": "b4fb8570-573d-4261-cfac-4f966cfcead2"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100, Loss: 0.7577596306800842\n",
            "Epoch 11/100, Loss: 0.7480853796005249\n",
            "Epoch 21/100, Loss: 0.7389706373214722\n",
            "Epoch 31/100, Loss: 0.7303656339645386\n",
            "Epoch 41/100, Loss: 0.7222272753715515\n",
            "Epoch 51/100, Loss: 0.7145188450813293\n",
            "Epoch 61/100, Loss: 0.707207977771759\n",
            "Epoch 71/100, Loss: 0.7002667784690857\n",
            "Epoch 81/100, Loss: 0.693670392036438\n",
            "Epoch 91/100, Loss: 0.6873966455459595\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S-i-oubUlZ6e"
      },
      "source": [
        "**Task:** Test your model on the given dataset. You must split your data into train and test, with a 0.2 split, then normalize your data using X_train data. Finally, report 4 different evaluation metrics of the model on the test set. (You might want to first make the Target column binary!)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0KXzIy_2u-pG",
        "outputId": "76b76887-4129-4316-c987-3770208b0049"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100, Loss: 0.6703535318374634\n",
            "Epoch 11/100, Loss: 0.6551795601844788\n",
            "Epoch 21/100, Loss: 0.6407490372657776\n",
            "Epoch 31/100, Loss: 0.6270216703414917\n",
            "Epoch 41/100, Loss: 0.6139588952064514\n",
            "Epoch 51/100, Loss: 0.6015231609344482\n",
            "Epoch 61/100, Loss: 0.5896787047386169\n",
            "Epoch 71/100, Loss: 0.5783915519714355\n",
            "Epoch 81/100, Loss: 0.5676292777061462\n",
            "Epoch 91/100, Loss: 0.5573612451553345\n",
            "Evaluation Metrics:\n",
            "Accuracy: 0.8705\n",
            "Precision: 0.9602704987320372\n",
            "Recall: 0.8427299703264095\n",
            "F1-score: 0.8976689055709206\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "class MyLogisticRegression:\n",
        "    def __init__(self, input_dim):\n",
        "        self.model = nn.Sequential(\n",
        "            nn.Linear(input_dim, 1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def loss_fn(self, y_pred, y_true):\n",
        "        return nn.BCELoss()(y_pred, y_true)\n",
        "\n",
        "    def fit(self, X_train, y_train, epochs=100, lr=0.01):\n",
        "        optimizer = optim.SGD(self.model.parameters(), lr=lr)\n",
        "\n",
        "        for epoch in range(epochs):\n",
        "            optimizer.zero_grad()\n",
        "            y_pred = self.model(X_train).squeeze(1)  # Ensure predictions are a column vector\n",
        "            loss = self.loss_fn(y_pred, y_train)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            if epoch % 10 == 0:\n",
        "                print(f\"Epoch {epoch+1}/{epochs}, Loss: {loss.item()}\")\n",
        "\n",
        "    def predict(self, X):\n",
        "        with torch.no_grad():\n",
        "            self.model.eval()\n",
        "            y_pred = self.model(X).squeeze(1)  # Ensure predictions are a column vector\n",
        "            return (y_pred >= 0.5).float()\n",
        "\n",
        "# Load dataset\n",
        "data = pd.read_csv(\"/content/drive/My Drive/data_logistic.csv\")\n",
        "\n",
        "# Preprocessing\n",
        "X = data.iloc[:, :-1].values\n",
        "y = data.iloc[:, -1].values\n",
        "\n",
        "# Convert target column to binary\n",
        "y_binary = (y > 0).astype(int)\n",
        "\n",
        "# Splitting the data into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y_binary, test_size=0.2, random_state=42)\n",
        "\n",
        "# Normalize the features using the mean and standard deviation of the training set\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# Convert to PyTorch tensors\n",
        "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
        "y_train_tensor = torch.tensor(y_train, dtype=torch.float32).flatten()  # Flatten the target tensor\n",
        "\n",
        "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
        "\n",
        "# Initialize and train the model\n",
        "model = MyLogisticRegression(input_dim=X_train.shape[1])\n",
        "model.fit(X_train_tensor, y_train_tensor)\n",
        "\n",
        "# Predictions\n",
        "y_pred = model.predict(X_test_tensor)\n",
        "\n",
        "# Evaluation\n",
        "accuracy = accuracy_score(y_test, y_pred.numpy())\n",
        "precision = precision_score(y_test, y_pred.numpy())\n",
        "recall = recall_score(y_test, y_pred.numpy())\n",
        "f1 = f1_score(y_test, y_pred.numpy())\n",
        "\n",
        "print(\"Evaluation Metrics:\")\n",
        "print(f\"Accuracy: {accuracy}\")\n",
        "print(f\"Precision: {precision}\")\n",
        "print(f\"Recall: {recall}\")\n",
        "print(f\"F1-score: {f1}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ji0RXNGKv1pa"
      },
      "source": [
        "**Question:** What are each of your used evaluation metrics? And for each one, mention situations in which they convey more data on the model performance in specific tasks."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ldveD35twRRZ"
      },
      "source": [
        "**Your answer:**\n",
        "Sure, let's discuss each of the evaluation metrics used in this context:\n",
        "\n",
        "1. **Accuracy**: Accuracy measures the proportion of correctly classified instances out of the total instances. It is calculated as (TP + TN) / (TP + TN + FP + FN), where TP is the number of true positives, TN is the number of true negatives, FP is the number of false positives, and FN is the number of false negatives. Accuracy provides an overall assessment of the model's performance but can be misleading if the classes are imbalanced.\n",
        "\n",
        "2. **Precision**: Precision measures the proportion of true positive predictions among all positive predictions made by the model. It is calculated as TP / (TP + FP). Precision is useful in tasks where the cost of false positives is high, such as medical diagnosis or spam detection. A high precision indicates that the model is making fewer false positive errors.\n",
        "\n",
        "3. **Recall (Sensitivity)**: Recall measures the proportion of true positive predictions among all actual positive instances in the data. It is calculated as TP / (TP + FN). Recall is important in tasks where missing positive instances is costly, such as disease detection or fault detection. A high recall indicates that the model is capturing a large proportion of positive instances.\n",
        "\n",
        "4. **F1-score**: The F1-score is the harmonic mean of precision and recall, combining both measures into a single value. It is calculated as 2 * (precision * recall) / (precision + recall). The F1-score provides a balance between precision and recall and is useful when there is an uneven class distribution or when false positives and false negatives have different costs. It is particularly informative in binary classification tasks where classes are imbalanced.\n",
        "\n",
        "In summary:\n",
        "- **Accuracy** gives an overall view of the model's performance but can be misleading in imbalanced datasets.\n",
        "- **Precision** is valuable when minimizing false positives is important.\n",
        "- **Recall** is valuable when minimizing false negatives is important.\n",
        "- **F1-score** provides a balanced measure of precision and recall, suitable for imbalanced datasets or when both false positives and false negatives are important."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1ZCeRHZSw-mh"
      },
      "source": [
        "**Task:** Now test the built-in function of Python for Logistic Regression, and report all the same metrics used before."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "Vb5lRSQXDLR3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b4c3e956-f24f-4b44-efaf-22c856d14f36"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluation Metrics for Built-in Logistic Regression:\n",
            "Accuracy: 0.9995\n",
            "Precision: 0.9992587101556709\n",
            "Recall: 1.0\n",
            "F1-score: 0.99962921764924\n"
          ]
        }
      ],
      "source": [
        "# Your code goes here!\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "# Load dataset\n",
        "\n",
        "# Preprocessing\n",
        "X = data.iloc[:, :-1].values\n",
        "y = data.iloc[:, -1].values\n",
        "\n",
        "# Convert target column to binary\n",
        "y_binary = (y > 0).astype(int)\n",
        "\n",
        "# Splitting the data into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y_binary, test_size=0.2, random_state=42)\n",
        "\n",
        "# Normalize the features using the mean and standard deviation of the training set\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# Initialize and train the logistic regression model\n",
        "model = LogisticRegression()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predictions\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Evaluation\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "precision = precision_score(y_test, y_pred)\n",
        "recall = recall_score(y_test, y_pred)\n",
        "f1 = f1_score(y_test, y_pred)\n",
        "\n",
        "print(\"Evaluation Metrics for Built-in Logistic Regression:\")\n",
        "print(f\"Accuracy: {accuracy}\")\n",
        "print(f\"Precision: {precision}\")\n",
        "print(f\"Recall: {recall}\")\n",
        "print(f\"F1-score: {f1}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "class MyLogisticRegression:\n",
        "    def __init__(self, input_dim):\n",
        "        self.model = nn.Sequential(\n",
        "            nn.Linear(input_dim, 1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def loss_fn(self, y_pred, y_true):\n",
        "        return nn.BCELoss()(y_pred, y_true)\n",
        "\n",
        "    def fit(self, X_train, y_train, epochs=1000, lr=0.1):\n",
        "        optimizer = optim.Adam(self.model.parameters(), lr=lr)\n",
        "\n",
        "        for epoch in range(epochs):\n",
        "            optimizer.zero_grad()\n",
        "            y_pred = self.model(X_train).squeeze(1)  # Ensure predictions are a column vector\n",
        "            loss = self.loss_fn(y_pred, y_train)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            if epoch % 100 == 0:\n",
        "                print(f\"Epoch {epoch+1}/{epochs}, Loss: {loss.item()}\")\n",
        "\n",
        "    def predict(self, X):\n",
        "        with torch.no_grad():\n",
        "            self.model.eval()\n",
        "            y_pred = self.model(X).squeeze(1)  # Ensure predictions are a column vector\n",
        "            return (y_pred >= 0.5).float()\n",
        "\n",
        "# Load dataset\n",
        "\n",
        "# Preprocessing\n",
        "X = data.iloc[:, :-1].values\n",
        "y = data.iloc[:, -1].values\n",
        "\n",
        "# Convert target column to binary\n",
        "y_binary = (y > 0).astype(int)\n",
        "\n",
        "# Splitting the data into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y_binary, test_size=0.2, random_state=42)\n",
        "\n",
        "# Normalize the features using the mean and standard deviation of the training set\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# Convert to PyTorch tensors\n",
        "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
        "y_train_tensor = torch.tensor(y_train, dtype=torch.float32).flatten()  # Flatten the target tensor\n",
        "\n",
        "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
        "\n",
        "# Initialize and train the model\n",
        "model = MyLogisticRegression(input_dim=X_train.shape[1])\n",
        "model.fit(X_train_tensor, y_train_tensor)\n",
        "\n",
        "# Predictions\n",
        "y_pred = model.predict(X_test_tensor)\n",
        "\n",
        "# Evaluation\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "precision = precision_score(y_test, y_pred)\n",
        "recall = recall_score(y_test, y_pred)\n",
        "f1 = f1_score(y_test, y_pred)\n",
        "\n",
        "print(\"Evaluation Metrics for Custom Logistic Regression:\")\n",
        "print(f\"Accuracy: {accuracy}\")\n",
        "print(f\"Precision: {precision}\")\n",
        "print(f\"Recall: {recall}\")\n",
        "print(f\"F1-score: {f1}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GsdHc34McoTD",
        "outputId": "ea0ef858-320c-4a88-d9a0-d98d8da78325"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/1000, Loss: 0.7286030650138855\n",
            "Epoch 101/1000, Loss: 0.0970899909734726\n",
            "Epoch 201/1000, Loss: 0.0714481920003891\n",
            "Epoch 301/1000, Loss: 0.05899488925933838\n",
            "Epoch 401/1000, Loss: 0.05124291777610779\n",
            "Epoch 501/1000, Loss: 0.04580654576420784\n",
            "Epoch 601/1000, Loss: 0.04171392694115639\n",
            "Epoch 701/1000, Loss: 0.03848390281200409\n",
            "Epoch 801/1000, Loss: 0.03584703430533409\n",
            "Epoch 901/1000, Loss: 0.033638909459114075\n",
            "Evaluation Metrics for Custom Logistic Regression:\n",
            "Accuracy: 0.999\n",
            "Precision: 0.9992581602373887\n",
            "Recall: 0.9992581602373887\n",
            "F1-score: 0.9992581602373887\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RCvIymmMy_ji"
      },
      "source": [
        "**Question:** Compare your function with the built-in function. On the matters of performance and parameters. Briefly explain what the parameters of the built-in function are and how they affect the model's performance?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EY0ohM16z3De"
      },
      "source": [
        "**Your answer:**\n",
        "Comparing the custom logistic regression function with the built-in logistic regression function reveals differences in terms of performance and parameters.\n",
        "\n",
        "**Performance:**\n",
        "- Both implementations yield comparable performance in terms of evaluation metrics such as accuracy, precision, recall, and F1-score.\n",
        "- The performance of the custom logistic regression function heavily depends on the implementation details, such as the choice of optimization algorithm, learning rate, and number of epochs.\n",
        "- The built-in logistic regression function (from `sklearn.linear_model.LogisticRegression`) is optimized and extensively tested, offering a reliable and efficient solution out of the box.\n",
        "\n",
        "**Parameters of Built-in Logistic Regression Function:**\n",
        "- The `LogisticRegression` class in scikit-learn offers various parameters to fine-tune the model:\n",
        "  - `penalty`: Specifies the norm used in the penalization (e.g., L1 or L2 regularization).\n",
        "  - `C`: Inverse of regularization strength; smaller values specify stronger regularization.\n",
        "  - `solver`: Algorithm to use in the optimization problem (e.g., 'liblinear', 'lbfgs', 'sag', 'saga').\n",
        "  - `max_iter`: Maximum number of iterations taken for the solver to converge.\n",
        "  - `tol`: Tolerance for stopping criteria.\n",
        "  - `class_weight`: Weights associated with classes in the form `{class_label: weight}` to address class imbalance.\n",
        "  - `random_state`: Seed for random number generation to ensure reproducibility.\n",
        "\n",
        "**Effect of Parameters on Model's Performance:**\n",
        "- `penalty` and `C`: Control the regularization strength. Higher `C` values result in less regularization, potentially leading to overfitting.\n",
        "- `solver` and `max_iter`: Influence the optimization algorithm and the number of iterations required for convergence. Certain solvers may perform better or worse depending on the dataset size and characteristics.\n",
        "- `class_weight`: Can be used to address class imbalance by assigning different weights to different classes.\n",
        "- `random_state`: Ensures reproducibility of results by fixing the seed for random number generation.\n",
        "\n",
        "In summary, while the custom implementation offers flexibility and control over the logistic regression model, the built-in function provides a more convenient and optimized solution with parameters that can be adjusted to tailor the model's behavior to specific datasets and tasks."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ClMqoYlr2kr7"
      },
      "source": [
        "# Multinomial Logistic Regression"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ukvlqDe52xP5"
      },
      "source": [
        "**Task:** Implement your own Multinomial Logistic Regression model. Your model must be able to handle any number of labels!"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "class MyMultinomialLogisticRegression:\n",
        "    def __init__(self, input_dim, output_dim):\n",
        "        self.model = nn.Sequential(\n",
        "            nn.Linear(input_dim, output_dim)\n",
        "        )\n",
        "\n",
        "    def loss_fn(self, y_pred, y_true):\n",
        "        return nn.CrossEntropyLoss()(y_pred, y_true)\n",
        "\n",
        "    def fit(self, X_train, y_train, epochs=100, lr=0.01):\n",
        "        optimizer = optim.SGD(self.model.parameters(), lr=lr)\n",
        "\n",
        "        for epoch in range(epochs):\n",
        "            optimizer.zero_grad()\n",
        "            y_pred = self.model(X_train)\n",
        "            loss = self.loss_fn(y_pred, y_train)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            if epoch % 10 == 0:\n",
        "                print(f\"Epoch {epoch+1}/{epochs}, Loss: {loss.item()}\")\n",
        "\n",
        "    def predict(self, X):\n",
        "        with torch.no_grad():\n",
        "            self.model.eval()\n",
        "            _, predicted = torch.max(self.model(X), 1)\n",
        "            return predicted\n",
        "\n",
        "# Load dataset\n",
        "\n",
        "# Preprocessing\n",
        "X = data.iloc[:, :-1].values\n",
        "y = data.iloc[:, -1].values\n",
        "\n",
        "# Convert target to tensor\n",
        "y_tensor = torch.tensor(y, dtype=torch.long)\n",
        "\n",
        "# Splitting the data into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y_tensor, test_size=0.2, random_state=42)\n",
        "\n",
        "# Normalize the features using the mean and standard deviation of the training set\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# Convert to PyTorch tensors\n",
        "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
        "y_train_tensor = y_train\n",
        "\n",
        "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
        "\n",
        "# Initialize and train the model\n",
        "input_dim = X_train.shape[1]\n",
        "output_dim = len(torch.unique(y_train))\n",
        "model = MyMultinomialLogisticRegression(input_dim, output_dim)\n",
        "model.fit(X_train_tensor, y_train_tensor)\n",
        "\n",
        "# Predictions\n",
        "y_pred = model.predict(X_test_tensor)\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(y_test.numpy(), y_pred.numpy())\n",
        "print(f\"Accuracy: {accuracy}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z2i3RfE-c-Uf",
        "outputId": "651bc3a4-eb6b-4642-cb2a-d30b7da5338e"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100, Loss: 0.8143534064292908\n",
            "Epoch 11/100, Loss: 0.7715678811073303\n",
            "Epoch 21/100, Loss: 0.7326731085777283\n",
            "Epoch 31/100, Loss: 0.6973704099655151\n",
            "Epoch 41/100, Loss: 0.6653509140014648\n",
            "Epoch 51/100, Loss: 0.6363070011138916\n",
            "Epoch 61/100, Loss: 0.6099427938461304\n",
            "Epoch 71/100, Loss: 0.5859805345535278\n",
            "Epoch 81/100, Loss: 0.5641641616821289\n",
            "Epoch 91/100, Loss: 0.5442614555358887\n",
            "Accuracy: 0.8095\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zPQ3Rtay3Y2_"
      },
      "source": [
        "**Task:** Test your model on the given dataset. Do the same as the previous part, but here you might want to first make the Target column quantized into $i$ levels. Change $i$ from 2 to 10."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "class MyMultinomialLogisticRegression:\n",
        "    def __init__(self, input_dim, output_dim):\n",
        "        self.model = nn.Sequential(\n",
        "            nn.Linear(input_dim, output_dim)\n",
        "        )\n",
        "\n",
        "    def loss_fn(self, y_pred, y_true):\n",
        "        return nn.CrossEntropyLoss()(y_pred, y_true)\n",
        "\n",
        "    def fit(self, X_train, y_train, epochs=100, lr=0.01):\n",
        "        optimizer = optim.SGD(self.model.parameters(), lr=lr)\n",
        "\n",
        "        for epoch in range(epochs):\n",
        "            optimizer.zero_grad()\n",
        "            y_pred = self.model(X_train)\n",
        "            loss = self.loss_fn(y_pred, y_train)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            if epoch % 10 == 0:\n",
        "                print(f\"Epoch {epoch+1}/{epochs}, Loss: {loss.item()}\")\n",
        "\n",
        "    def predict(self, X):\n",
        "        with torch.no_grad():\n",
        "            self.model.eval()\n",
        "            _, predicted = torch.max(self.model(X), 1)\n",
        "            return predicted\n",
        "\n",
        "# Load dataset\n",
        "# Load dataset\n",
        "\n",
        "# Assuming the target column is the last column in the dataset\n",
        "target_column = data.columns[-1]\n",
        "\n",
        "for i in range(2, 11):\n",
        "    print(f\"Quantizing target column into {i} levels...\")\n",
        "    quantized_target = pd.qcut(data[target_column], i, labels=False, duplicates='drop')  # Adjusted to handle duplicate bin edges\n",
        "    X = data.iloc[:, :-1].values\n",
        "    y = quantized_target.values\n",
        "\n",
        "    # Splitting the data into train and test sets\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "    # Normalize the features using the mean and standard deviation of the training set\n",
        "    scaler = StandardScaler()\n",
        "    X_train = scaler.fit_transform(X_train)\n",
        "    X_test = scaler.transform(X_test)\n",
        "\n",
        "    # Convert to PyTorch tensors\n",
        "    X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
        "    y_train_tensor = torch.tensor(y_train, dtype=torch.long)\n",
        "\n",
        "    X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
        "\n",
        "    # Initialize and train the model\n",
        "    input_dim = X_train.shape[1]\n",
        "    output_dim = i\n",
        "    model = MyMultinomialLogisticRegression(input_dim, output_dim)\n",
        "    model.fit(X_train_tensor, y_train_tensor)\n",
        "\n",
        "    # Predictions\n",
        "    y_pred = model.predict(X_test_tensor)\n",
        "\n",
        "    # Calculate accuracy\n",
        "    accuracy = accuracy_score(y_test, y_pred.numpy())\n",
        "    print(f\"Accuracy for {i} levels: {accuracy}\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3D2KAoNdfErZ",
        "outputId": "6ad52e07-5f4b-4988-923b-f010e64c52f1"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Quantizing target column into 2 levels...\n",
            "Epoch 1/100, Loss: 0.8510392308235168\n",
            "Epoch 11/100, Loss: 0.789694607257843\n",
            "Epoch 21/100, Loss: 0.7335090041160583\n",
            "Epoch 31/100, Loss: 0.6821690201759338\n",
            "Epoch 41/100, Loss: 0.635343611240387\n",
            "Epoch 51/100, Loss: 0.5926943421363831\n",
            "Epoch 61/100, Loss: 0.5538826584815979\n",
            "Epoch 71/100, Loss: 0.5185773372650146\n",
            "Epoch 81/100, Loss: 0.4864601492881775\n",
            "Epoch 91/100, Loss: 0.4572303593158722\n",
            "Accuracy for 2 levels: 0.9345\n",
            "Quantizing target column into 3 levels...\n",
            "Epoch 1/100, Loss: 1.1595712900161743\n",
            "Epoch 11/100, Loss: 1.0904912948608398\n",
            "Epoch 21/100, Loss: 1.0255450010299683\n",
            "Epoch 31/100, Loss: 0.9646261930465698\n",
            "Epoch 41/100, Loss: 0.9076095819473267\n",
            "Epoch 51/100, Loss: 0.8543539643287659\n",
            "Epoch 61/100, Loss: 0.804702877998352\n",
            "Epoch 71/100, Loss: 0.7584881782531738\n",
            "Epoch 81/100, Loss: 0.7155327200889587\n",
            "Epoch 91/100, Loss: 0.6756531596183777\n",
            "Accuracy for 3 levels: 0.797\n",
            "Quantizing target column into 4 levels...\n",
            "Epoch 1/100, Loss: 1.476307988166809\n",
            "Epoch 11/100, Loss: 1.4019346237182617\n",
            "Epoch 21/100, Loss: 1.3308600187301636\n",
            "Epoch 31/100, Loss: 1.2630555629730225\n",
            "Epoch 41/100, Loss: 1.1984816789627075\n",
            "Epoch 51/100, Loss: 1.1370892524719238\n",
            "Epoch 61/100, Loss: 1.0788185596466064\n",
            "Epoch 71/100, Loss: 1.0236009359359741\n",
            "Epoch 81/100, Loss: 0.9713574647903442\n",
            "Epoch 91/100, Loss: 0.9220010042190552\n",
            "Accuracy for 4 levels: 0.8445\n",
            "Quantizing target column into 5 levels...\n",
            "Epoch 1/100, Loss: 1.3901985883712769\n",
            "Epoch 11/100, Loss: 1.3222618103027344\n",
            "Epoch 21/100, Loss: 1.2573513984680176\n",
            "Epoch 31/100, Loss: 1.1954363584518433\n",
            "Epoch 41/100, Loss: 1.1364773511886597\n",
            "Epoch 51/100, Loss: 1.0804235935211182\n",
            "Epoch 61/100, Loss: 1.0272154808044434\n",
            "Epoch 71/100, Loss: 0.9767840504646301\n",
            "Epoch 81/100, Loss: 0.9290521144866943\n",
            "Epoch 91/100, Loss: 0.8839344382286072\n",
            "Accuracy for 5 levels: 0.847\n",
            "Quantizing target column into 6 levels...\n",
            "Epoch 1/100, Loss: 2.062035083770752\n",
            "Epoch 11/100, Loss: 1.9685146808624268\n",
            "Epoch 21/100, Loss: 1.8778597116470337\n",
            "Epoch 31/100, Loss: 1.790175199508667\n",
            "Epoch 41/100, Loss: 1.705552577972412\n",
            "Epoch 51/100, Loss: 1.6240674257278442\n",
            "Epoch 61/100, Loss: 1.545778751373291\n",
            "Epoch 71/100, Loss: 1.470727801322937\n",
            "Epoch 81/100, Loss: 1.3989354372024536\n",
            "Epoch 91/100, Loss: 1.3304046392440796\n",
            "Accuracy for 6 levels: 0.684\n",
            "Quantizing target column into 7 levels...\n",
            "Epoch 1/100, Loss: 1.8447223901748657\n",
            "Epoch 11/100, Loss: 1.7656077146530151\n",
            "Epoch 21/100, Loss: 1.6890149116516113\n",
            "Epoch 31/100, Loss: 1.6149780750274658\n",
            "Epoch 41/100, Loss: 1.543522834777832\n",
            "Epoch 51/100, Loss: 1.4746662378311157\n",
            "Epoch 61/100, Loss: 1.4084179401397705\n",
            "Epoch 71/100, Loss: 1.344778299331665\n",
            "Epoch 81/100, Loss: 1.283738374710083\n",
            "Epoch 91/100, Loss: 1.2252815961837769\n",
            "Accuracy for 7 levels: 0.8095\n",
            "Quantizing target column into 8 levels...\n",
            "Epoch 1/100, Loss: 2.181884527206421\n",
            "Epoch 11/100, Loss: 2.09224009513855\n",
            "Epoch 21/100, Loss: 2.005018949508667\n",
            "Epoch 31/100, Loss: 1.9202873706817627\n",
            "Epoch 41/100, Loss: 1.8381046056747437\n",
            "Epoch 51/100, Loss: 1.758520483970642\n",
            "Epoch 61/100, Loss: 1.681576132774353\n",
            "Epoch 71/100, Loss: 1.6073037385940552\n",
            "Epoch 81/100, Loss: 1.5357249975204468\n",
            "Epoch 91/100, Loss: 1.466852068901062\n",
            "Accuracy for 8 levels: 0.6285\n",
            "Quantizing target column into 9 levels...\n",
            "Epoch 1/100, Loss: 2.481912612915039\n",
            "Epoch 11/100, Loss: 2.3851702213287354\n",
            "Epoch 21/100, Loss: 2.2904844284057617\n",
            "Epoch 31/100, Loss: 2.1979551315307617\n",
            "Epoch 41/100, Loss: 2.1076791286468506\n",
            "Epoch 51/100, Loss: 2.0197484493255615\n",
            "Epoch 61/100, Loss: 1.9342482089996338\n",
            "Epoch 71/100, Loss: 1.8512576818466187\n",
            "Epoch 81/100, Loss: 1.7708461284637451\n",
            "Epoch 91/100, Loss: 1.693074107170105\n",
            "Accuracy for 9 levels: 0.465\n",
            "Quantizing target column into 10 levels...\n",
            "Epoch 1/100, Loss: 2.1753077507019043\n",
            "Epoch 11/100, Loss: 2.0895307064056396\n",
            "Epoch 21/100, Loss: 2.005917549133301\n",
            "Epoch 31/100, Loss: 1.9245283603668213\n",
            "Epoch 41/100, Loss: 1.845417857170105\n",
            "Epoch 51/100, Loss: 1.7686351537704468\n",
            "Epoch 61/100, Loss: 1.6942230463027954\n",
            "Epoch 71/100, Loss: 1.6222171783447266\n",
            "Epoch 81/100, Loss: 1.5526448488235474\n",
            "Epoch 91/100, Loss: 1.4855265617370605\n",
            "Accuracy for 10 levels: 0.626\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load dataset\n",
        "\n",
        "# Assuming the target column is the last column in the dataset\n",
        "target_column = data.columns[-1]\n",
        "\n",
        "for i in range(2, 11):\n",
        "    print(f\"Quantizing target column into {i} levels...\")\n",
        "    quantized_target = pd.qcut(data[target_column], i, labels=False, duplicates='drop')  # Adjusted to handle duplicate bin edges\n",
        "    X = data.iloc[:, :-1].values\n",
        "    y = quantized_target.values\n",
        "\n",
        "    # Splitting the data into train and test sets\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "    # Normalize the features using the mean and standard deviation of the training set\n",
        "    scaler = StandardScaler()\n",
        "    X_train = scaler.fit_transform(X_train)\n",
        "    X_test = scaler.transform(X_test)\n",
        "\n",
        "    # Convert to PyTorch tensors\n",
        "    X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
        "    y_train_tensor = torch.tensor(y_train, dtype=torch.long)\n",
        "\n",
        "    X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
        "\n",
        "    # Initialize and train the model\n",
        "    input_dim = X_train.shape[1]\n",
        "    output_dim = i\n",
        "    model = MyMultinomialLogisticRegression(input_dim, output_dim)\n",
        "    model.fit(X_train_tensor, y_train_tensor, epochs=200)  # Increased epochs to 1000\n",
        "\n",
        "    # Predictions\n",
        "    y_pred = model.predict(X_test_tensor)\n",
        "\n",
        "    # Calculate accuracy\n",
        "    accuracy = accuracy_score(y_test, y_pred.numpy())\n",
        "    print(f\"Accuracy for {i} levels: {accuracy}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O4OoGiXSfouQ",
        "outputId": "696b90d8-73b4-4628-a938-c7ed52da6ffd"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Quantizing target column into 2 levels...\n",
            "Epoch 1/200, Loss: 0.4394930303096771\n",
            "Epoch 11/200, Loss: 0.41442081332206726\n",
            "Epoch 21/200, Loss: 0.39153745770454407\n",
            "Epoch 31/200, Loss: 0.37062183022499084\n",
            "Epoch 41/200, Loss: 0.3514743447303772\n",
            "Epoch 51/200, Loss: 0.33391496539115906\n",
            "Epoch 61/200, Loss: 0.3177827000617981\n",
            "Epoch 71/200, Loss: 0.30293360352516174\n",
            "Epoch 81/200, Loss: 0.28923946619033813\n",
            "Epoch 91/200, Loss: 0.2765859365463257\n",
            "Epoch 101/200, Loss: 0.2648714482784271\n",
            "Epoch 111/200, Loss: 0.25400567054748535\n",
            "Epoch 121/200, Loss: 0.24390797317028046\n",
            "Epoch 131/200, Loss: 0.23450689017772675\n",
            "Epoch 141/200, Loss: 0.22573862969875336\n",
            "Epoch 151/200, Loss: 0.2175462394952774\n",
            "Epoch 161/200, Loss: 0.2098788470029831\n",
            "Epoch 171/200, Loss: 0.20269104838371277\n",
            "Epoch 181/200, Loss: 0.19594219326972961\n",
            "Epoch 191/200, Loss: 0.18959563970565796\n",
            "Accuracy for 2 levels: 1.0\n",
            "Quantizing target column into 3 levels...\n",
            "Epoch 1/200, Loss: 1.247334599494934\n",
            "Epoch 11/200, Loss: 1.176951289176941\n",
            "Epoch 21/200, Loss: 1.1103790998458862\n",
            "Epoch 31/200, Loss: 1.0475436449050903\n",
            "Epoch 41/200, Loss: 0.9883553981781006\n",
            "Epoch 51/200, Loss: 0.9327098727226257\n",
            "Epoch 61/200, Loss: 0.8804897665977478\n",
            "Epoch 71/200, Loss: 0.8315662145614624\n",
            "Epoch 81/200, Loss: 0.7858004570007324\n",
            "Epoch 91/200, Loss: 0.7430459260940552\n",
            "Epoch 101/200, Loss: 0.7031497359275818\n",
            "Epoch 111/200, Loss: 0.665956437587738\n",
            "Epoch 121/200, Loss: 0.6313085556030273\n",
            "Epoch 131/200, Loss: 0.5990492701530457\n",
            "Epoch 141/200, Loss: 0.5690240263938904\n",
            "Epoch 151/200, Loss: 0.5410824418067932\n",
            "Epoch 161/200, Loss: 0.515079140663147\n",
            "Epoch 171/200, Loss: 0.4908747375011444\n",
            "Epoch 181/200, Loss: 0.46833670139312744\n",
            "Epoch 191/200, Loss: 0.44734010100364685\n",
            "Accuracy for 3 levels: 1.0\n",
            "Quantizing target column into 4 levels...\n",
            "Epoch 1/200, Loss: 1.5976228713989258\n",
            "Epoch 11/200, Loss: 1.5097496509552002\n",
            "Epoch 21/200, Loss: 1.4261820316314697\n",
            "Epoch 31/200, Loss: 1.3469125032424927\n",
            "Epoch 41/200, Loss: 1.2719048261642456\n",
            "Epoch 51/200, Loss: 1.2010949850082397\n",
            "Epoch 61/200, Loss: 1.1343942880630493\n",
            "Epoch 71/200, Loss: 1.071690559387207\n",
            "Epoch 81/200, Loss: 1.0128509998321533\n",
            "Epoch 91/200, Loss: 0.9577262997627258\n",
            "Epoch 101/200, Loss: 0.9061543941497803\n",
            "Epoch 111/200, Loss: 0.8579626083374023\n",
            "Epoch 121/200, Loss: 0.8129721879959106\n",
            "Epoch 131/200, Loss: 0.7710011005401611\n",
            "Epoch 141/200, Loss: 0.7318673133850098\n",
            "Epoch 151/200, Loss: 0.6953903436660767\n",
            "Epoch 161/200, Loss: 0.6613943576812744\n",
            "Epoch 171/200, Loss: 0.6297090649604797\n",
            "Epoch 181/200, Loss: 0.6001711487770081\n",
            "Epoch 191/200, Loss: 0.5726258754730225\n",
            "Accuracy for 4 levels: 1.0\n",
            "Quantizing target column into 5 levels...\n",
            "Epoch 1/200, Loss: 1.2757315635681152\n",
            "Epoch 11/200, Loss: 1.2130143642425537\n",
            "Epoch 21/200, Loss: 1.1531994342803955\n",
            "Epoch 31/200, Loss: 1.0962421894073486\n",
            "Epoch 41/200, Loss: 1.0420905351638794\n",
            "Epoch 51/200, Loss: 0.9906847476959229\n",
            "Epoch 61/200, Loss: 0.9419567584991455\n",
            "Epoch 71/200, Loss: 0.8958318829536438\n",
            "Epoch 81/200, Loss: 0.852228581905365\n",
            "Epoch 91/200, Loss: 0.8110592365264893\n",
            "Epoch 101/200, Loss: 0.7722315788269043\n",
            "Epoch 111/200, Loss: 0.735649049282074\n",
            "Epoch 121/200, Loss: 0.7012119889259338\n",
            "Epoch 131/200, Loss: 0.668819010257721\n",
            "Epoch 141/200, Loss: 0.638367235660553\n",
            "Epoch 151/200, Loss: 0.6097540259361267\n",
            "Epoch 161/200, Loss: 0.5828775763511658\n",
            "Epoch 171/200, Loss: 0.557637631893158\n",
            "Epoch 181/200, Loss: 0.5339364409446716\n",
            "Epoch 191/200, Loss: 0.5116793513298035\n",
            "Accuracy for 5 levels: 0.9845\n",
            "Quantizing target column into 6 levels...\n",
            "Epoch 1/200, Loss: 1.4443515539169312\n",
            "Epoch 11/200, Loss: 1.3746824264526367\n",
            "Epoch 21/200, Loss: 1.3081306219100952\n",
            "Epoch 31/200, Loss: 1.2446720600128174\n",
            "Epoch 41/200, Loss: 1.1842684745788574\n",
            "Epoch 51/200, Loss: 1.1268670558929443\n",
            "Epoch 61/200, Loss: 1.0724035501480103\n",
            "Epoch 71/200, Loss: 1.0208014249801636\n",
            "Epoch 81/200, Loss: 0.9719752669334412\n",
            "Epoch 91/200, Loss: 0.9258306622505188\n",
            "Epoch 101/200, Loss: 0.8822661638259888\n",
            "Epoch 111/200, Loss: 0.8411755561828613\n",
            "Epoch 121/200, Loss: 0.8024484515190125\n",
            "Epoch 131/200, Loss: 0.765972375869751\n",
            "Epoch 141/200, Loss: 0.7316337823867798\n",
            "Epoch 151/200, Loss: 0.6993191242218018\n",
            "Epoch 161/200, Loss: 0.6689168214797974\n",
            "Epoch 171/200, Loss: 0.6403166651725769\n",
            "Epoch 181/200, Loss: 0.613412082195282\n",
            "Epoch 191/200, Loss: 0.588100016117096\n",
            "Accuracy for 6 levels: 1.0\n",
            "Quantizing target column into 7 levels...\n",
            "Epoch 1/200, Loss: 2.612617254257202\n",
            "Epoch 11/200, Loss: 2.5104708671569824\n",
            "Epoch 21/200, Loss: 2.410317897796631\n",
            "Epoch 31/200, Loss: 2.3122758865356445\n",
            "Epoch 41/200, Loss: 2.2164597511291504\n",
            "Epoch 51/200, Loss: 2.1229820251464844\n",
            "Epoch 61/200, Loss: 2.0319488048553467\n",
            "Epoch 71/200, Loss: 1.943461298942566\n",
            "Epoch 81/200, Loss: 1.8576112985610962\n",
            "Epoch 91/200, Loss: 1.7744815349578857\n",
            "Epoch 101/200, Loss: 1.6941423416137695\n",
            "Epoch 111/200, Loss: 1.6166529655456543\n",
            "Epoch 121/200, Loss: 1.5420584678649902\n",
            "Epoch 131/200, Loss: 1.47038996219635\n",
            "Epoch 141/200, Loss: 1.401663899421692\n",
            "Epoch 151/200, Loss: 1.3358818292617798\n",
            "Epoch 161/200, Loss: 1.2730309963226318\n",
            "Epoch 171/200, Loss: 1.2130837440490723\n",
            "Epoch 181/200, Loss: 1.1560001373291016\n",
            "Epoch 191/200, Loss: 1.1017261743545532\n",
            "Accuracy for 7 levels: 0.9185\n",
            "Quantizing target column into 8 levels...\n",
            "Epoch 1/200, Loss: 2.0010242462158203\n",
            "Epoch 11/200, Loss: 1.9169394969940186\n",
            "Epoch 21/200, Loss: 1.8353101015090942\n",
            "Epoch 31/200, Loss: 1.756195306777954\n",
            "Epoch 41/200, Loss: 1.6796470880508423\n",
            "Epoch 51/200, Loss: 1.605706810951233\n",
            "Epoch 61/200, Loss: 1.5344064235687256\n",
            "Epoch 71/200, Loss: 1.4657678604125977\n",
            "Epoch 81/200, Loss: 1.3998022079467773\n",
            "Epoch 91/200, Loss: 1.3365100622177124\n",
            "Epoch 101/200, Loss: 1.275880217552185\n",
            "Epoch 111/200, Loss: 1.2178926467895508\n",
            "Epoch 121/200, Loss: 1.1625157594680786\n",
            "Epoch 131/200, Loss: 1.1097084283828735\n",
            "Epoch 141/200, Loss: 1.0594208240509033\n",
            "Epoch 151/200, Loss: 1.0115939378738403\n",
            "Epoch 161/200, Loss: 0.9661619067192078\n",
            "Epoch 171/200, Loss: 0.9230520129203796\n",
            "Epoch 181/200, Loss: 0.8821859955787659\n",
            "Epoch 191/200, Loss: 0.84348064661026\n",
            "Accuracy for 8 levels: 0.998\n",
            "Quantizing target column into 9 levels...\n",
            "Epoch 1/200, Loss: 2.4662156105041504\n",
            "Epoch 11/200, Loss: 2.3698019981384277\n",
            "Epoch 21/200, Loss: 2.2754056453704834\n",
            "Epoch 31/200, Loss: 2.183133602142334\n",
            "Epoch 41/200, Loss: 2.0930893421173096\n",
            "Epoch 51/200, Loss: 2.0053720474243164\n",
            "Epoch 61/200, Loss: 1.920073390007019\n",
            "Epoch 71/200, Loss: 1.837278962135315\n",
            "Epoch 81/200, Loss: 1.757063627243042\n",
            "Epoch 91/200, Loss: 1.6794930696487427\n",
            "Epoch 101/200, Loss: 1.604621410369873\n",
            "Epoch 111/200, Loss: 1.5324901342391968\n",
            "Epoch 121/200, Loss: 1.4631272554397583\n",
            "Epoch 131/200, Loss: 1.3965486288070679\n",
            "Epoch 141/200, Loss: 1.332755446434021\n",
            "Epoch 151/200, Loss: 1.271736741065979\n",
            "Epoch 161/200, Loss: 1.2134666442871094\n",
            "Epoch 171/200, Loss: 1.1579090356826782\n",
            "Epoch 181/200, Loss: 1.1050153970718384\n",
            "Epoch 191/200, Loss: 1.054726004600525\n",
            "Accuracy for 9 levels: 0.9745\n",
            "Quantizing target column into 10 levels...\n",
            "Epoch 1/200, Loss: 2.157153844833374\n",
            "Epoch 11/200, Loss: 2.071748733520508\n",
            "Epoch 21/200, Loss: 1.9885332584381104\n",
            "Epoch 31/200, Loss: 1.9075721502304077\n",
            "Epoch 41/200, Loss: 1.8289231061935425\n",
            "Epoch 51/200, Loss: 1.752636194229126\n",
            "Epoch 61/200, Loss: 1.6787543296813965\n",
            "Epoch 71/200, Loss: 1.6073110103607178\n",
            "Epoch 81/200, Loss: 1.538332462310791\n",
            "Epoch 91/200, Loss: 1.4718362092971802\n",
            "Epoch 101/200, Loss: 1.4078289270401\n",
            "Epoch 111/200, Loss: 1.3463107347488403\n",
            "Epoch 121/200, Loss: 1.287271499633789\n",
            "Epoch 131/200, Loss: 1.2306921482086182\n",
            "Epoch 141/200, Loss: 1.1765459775924683\n",
            "Epoch 151/200, Loss: 1.1247975826263428\n",
            "Epoch 161/200, Loss: 1.075404167175293\n",
            "Epoch 171/200, Loss: 1.0283156633377075\n",
            "Epoch 181/200, Loss: 0.9834755063056946\n",
            "Epoch 191/200, Loss: 0.9408215880393982\n",
            "Accuracy for 10 levels: 0.996\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Of2sHl5Z4dXi"
      },
      "source": [
        "**Question:** Report for which $i$ your model performs best. Describe and analyze the results! You could use visualizations or any other method!"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load dataset\n",
        "\n",
        "# Assuming the target column is the last column in the dataset\n",
        "target_column = data.columns[-1]\n",
        "\n",
        "accuracy_scores = []\n",
        "\n",
        "for i in range(2, 11):\n",
        "    quantized_target = pd.qcut(data[target_column], i, labels=False, duplicates='drop')  # Adjusted to handle duplicate bin edges\n",
        "    X = data.iloc[:, :-1].values\n",
        "    y = quantized_target.values\n",
        "\n",
        "    # Splitting the data into train and test sets\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "    # Normalize the features using the mean and standard deviation of the training set\n",
        "    scaler = StandardScaler()\n",
        "    X_train = scaler.fit_transform(X_train)\n",
        "    X_test = scaler.transform(X_test)\n",
        "\n",
        "    # Convert to PyTorch tensors\n",
        "    X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
        "    y_train_tensor = torch.tensor(y_train, dtype=torch.long)\n",
        "\n",
        "    X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
        "\n",
        "    # Initialize and train the model\n",
        "    input_dim = X_train.shape[1]\n",
        "    output_dim = i\n",
        "    model = MyMultinomialLogisticRegression(input_dim, output_dim)\n",
        "    model.fit(X_train_tensor, y_train_tensor, epochs=200)  # Increased epochs to 1000\n",
        "\n",
        "    # Predictions\n",
        "    y_pred = model.predict(X_test_tensor)\n",
        "\n",
        "    # Calculate accuracy\n",
        "    accuracy = accuracy_score(y_test, y_pred.numpy())\n",
        "    accuracy_scores.append(accuracy)\n",
        "\n",
        "# Plotting the accuracy scores\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(range(2, 11), accuracy_scores, marker='o', linestyle='-')\n",
        "plt.title('Accuracy vs. Number of Levels (i)')\n",
        "plt.xlabel('Number of Levels (i)')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "# Find the index of the maximum accuracy score\n",
        "best_i_index = accuracy_scores.index(max(accuracy_scores))\n",
        "best_i = best_i_index + 2  # Increment index by 2 to account for starting from 2\n",
        "best_accuracy = accuracy_scores[best_i_index]\n",
        "\n",
        "print(f\"The model performs best for i = {best_i} with an accuracy of {best_accuracy:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "lA9sytCDgGaz",
        "outputId": "15c5f0de-2056-4d3c-ac76-64e8970220fc"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/200, Loss: 0.844975471496582\n",
            "Epoch 11/200, Loss: 0.7871277332305908\n",
            "Epoch 21/200, Loss: 0.7332953810691833\n",
            "Epoch 31/200, Loss: 0.6833488941192627\n",
            "Epoch 41/200, Loss: 0.6371359825134277\n",
            "Epoch 51/200, Loss: 0.5944862961769104\n",
            "Epoch 61/200, Loss: 0.5552138090133667\n",
            "Epoch 71/200, Loss: 0.5191212296485901\n",
            "Epoch 81/200, Loss: 0.48600393533706665\n",
            "Epoch 91/200, Loss: 0.4556543529033661\n",
            "Epoch 101/200, Loss: 0.42786550521850586\n",
            "Epoch 111/200, Loss: 0.40243443846702576\n",
            "Epoch 121/200, Loss: 0.3791649341583252\n",
            "Epoch 131/200, Loss: 0.3578701615333557\n",
            "Epoch 141/200, Loss: 0.33837342262268066\n",
            "Epoch 151/200, Loss: 0.32050999999046326\n",
            "Epoch 161/200, Loss: 0.3041275143623352\n",
            "Epoch 171/200, Loss: 0.2890855073928833\n",
            "Epoch 181/200, Loss: 0.2752561569213867\n",
            "Epoch 191/200, Loss: 0.26252293586730957\n",
            "Epoch 1/200, Loss: 0.927702009677887\n",
            "Epoch 11/200, Loss: 0.8708921670913696\n",
            "Epoch 21/200, Loss: 0.818280041217804\n",
            "Epoch 31/200, Loss: 0.7696250081062317\n",
            "Epoch 41/200, Loss: 0.7246796488761902\n",
            "Epoch 51/200, Loss: 0.6831936240196228\n",
            "Epoch 61/200, Loss: 0.6449195146560669\n",
            "Epoch 71/200, Loss: 0.6096150279045105\n",
            "Epoch 81/200, Loss: 0.5770473480224609\n",
            "Epoch 91/200, Loss: 0.5469943284988403\n",
            "Epoch 101/200, Loss: 0.5192465782165527\n",
            "Epoch 111/200, Loss: 0.49360865354537964\n",
            "Epoch 121/200, Loss: 0.469898521900177\n",
            "Epoch 131/200, Loss: 0.44794878363609314\n",
            "Epoch 141/200, Loss: 0.4276055097579956\n",
            "Epoch 151/200, Loss: 0.40872788429260254\n",
            "Epoch 161/200, Loss: 0.3911876976490021\n",
            "Epoch 171/200, Loss: 0.3748683035373688\n",
            "Epoch 181/200, Loss: 0.3596639633178711\n",
            "Epoch 191/200, Loss: 0.34547871351242065\n",
            "Epoch 1/200, Loss: 1.2301537990570068\n",
            "Epoch 11/200, Loss: 1.1674225330352783\n",
            "Epoch 21/200, Loss: 1.1077415943145752\n",
            "Epoch 31/200, Loss: 1.0510587692260742\n",
            "Epoch 41/200, Loss: 0.9973128437995911\n",
            "Epoch 51/200, Loss: 0.9464334845542908\n",
            "Epoch 61/200, Loss: 0.8983418345451355\n",
            "Epoch 71/200, Loss: 0.852950930595398\n",
            "Epoch 81/200, Loss: 0.8101668357849121\n",
            "Epoch 91/200, Loss: 0.7698896527290344\n",
            "Epoch 101/200, Loss: 0.7320139408111572\n",
            "Epoch 111/200, Loss: 0.6964308023452759\n",
            "Epoch 121/200, Loss: 0.6630288362503052\n",
            "Epoch 131/200, Loss: 0.6316950917243958\n",
            "Epoch 141/200, Loss: 0.6023163795471191\n",
            "Epoch 151/200, Loss: 0.5747810006141663\n",
            "Epoch 161/200, Loss: 0.5489786267280579\n",
            "Epoch 171/200, Loss: 0.5248019695281982\n",
            "Epoch 181/200, Loss: 0.5021472573280334\n",
            "Epoch 191/200, Loss: 0.48091480135917664\n",
            "Epoch 1/200, Loss: 1.757703423500061\n",
            "Epoch 11/200, Loss: 1.6741600036621094\n",
            "Epoch 21/200, Loss: 1.5937581062316895\n",
            "Epoch 31/200, Loss: 1.5165218114852905\n",
            "Epoch 41/200, Loss: 1.4424631595611572\n",
            "Epoch 51/200, Loss: 1.3715802431106567\n",
            "Epoch 61/200, Loss: 1.3038591146469116\n",
            "Epoch 71/200, Loss: 1.2392737865447998\n",
            "Epoch 81/200, Loss: 1.1777846813201904\n",
            "Epoch 91/200, Loss: 1.1193416118621826\n",
            "Epoch 101/200, Loss: 1.0638819932937622\n",
            "Epoch 111/200, Loss: 1.0113341808319092\n",
            "Epoch 121/200, Loss: 0.9616162180900574\n",
            "Epoch 131/200, Loss: 0.9146378636360168\n",
            "Epoch 141/200, Loss: 0.8703020215034485\n",
            "Epoch 151/200, Loss: 0.8285049200057983\n",
            "Epoch 161/200, Loss: 0.7891387939453125\n",
            "Epoch 171/200, Loss: 0.752092182636261\n",
            "Epoch 181/200, Loss: 0.7172518968582153\n",
            "Epoch 191/200, Loss: 0.684503972530365\n",
            "Epoch 1/200, Loss: 2.1644656658172607\n",
            "Epoch 11/200, Loss: 2.067941904067993\n",
            "Epoch 21/200, Loss: 1.974244475364685\n",
            "Epoch 31/200, Loss: 1.88347327709198\n",
            "Epoch 41/200, Loss: 1.7957165241241455\n",
            "Epoch 51/200, Loss: 1.7110493183135986\n",
            "Epoch 61/200, Loss: 1.6295329332351685\n",
            "Epoch 71/200, Loss: 1.551213264465332\n",
            "Epoch 81/200, Loss: 1.4761192798614502\n",
            "Epoch 91/200, Loss: 1.4042633771896362\n",
            "Epoch 101/200, Loss: 1.3356411457061768\n",
            "Epoch 111/200, Loss: 1.270231008529663\n",
            "Epoch 121/200, Loss: 1.207996129989624\n",
            "Epoch 131/200, Loss: 1.1488832235336304\n",
            "Epoch 141/200, Loss: 1.0928256511688232\n",
            "Epoch 151/200, Loss: 1.0397443771362305\n",
            "Epoch 161/200, Loss: 0.9895488619804382\n",
            "Epoch 171/200, Loss: 0.9421396851539612\n",
            "Epoch 181/200, Loss: 0.8974096775054932\n",
            "Epoch 191/200, Loss: 0.8552463054656982\n",
            "Epoch 1/200, Loss: 1.6573359966278076\n",
            "Epoch 11/200, Loss: 1.580810308456421\n",
            "Epoch 21/200, Loss: 1.5072762966156006\n",
            "Epoch 31/200, Loss: 1.4367376565933228\n",
            "Epoch 41/200, Loss: 1.3691853284835815\n",
            "Epoch 51/200, Loss: 1.3045992851257324\n",
            "Epoch 61/200, Loss: 1.2429476976394653\n",
            "Epoch 71/200, Loss: 1.1841881275177002\n",
            "Epoch 81/200, Loss: 1.1282669305801392\n",
            "Epoch 91/200, Loss: 1.0751218795776367\n",
            "Epoch 101/200, Loss: 1.0246812105178833\n",
            "Epoch 111/200, Loss: 0.9768651723861694\n",
            "Epoch 121/200, Loss: 0.9315882325172424\n",
            "Epoch 131/200, Loss: 0.8887584805488586\n",
            "Epoch 141/200, Loss: 0.8482799530029297\n",
            "Epoch 151/200, Loss: 0.8100531101226807\n",
            "Epoch 161/200, Loss: 0.7739766240119934\n",
            "Epoch 171/200, Loss: 0.7399476170539856\n",
            "Epoch 181/200, Loss: 0.7078633904457092\n",
            "Epoch 191/200, Loss: 0.6776221990585327\n",
            "Epoch 1/200, Loss: 2.173445701599121\n",
            "Epoch 11/200, Loss: 2.0861635208129883\n",
            "Epoch 21/200, Loss: 2.0011229515075684\n",
            "Epoch 31/200, Loss: 1.9183872938156128\n",
            "Epoch 41/200, Loss: 1.8380134105682373\n",
            "Epoch 51/200, Loss: 1.7600523233413696\n",
            "Epoch 61/200, Loss: 1.6845470666885376\n",
            "Epoch 71/200, Loss: 1.6115338802337646\n",
            "Epoch 81/200, Loss: 1.5410404205322266\n",
            "Epoch 91/200, Loss: 1.4730865955352783\n",
            "Epoch 101/200, Loss: 1.407683253288269\n",
            "Epoch 111/200, Loss: 1.344832420349121\n",
            "Epoch 121/200, Loss: 1.2845271825790405\n",
            "Epoch 131/200, Loss: 1.2267520427703857\n",
            "Epoch 141/200, Loss: 1.1714826822280884\n",
            "Epoch 151/200, Loss: 1.1186857223510742\n",
            "Epoch 161/200, Loss: 1.0683200359344482\n",
            "Epoch 171/200, Loss: 1.0203369855880737\n",
            "Epoch 181/200, Loss: 0.9746799468994141\n",
            "Epoch 191/200, Loss: 0.9312868118286133\n",
            "Epoch 1/200, Loss: 2.607807159423828\n",
            "Epoch 11/200, Loss: 2.511057138442993\n",
            "Epoch 21/200, Loss: 2.416266918182373\n",
            "Epoch 31/200, Loss: 2.323514938354492\n",
            "Epoch 41/200, Loss: 2.232877492904663\n",
            "Epoch 51/200, Loss: 2.1444251537323\n",
            "Epoch 61/200, Loss: 2.05822491645813\n",
            "Epoch 71/200, Loss: 1.9743379354476929\n",
            "Epoch 81/200, Loss: 1.8928186893463135\n",
            "Epoch 91/200, Loss: 1.8137155771255493\n",
            "Epoch 101/200, Loss: 1.737069010734558\n",
            "Epoch 111/200, Loss: 1.6629133224487305\n",
            "Epoch 121/200, Loss: 1.5912730693817139\n",
            "Epoch 131/200, Loss: 1.5221660137176514\n",
            "Epoch 141/200, Loss: 1.4556009769439697\n",
            "Epoch 151/200, Loss: 1.3915784358978271\n",
            "Epoch 161/200, Loss: 1.3300907611846924\n",
            "Epoch 171/200, Loss: 1.2711212635040283\n",
            "Epoch 181/200, Loss: 1.2146459817886353\n",
            "Epoch 191/200, Loss: 1.1606323719024658\n",
            "Epoch 1/200, Loss: 2.501343250274658\n",
            "Epoch 11/200, Loss: 2.4058053493499756\n",
            "Epoch 21/200, Loss: 2.312105417251587\n",
            "Epoch 31/200, Loss: 2.22035551071167\n",
            "Epoch 41/200, Loss: 2.1306662559509277\n",
            "Epoch 51/200, Loss: 2.043144464492798\n",
            "Epoch 61/200, Loss: 1.9578925371170044\n",
            "Epoch 71/200, Loss: 1.8750066757202148\n",
            "Epoch 81/200, Loss: 1.7945750951766968\n",
            "Epoch 91/200, Loss: 1.716675877571106\n",
            "Epoch 101/200, Loss: 1.6413766145706177\n",
            "Epoch 111/200, Loss: 1.5687315464019775\n",
            "Epoch 121/200, Loss: 1.4987828731536865\n",
            "Epoch 131/200, Loss: 1.4315580129623413\n",
            "Epoch 141/200, Loss: 1.367070198059082\n",
            "Epoch 151/200, Loss: 1.3053183555603027\n",
            "Epoch 161/200, Loss: 1.2462869882583618\n",
            "Epoch 171/200, Loss: 1.1899477243423462\n",
            "Epoch 181/200, Loss: 1.1362589597702026\n",
            "Epoch 191/200, Loss: 1.0851672887802124\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x600 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAA1cAAAIjCAYAAADvBuGTAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAACVSElEQVR4nOzdeVxU9foH8M+ZYRb2fQcBccEVXMlMS0VRy2u2WVZuZTfTXxm3vFou2ea1rmaZaXVTy+VmZXpbLcUtc1dwSXBXEGWXHYZh5vz+gBlBQEFnOLN83q+XL50zZ84852EiHr7f7/MVRFEUQURERERERHdEJnUAREREREREtoDFFRERERERkQmwuCIiIiIiIjIBFldEREREREQmwOKKiIiIiIjIBFhcERERERERmQCLKyIiIiIiIhNgcUVERERERGQCLK6IiIiIiIhMgMUVERGRiQiCgKlTp0odRpNUVVVh+vTpCA0NhUwmw4MPPih1SLdl1apVEAQBFy9eNMn19Ho9OnfujHfeeeem73HXXXdh+vTpJnlPIrIdLK6IyO598sknEAQBsbGxUodCt3Dx4kUIggBBELBhw4Z6z7/xxhsQBAG5ubkSRGddVqxYgffffx+PPPIIvvzyS7z88suNnnvfffehc+fOLRiddP773/8iPT39lkXyP//5TyxduhSZmZktFBkRWQMWV0Rk99auXYvw8HAcOHAAZ8+elTocaqI333wToihKHYbV2rZtG4KDg/HBBx/g6aefxr333it1SBbh/fffx+OPPw53d3fjsaeffhrl5eUICwszHhs5ciTc3NzwySefSBEmEVkoFldEZNcuXLiAPXv2YNGiRfD19cXatWulDqlRpaWlUodgMWJiYnDs2DFs3LhR6lBaXEVFBfR6/R1fJzs7Gx4eHncekA1JSkrC0aNH8dhjj9U5LpfLoVarIQiC8ZhMJsMjjzyCr776ikU+ERmxuCIiu7Z27Vp4enri/vvvxyOPPNJocVVQUICXX34Z4eHhUKlUCAkJwdixY+tMP6uoqMAbb7yBdu3aQa1WIzAwEA899BDOnTsHANixYwcEQcCOHTvqXNsw1W3VqlXGY+PHj4eLiwvOnTuH4cOHw9XVFU8++SQA4I8//sCjjz6KVq1aQaVSITQ0FC+//DLKy8vrxZ2amorHHnsMvr6+cHR0RPv27fH6668DALZv3w5BEBosUNatWwdBELB3794G83Ho0CEIgoAvv/yy3nO//fYbBEHATz/9BAAoLi7GtGnTjLnz8/PD4MGDceTIkQav3RSPP/442rVr16TRq/DwcIwfP77e8fvuuw/33Xef8bHh6/PNN99g3rx5CA4OhqurKx555BEUFhZCo9Fg2rRp8PPzg4uLCyZMmACNRtPge65duxbt27eHWq1Gjx49sGvXrnrnZGRkYOLEifD394dKpUKnTp2wYsWKOucYYvr6668xa9YsBAcHw8nJCUVFRY3eb2lpKf7xj38gNDQUKpUK7du3x7///W9jngyft+3bt+Ovv/4yTrO88XN5O3799Vf069cPzs7OcHV1xf3334+//vrL+Py///1vCIKAS5cu1XvtzJkzoVQqce3aNeOx/fv3Y+jQoXB3d4eTkxPuvfde/Pnnn7eM49ChQ4iPj4ePjw8cHR0RERGBiRMn3vJ1mzZtglKpRP/+/escb2xd1+DBg3Hp0iUkJyff8tpEZB8cpA6AiEhKa9euxUMPPQSlUoknnngCy5Ytw8GDB9GrVy/jOSUlJejXrx9SUlIwceJEdO/eHbm5ufjhhx9w+fJl+Pj4QKfT4YEHHkBiYiIef/xxvPTSSyguLsaWLVtw4sQJREZGNju2qqoqxMfH45577sG///1vODk5AQC+/fZblJWVYfLkyfD29saBAwewZMkSXL58Gd9++63x9ceOHUO/fv2gUCjw3HPPITw8HOfOncOPP/6Id955B/fddx9CQ0Oxdu1ajBo1ql5eIiMj0adPnwZj69mzJ1q3bo1vvvkG48aNq/Pc+vXr4enpifj4eADA888/j++++w5Tp05Fx44dkZeXh927dyMlJQXdu3dvdl6A6pGEWbNmYezYsdi4cSMeeuih27pOQ+bPnw9HR0fMmDEDZ8+exZIlS6BQKCCTyXDt2jW88cYb2LdvH1atWoWIiAjMmTOnzut37tyJ9evX48UXX4RKpcInn3yCoUOH4sCBA8Z1S1lZWbjrrruMDTB8fX3x66+/4plnnkFRURGmTZtW55pvvfUWlEolXnnlFWg0GiiVygZjF0URf/vb37B9+3Y888wziImJwW+//YZXX30VGRkZ+OCDD+Dr64vVq1fjnXfeQUlJCebPnw8A6NChwx3lbfXq1Rg3bhzi4+OxYMEClJWVYdmyZbjnnnuQlJSE8PBwPPbYY5g+fTq++eYbvPrqq3Ve/80332DIkCHw9PQEUD1tcdiwYejRowfmzp0LmUyGlStXYuDAgfjjjz/Qu3fvBuPIzs7GkCFD4OvrixkzZsDDwwMXL17E999/f8t72LNnDzp37gyFQtGke+7RowcA4M8//0S3bt2a9BoisnEiEZGdOnTokAhA3LJliyiKoqjX68WQkBDxpZdeqnPenDlzRADi999/X+8aer1eFEVRXLFihQhAXLRoUaPnbN++XQQgbt++vc7zFy5cEAGIK1euNB4bN26cCECcMWNGveuVlZXVOzZ//nxREATx0qVLxmP9+/cXXV1d6xyrHY8oiuLMmTNFlUolFhQUGI9lZ2eLDg4O4ty5c+u9T20zZ84UFQqFmJ+fbzym0WhEDw8PceLEicZj7u7u4pQpU256raYy5Or9998Xq6qqxLZt24rR0dHGe5o7d64IQMzJyTG+JiwsTBw3bly9a917773ivffea3xs+Pp07txZrKysNB5/4oknREEQxGHDhtV5fZ8+fcSwsLA6xwCIAMRDhw4Zj126dElUq9XiqFGjjMeeeeYZMTAwUMzNza3z+scff1x0d3c3fo0NMbVu3brBr/uNNm3aJAIQ33777TrHH3nkEVEQBPHs2bN17r9Tp063vGZTzi0uLhY9PDzESZMm1TmemZkpuru71znep08fsUePHnXOO3DggAhA/Oqrr0RRrP6Mtm3bVoyPj6/zeS0rKxMjIiLEwYMHG4+tXLlSBCBeuHBBFEVR3LhxowhAPHjwYJPurbaQkBDx4Ycfrnf8xveoTalUipMnT272exGRbeK0QCKyW2vXroW/vz8GDBgAoLqN9ujRo/H1119Dp9MZz9uwYQOio6Prje4YXmM4x8fHB//3f//X6Dm3Y/LkyfWOOTo6Gv9dWlqK3Nxc3H333RBFEUlJSQCAnJwc7Nq1CxMnTkSrVq0ajWfs2LHQaDT47rvvjMfWr1+PqqoqPPXUUzeNbfTo0dBqtXVGBH7//XcUFBRg9OjRxmMeHh7Yv38/rly50sS7bhrD6NXRo0exadMmk1137NixdUYuYmNjIYpivWllsbGxSE9PR1VVVZ3jffr0MY5oAECrVq0wcuRI/Pbbb9DpdBBFERs2bMCIESMgiiJyc3ONf+Lj41FYWFhvyuS4cePqfN0b88svv0Aul+PFF1+sc/wf//gHRFHEr7/+2uQ8NMeWLVtQUFCAJ554os79yOVyxMbGYvv27cZzR48ejcOHDxunywLVnzmVSoWRI0cCAJKTk3HmzBmMGTMGeXl5xuuVlpZi0KBB2LVrV6PrzgzryH766Sdotdpm3UdeXp5x5KypPD092Z2SiIxYXBGRXdLpdPj6668xYMAAXLhwAWfPnsXZs2cRGxuLrKwsJCYmGs89d+7cLdtQnzt3Du3bt4eDg+lmWzs4OCAkJKTe8bS0NIwfPx5eXl5wcXGBr6+vsdNbYWEhAOD8+fMAcMu4o6Ki0KtXrzprzdauXYu77roLbdq0uelro6OjERUVhfXr1xuPrV+/Hj4+Phg4cKDx2HvvvYcTJ04gNDQUvXv3xhtvvGGM7049+eSTaNOmjUk7B95YjBq6xoWGhtY7rtfrjTk3aNu2bb1rtmvXDmVlZcjJyUFOTg4KCgrw2WefwdfXt86fCRMmAKie2lZbREREk2K/dOkSgoKC4OrqWue4YcpfQ2udTOHMmTMAgIEDB9a7p99//73O/Tz66KOQyWTGz40oivj2228xbNgwuLm51bneuHHj6l3vP//5DzQaTb28G9x77714+OGHMW/ePPj4+GDkyJFYuXJlo+vjbtTcz5Eoinf0CxQisi1cc0VEdmnbtm24evUqvv76a3z99df1nl+7di2GDBli0vds7Aew2qNktalUKshksnrnDh48GPn5+fjnP/+JqKgoODs7IyMjA+PHj7+tLnJjx47FSy+9hMuXL0Oj0WDfvn34+OOPm/Ta0aNH45133kFubi5cXV3xww8/4IknnqhTZD722GPo168fNm7ciN9//x3vv/8+FixYgO+//x7Dhg1rdry1GUavxo8fj//9738NnnOzvMvl8gav2dh7NaS5P4wbvkZPPfVUvfVqBl27dq3zuCmjVlIy3NPq1asREBBQ7/nan4egoCD069cP33zzDV577TXs27cPaWlpWLBgQb3rvf/++4iJiWnwPV1cXBo8LggCvvvuO+zbtw8//vgjfvvtN0ycOBELFy7Evn37Gn0dAHh7e9dpqNEUBQUF8PHxadZriMh2sbgiIru0du1a+Pn5YenSpfWe+/7777Fx40YsX74cjo6OiIyMxIkTJ256vcjISOzfvx9arbbRxfCG6UYFBQV1jjdnNOH48eM4ffo0vvzyS4wdO9Z4fMuWLXXOa926NQDcMm6guvNeQkIC/vvf/6K8vBwKhaLOtL6bGT16NObNm4cNGzbA398fRUVFePzxx+udFxgYiBdeeAEvvPACsrOz0b17d7zzzjt3XFwB1UXK22+/jXnz5uFvf/tbvec9PT3r5RyozrshT6ZkGHWp7fTp03BycoKvry8AwNXVFTqdDnFxcSZ977CwMGzduhXFxcV1Rq9SU1ONz5uDoWGLn59fk+5p9OjReOGFF3Dq1CmsX78eTk5OGDFiRL3rubm53XaO7rrrLtx111145513sG7dOjz55JP4+uuv8eyzzzb6mqioKFy4cKHJ75GRkYHKyso7bgZCRLaD0wKJyO6Ul5fj+++/xwMPPIBHHnmk3p+pU6eiuLgYP/zwAwDg4YcfxtGjRxtsWW4YtXj44YeRm5vb4IiP4ZywsDDI5fJ6bbmbswmpYfSk9miJKIr48MMP65zn6+uL/v37Y8WKFUhLS2swHgMfHx8MGzYMa9aswdq1azF06NAm/ya+Q4cO6NKlC9avX4/169cjMDCwThtrnU5Xb/qWn58fgoKC6kzTys3NRWpqKsrKypr0vrUZRq+Sk5ONX7PaIiMjsW/fPlRWVhqP/fTTT0hPT2/2ezXF3r1766yZSk9Px//+9z8MGTIEcrkccrkcDz/8MDZs2NBg8ZuTk3Pb7z18+HDodLp6n8MPPvgAgiCYpJhtSHx8PNzc3PDuu+82uM7pxnt6+OGHIZfL8d///hfffvstHnjgATg7Oxuf79GjByIjI/Hvf/8bJSUlt7xebdeuXav3GTeMft1qamCfPn1w4sSJJk8hPHz4MADg7rvvbtL5RGT7OHJFRHbnhx9+QHFxcYOjHED1b7wNGwqPHj0ar776Kr777js8+uijmDhxInr06IH8/Hz88MMPWL58OaKjozF27Fh89dVXSEhIwIEDB9CvXz+UlpZi69ateOGFFzBy5Ei4u7vj0UcfxZIlSyAIAiIjI/HTTz/VW19zM1FRUYiMjMQrr7yCjIwMuLm5YcOGDQ1OZfroo49wzz33oHv37njuuecQERGBixcv4ueff663L8/YsWPxyCOPAKhu+90co0ePxpw5c6BWq/HMM8/UmcpYXFyMkJAQPPLII4iOjoaLiwu2bt2KgwcPYuHChcbzPv74Y8ybNw/bt2+vs/dUUz355JN46623Gtxv6Nlnn8V3332HoUOH4rHHHsO5c+ewZs2a22qP3xSdO3dGfHx8nVbsADBv3jzjOf/617+wfft2xMbGYtKkSejYsSPy8/Nx5MgRbN26Ffn5+bf13iNGjMCAAQPw+uuv4+LFi4iOjsbvv/+O//3vf5g2bdod3XNOTg7efvvtescjIiLw5JNPYtmyZXj66afRvXt3PP744/D19UVaWhp+/vln9O3bt07B5+fnhwEDBmDRokUoLi6uN1Iqk8nwn//8B8OGDUOnTp0wYcIEBAcHIyMjA9u3b4ebmxt+/PHHBuP88ssv8cknn2DUqFGIjIxEcXExPv/8c7i5uWH48OE3vceRI0firbfews6dO5s0LXjLli1o1aoV27AT0XUt36CQiEhaI0aMENVqtVhaWtroOePHjxcVCoWxVXZeXp44depUMTg4WFQqlWJISIg4bty4Oq20y8rKxNdff12MiIgQFQqFGBAQID7yyCPiuXPnjOfk5OSIDz/8sOjk5CR6enqKf//738UTJ0402Ird2dm5wdhOnjwpxsXFiS4uLqKPj484adIk8ejRo/WuIYqieOLECXHUqFGih4eHqFarxfbt24uzZ8+ud02NRiN6enqK7u7uYnl5eVPSaHTmzBljC/Ldu3fXu+6rr74qRkdHi66urqKzs7MYHR0tfvLJJ3XOM7RQv7FN/Y1qt2K/kaFdNm5oxS6Korhw4UIxODhYVKlUYt++fcVDhw412or922+/bfC6N7b2bqjtOwBxypQp4po1a8S2bduKKpVK7NatW4P3lZWVJU6ZMkUMDQ01fl4GDRokfvbZZ7eM6WaKi4vFl19+WQwKChIVCoXYtm1b8f3336/T0lwUm9+K3ZDbG/8MGjSoTrzx8fGiu7u7qFarxcjISHH8+PF1WtMbfP755yIA0dXVtdHPXFJSkvjQQw+J3t7eokqlEsPCwsTHHntMTExMNJ5zY5v0I0eOiE888YTYqlUrUaVSiX5+fuIDDzzQYAwN6dq1q/jMM8/UOdZQK3adTicGBgaKs2bNatJ1icg+CKJoovZKRERktaqqqhAUFIQRI0bgiy++kDocIsmsXr0aU6ZMQVpamrGte0M2bdqEMWPG4Ny5cwgMDGy5AInIonHNFRERYdOmTcjJyanTJIPIHj355JNo1apVg81ualuwYAGmTp3KwoqI6uDIFRGRHdu/fz+OHTuGt956Cz4+PvU2ryUiIqKm48gVEZEdW7ZsGSZPngw/Pz989dVXUodDRERk1ThyRUREREREZAIcuSIiIiIiIjIBFldEREREREQmwE2EG6DX63HlyhW4urpCEASpwyEiIiIiIomIooji4mIEBQVBJrv52BSLqwZcuXIFoaGhUodBREREREQWIj09HSEhITc9h8VVA1xdXQFUJ9DNzU3SWLRaLX7//XcMGTIECoVC0lhsEfNrfsyxeTG/5sX8mhfza17Mr3kxv+ZlSfktKipCaGiosUa4GRZXDTBMBXRzc7OI4srJyQlubm6Sf7BsEfNrfsyxeTG/5sX8mhfza17Mr3kxv+ZlifltynIhNrQgIiIiIiIyARZXREREREREJsDiioiIiIiIyARYXBEREREREZkAiysiIiIiIiITYHFFRERERERkAiyuiIiIiIiITIDFFRERERERkQmwuCIiIiIiIjIBFldEREREREQmwOKKiIiIiIjIBFhcERERERERmQCLKyIiIiIiIhNgcUV2S6cXsf9CPg7nCth/IR86vSh1SERERERkxSQtrnbt2oURI0YgKCgIgiBg06ZNt3zNjh070L17d6hUKrRp0warVq2qd87SpUsRHh4OtVqN2NhYHDhwwPTBk1XbfOIq7lmwDU+tOISvzsjx1IpDuGfBNmw+cVXq0IiIiIjISklaXJWWliI6OhpLly5t0vkXLlzA/fffjwEDBiA5ORnTpk3Ds88+i99++814zvr165GQkIC5c+fiyJEjiI6ORnx8PLKzs811G2RlNp+4islrjuBqYUWd45mFFZi85ggLLCIiIiK6LQ5SvvmwYcMwbNiwJp+/fPlyREREYOHChQCADh06YPfu3fjggw8QHx8PAFi0aBEmTZqECRMmGF/z888/Y8WKFZgxY4bpb4Ksik4vYt6PJ9HQBEARgABg3o8nMbhjAOQyoYWjIyIiIiJrJmlx1Vx79+5FXFxcnWPx8fGYNm0aAKCyshKHDx/GzJkzjc/LZDLExcVh7969jV5Xo9FAo9EYHxcVFQEAtFottFqtCe+g+QzvL3UctmL/hfx6I1a1iQCuFlZg5Me70THQFUEejgj2UCPIQ40gd0cEuKngIOdSxebgZ9i8mF/zYn7Ni/k1L+bXvJhf87Kk/DYnBqsqrjIzM+Hv71/nmL+/P4qKilBeXo5r165Bp9M1eE5qamqj150/fz7mzZtX7/jvv/8OJycn0wR/h7Zs2SJ1CDbhUI4AQH7L805cKcKJK0X1jgsQ4a4EvFSAp0qEpwrwUonwUl5/rLr15e0SP8PmxfyaF/NrXsyveTG/5sX8mpcl5LesrKzJ51pVcWUuM2fOREJCgvFxUVERQkNDMWTIELi5uUkYWXWlvGXLFgwePBgKhULSWKzd0cuF+GTTXwBKbnnupHvCoVbIcKWwAlcKKpBRUI6rhRXQ6oCCyuo/KG542qCnk8I40hXkoUawhyOC3GtGvzwc4eWkgCDYz5RDfobNi/k1L+bXvJhf82J+zYv5NS9Lyq9hVltTWFVxFRAQgKysrDrHsrKy4ObmBkdHR8jlcsjl8gbPCQgIaPS6KpUKKpWq3nGFQiH5F9PAkmKxNml5ZXjvt1T8dOzWjSoEAAHuaswY3rHemiu9XkRuiQaXC8qRca0cGQXluFLr3xnXylGsqcK1Mi2ulWnx15XiBt/DUSE3Flohno4I9nBEsKcjgtyr/w5wU9vk1EN+hs2L+TUv5te8mF/zYn7Ni/k1L0vIb3Pe36qKqz59+uCXX36pc2zLli3o06cPAECpVKJHjx5ITEzEgw8+CADQ6/VITEzE1KlTWzpckti10kos2XYWq/ddhFYnQhCAh7qFoEeYB17feAIA6jS2MJRSc0fUL6wAQCYT4Oemhp+bGt1beTb4nkUV2upi61o5rhRW/20oxq4UlCO7WINyrQ7nckpxLqe0wWvIBCDATY3g2oWXR/W/Q2r+7aS0qv90iYiIiOyCpD+hlZSU4OzZs8bHFy5cQHJyMry8vNCqVSvMnDkTGRkZ+OqrrwAAzz//PD7++GNMnz4dEydOxLZt2/DNN9/g559/Nl4jISEB48aNQ8+ePdG7d28sXrwYpaWlxu6BZPsqtDqs2nMRS7efRXFFFQCgX1sfzBgWhU5B7gAAL2cl5v14sk5ziwB3NeaO6IihnQNv+73d1Aq4BSrQIbDh6aSaKh2u1kwzzLhh1OtKYXUBptWJ1dMRCytwENcavI6Xs9I45TDYwwlBHuqaUTAnBHs6wtPOph4SERERWQJJi6tDhw5hwIABxseGdU/jxo3DqlWrcPXqVaSlpRmfj4iIwM8//4yXX34ZH374IUJCQvCf//zH2IYdAEaPHo2cnBzMmTMHmZmZiImJwebNm+s1uSDbo9eL2JScgX//dgpXaoqmqABXvDa8A/q3861z7tDOgRjcMQB7z2bj9z/2Y0i/WPRp42f29usqBznCfZwR7uPc6D3klGhwuWakK6OBKYjFmirkl1Yiv7QSJzIangNsmHoY7OlUU4AZRsKqiy9/15bpeqjTi9h/IR+HcwV4X8hvkRwTERERSUXS4uq+++6DKDa041C1VatWNfiapKSkm1536tSpnAZoZ3afycW7v6Tg5NXqYiPQXY1/DGmPUd2CG/1hXi4TEBvhhbwUEbERXhbxQ79MJsDfTQ1/NzV6hDU89bCwXFt3rdcNRVhOE6YeymVC9dRD47RDtbHwCq6ZguiovLO2h5tPXK01OijHV2cOIdAEo4NERERElooLN8iqpVwtwr9+TcXO0zkAAFeVAyYPiMTEvhFQK2yzJ7q7owLujo1PPazQ6pBZWGEsuC7f0HjjamH11ENDUYaLDb+Pl7PSWGgF1RRhtdd93Wzq4eYTVzF5zZF6mzVnFlZg8pojWPZUdxZYJsCRQSIiIsvC4oqs0tXCciz6/TS+O3IZogg4yAQ8dVcYXhzUFl7OSqnDk5Ra0fSphxm1mm3UHv0qqTX18HhGYYPXcVLKjY02anc+DHBXY/b//qpXWAHVDUQEAPN+PInBHQNYCNwBjgwSERFZHhZXZFWKK7RYvvMcvth9ARVaPQDg/i6BeDW+faPFBNXV1KmHddZ63TAKllOsQVmlDmezS3A2+9b7htUmArhaWIGxX+xHgLsjHGQC5HIBCpkAuUwGhVyAXCbAQSbAQS6r8+/qv6sf1z1XVus6Na+pOc9BJqv1GgEKee3nZTXHrl/HGgo+jgwSERFZJhZXZBUqq/T474E0fJh4BvmllQCAXuGemDm8Q6Nt0en2GaYedgxqfOrh1cIK46jX9b2/ynAmqwR5NV+jm/nzXJ6pwzYJQcD1oqymYDP8u8EirVYRWF2k1Zxbq4AzvMZQINY+98Zi0vBaubzudeQyGRQyAQKAmRuPc2SQiIjIArG4IosmiiI2n8jEgs2puJhXBgBo7euMGUOjMLijP9uNS0StkCPCxxkRDYwW7j2Xhyc+33fLazx1VyuEejqhSi+iSieiSq9HlV6ETi9Cq9PX/C1CV3O8Slf3uSp9zWt0hn/XnGt4rKt9vbrP6fQitHo9GuqnI4qAVidCq9OZIlUtzjAy2GXuZng4KeGscoCzygEuNX+q/y1v5LgDnFXy6mPq6mPOSge7LdK4po2ISBrW/P2XxRVZrMOX8vHOzyk4klYAAPBxUWJaXDuM7hUKRQu0Eafb0zvCC4HuamQWVjQ4uiKgek+xeX/rLPk3Sn1NkWUs1mqKPJ3x3zcWabWLvhuLu9qv09e6XnVhZ3xNA0WfVi9CV+s6hvdtKK7sIg0u5Zfd8t7KtHqU1drH7U44KqqLMVd1dfHlrLxekBmPK68XZs41xZlLTXFmLNrUDlA5WEejGa5pIyKShrV//2VxRRbnfE4J3tt8Cpv/ygRQ/YPdpP6t8Vz/1nBR8SNr6eQyAXNHdMTkNUcgAHUKLEMpNXdER8kLK6B6/ZlKZh0/7Bs0dWTw349Go62fC0o0VSjRVKG05k+JRlfz9/Xjdf/WGR9X6au/euVaHcq1OuSWaO44foVcMI6IXS+6FNWjacpao2nqWqNstYo5Y9GmcoCTQg6ZGT5HXNNGRCQNW/j+y59UyWLklmjw4dYzWHcgDTq9CJkAPNYzFC8Pbgd/N7XU4VEzDO0ciGVPda/1m6dqAVb0mydL1dSRwZvt8dYUoihCU6W/oRDT1SvGSm4o2ko1VShuoJgr11ZPs9TqRBSUaVFQpr3t2Iz3KgBOCnmtQsyhVoFW63hjRVvN+a41xxRyGXR6EfN+PMk1bURELcxWvv+yuCLJlVfq8MXu81i+8zxKNFUAgEFRfvjnsCi083eVODq6XUM7B2JwxwDsPZuN3//YjyH9Yq1qzrSlaqmRQUEQoFbIoVbI4e2iuqNrAUCVTo/SSl2touv6KNmNI2iGoqxEo60zkla7oNOL1evjSit1KK3UAbjzUTWlgwwqBxmKK6oaPcewpu3AhXz0ifS+4/ckIqJqBy7k1/mF7I2s5fsviyuSjE4vYsPhy1i45RSyiqp/MOoS7I6Zw6Nwd6SPxNGRKchlAmIjvJCXIiI2wouFlYlY48igg1wGd0cZ3B0Vd3wtURRRodU3OK3xViNsDRV0mqrqbR0qq/SorPn3rTz31UG09XdFmLczWnk5IdzHCa28nBHm7QRvZyWb7RARNVN2cdPWCTf1PKmwuKIWJ4oidpzOwb9+ScWprGIAQIinI16Nb48RXYPMsoaCyNbY88igIAhwVMrhqJTD1/XOR9W0Oj3KNDqUVFbhzzO5mL7h2C1fU6zR4UhagbHhTm3OSjlaeTsjzMsJYd5OaOXthPCaIizIw9EuvkZERM3l59q0JSBNPU8qLK6oRZ3IKMS7v6RgT80eR+6OCvzfwDZ4uk+Y1XQRI7IUHBk0DYVcBncnGdydFHi4Rwg+2Hr6pmva/N1U+PTpnrh8rRyX8kuRlleGS3llSMsvw5XCcpRW6pBytQgpV4saeC8BIZ5OaGUovLyqC68wbyeEejlBreD3QSKyT01dU9w7wqulQ2sWFlfUIi5fK8PC309jY1IGAEApl2F833BMua8N3J3ufJoQEZEpNGVN2xt/64ToUA9Eh3rUe72mSof0/HKk5ZfiUq2i61JeKdLzy1Gp0+NCbiku5JY2+P4Bbmq08naqNepVPQIW7u3M75VEZNNqf/+9kaV1G74ZFldkVoVlWizdcRar/ryISl31WoaRMUF4ZUh7hHo5SRwdEVF9d7KmTeUgRxs/F7Txc6n3nE4vIrOoApfyaka78stq/q4uxIorqpBZVIHMouoF2zdyd1QYR7vCvJ0Q5uVcXYh5O8HfVc0p1URk9YZ2DsS/H4vGP745Wue4Ja8pvhGLKzILTZUOq/dewpJtZ1FYXt1yuU9rb7w2vAO6hLhLHB0R0c2ZY02bXCYg2MMRwR6OuDuy7nOiKOJamba68MovqzXqVV14ZRdrUFiuxbHLhTh2ubDetVUOslpTDZ2Na73CvJwQ4ukEpQM3Xici6+BXs5bW10WJYYHlVremmMUVmZReL+Kn41fx/m+pSM8vBwC083fBzGEdcF97X3bQIiKr0ZJr2gRBgJezEl7OSnRr5Vnv+bLKKmPRVXu0Ky2/DJevlUNTpceZ7BKcyS6p91qZAAR5ONYpvMK8aoovb2duzk5EFiWpplHQXa290MP5stWtKeZ3VDKZfefz8O4vKcbfqvq5qvCPIe3wcPcQOMj5W1MiotvlpHRAVIAbogLc6j2n1elxpaC8erQrvwxpeaW11nqVoVyrw+Vr5bh8rRx/Iq/e631clDWjXs7XpxzWFGI+LuZrK6/Ti9h/IR+HcwV4X8i3qt9ME5H5JKVdAwDEhHoA+ZelDeY2sLiiO3Ymqxj/+jUVianZAKrbED9/bySe6RcBJyU/YkRE5qSQyxDm7Ywwb+d6z4miiJwSzfVphnmluJR/vdFGfmklckuq/zTWVj60VkfDVjVrvcK8nRDorr7tX5xtPnG11po2Ob46cwiBVrSmgojMQxRFJKUXAABiQtxxuf7yU4vHn3zptmUXVeCDraex/mA69GL1FJoxvVvhxUFtTbL3DBER3RlBEODnqoafqxq9wuu3Ly6q0BpbyTfWVj41sxipmcX1XusgExDi6YhW3s4INzbacDY23WisrfzmE1cxec2Req2WMwsrMHnNESx7qjsLLCI7dTGvDAVlWigdZIgKcMXlW287aHFYXFGzlWqq8Omu8/h813mUa3UAgPhO/pg+NAqRvvU7ZBERkWVyUyvQOdgdnYPrNxrSVFVPJ7yU13hb+Yt5ZbiYV4ZdDVzb301l7GgYXtNWPsTDEXN/+KvBPWxEVLdbnvfjSQzuGMApgkR2yDAlsEuwu9U24mFxRU1WpdPj64PpWLz1DHJLNACAbq088NrwDg3+RpSIiKyXykGOSF+XBn9ppje2la8uthpqK59VpEFWkQYHLjZ9Xo8I4GphdSv6PpHeJrwbIrIGyTVTArs1sI+gtWBxRbckiiK2nMzCgs2pOJdTvfFlmLcT/jk0CsM6B7ADIBGRnZHJBAR5OCLIw7FeESSKIgrKtDVru0rrtJU/lVmMooqqW14/u7jilucQke0xdAqMaeUhaRx3gsUV3VRS2jXM/yXV+JtHL2clXhzYBmNiw6x2uJaIiMxHEAR4Oivh6ays7vZVy95zeXji8323vIafq9pM0RGRpSqv1CHlahEANLglhbVgcUUNupRXivd+O4Wfj10FUL1B5TP3ROD5+yLhplZIHB0REVmj3hFeCHRXI7OwosF1VwAQ6K5G7whONSeyNyeuFKJKL8LPVYUgdzWqqm49ym2JWFxRHddKK/HRtjNYs+8StDoRggA83D0E/xjSDoHujlKHR0REVkwuEzB3REdMXnMEAtBggfVKfHs2syCyQ4ZmFt1aeVj1khMWVwQAqNDqsPLPi/hkx1kU18yH79/OFzOGRqFjUP1NK4mIiG7H0M6BWPZU91r7XFWTCYBeBL49lI4HugZC5dBwK3cisk2G9VbWPCUQYHFl9/R6ERuTMrDw91O4UvM/uY6Bbpg5PAr92vpKHB0REdmioZ0DMbhjAPaezcbvf+zHkH6xcHNSYczn+7HvfD4S1h/Fkie6QcYRLCK7YSyurLhTIMDiyq79cSYH839JxcmaxYNB7mr8Y0h7jOoWzP+hERGRWcllAmIjvJCXIiI2wgsKhQKfPt0D41cewM/Hr8LXVYW5Izpa9fQgImqaq4XlyCyqgFwmoEtI/X33rAmLKzuUcrUI839Nxa7TOQAAV5UDXhjQBhP6hkOt4DQMIiKSRt82Plj4WAxe/G8SVu25iAB3NZ6/N1LqsIjIzAyjVlEBrnBSWnd5Yt3RU7NcLSzHwt9PY8ORyxBFQCEX8NRdYfi/gW3h5ayUOjwiIiL8LToI2UUVePvnFPzr11T4uqjwcI8QqcMiIjOq3czC2rG4sgNFFVos33EOX+y+AE2VHgBwf9dATI9vjzBvZ4mjIyIiquvZfq2RVVSBz/+4gH9uOAZvFyXua+8ndVhEZCbJ6QUAgJhQ625mAbC4smmVVXqs238JH207i/zSSgBA73AvzBweZfWdWIiIyLbNHNYB2cUa/C/5Cl5YewRfP3cXuoZ4SB0WEZmYVqfHscuFADhyRRZKFEX8eiIT721OxcW8MgBApK8zZgzrgLgOflwcTEREFk8mE/D+I9HIK6nE7rO5mLDyIDZMvhvhPpxxQWRLUq8WQ1Olh7ujAhE2MKNKJnUAZFqHLubjoWV78MLaI7iYVwYfFxXeGdUZv03rj8Ed/VlYERGR1VA6yLD86R7oFOSGvNJKjFt5ADnFGqnDIiITSkqvXm8VE+phE92qOXJlI87llOC9zan47a8sAICjQo7n+rfGpP6t4aLil5mIiKyTi8oBKyf0wsPL9uBSXhkmrjqI/z53F//fRmQjrm8e7CFpHKbC70xWLrdEgw+3nsG6A2nQ6UXIBGB0r1C8HNcOfm5qqcMjIiK6Y36uanw1MRYPL9uD4xmFmLzmML4Y1wtKB07AIbJ21zsF2kY/AH5XsmA6vYj9F/JxOFfA/gv50OlF43PllTosSTyDe9/bjtX7LkGnFxHXwQ+/TeuP+Q91ZWFFREQ2JcLHGSvG94KjQo4/zuTinxuOQV/r/4tEZH3ySyuN/QFibKRhDUeuLNTmE1cx78eTuFpYAUCOr84cQqC7GrPv74hijRaLtpxGVlH1vPOuIe6YOawD+kR6Sxs0ERGRGcWEeuCTp7rj2S8PYWNSBvzcVJg5rIPUYRHRbUquWW8V6esMdyeFxNGYBosrC7T5xFVMXnMEN/4+7mphBV5Yd8T4OMTTEdOHRuGBLoE2sQCQiIjoVga098O/HuqCV787hk93noe/qxoT74mQOiwiug3X11vZxpRAgMWVxdHpRcz78WS9wqo2AcDM4VEYd3c4VA7ylgqNiIjIIjzaMxTZxRq8/9spvPXzSfi6qjAiOkjqsIioma5vHuwhaRymxDVXFubAhfyaqYCNEwF0CfZgYUVERHbrhfsiMbZPGEQR+Mc3R7HnXK7UIRFRM+j1IpJtrFMgwOLK4mQX37ywau55REREtkgQBMwd0QnDOgegUqfH3786jJNXiqQOi4ia6FxOCYo1VXBUyNHe31XqcEyGxZWF8XNtWpe/pp5HRERkq+QyAR+MjkHvCC8Ua6owfuUBpOeXSR0WETWBYb1V1xB3OMhtpySxnTuxEb0jvBDorkZj7SkEAIHuavSO8GrJsIiIiCySWiHH52N7or2/K7KLNRi38gCulVZKHRYR3UJSum3tb2XA4srCyGUC5o7oCAD1CizD47kjOkLO7oBEREQAAHdHBVZN7IUgdzXO55Ri4pcHUV6pkzosIrqJJBtcbwWwuLJIQzsHYtlT3RHgXnfqX4C7Gsue6o6hnQMlioyIiMgyBbo74suJveHuqEBSWgH+779HUKXTSx0WETWgRFOFU1nFAIBuNtQpEGArdos1tHMgBncMwN6z2fj9j/0Y0i8Wfdr4ccSKiIioEW39XfHFuJ548j/7sTUlG7M2ncD8h7pAEPj/TiJLciy9AKIIBHs4ws/NtvoIcOTKgsllAmIjvNDDR0RshBcLKyIiolvoGe6Fj57oBpkAfH0wHR9sPSN1SER0g6Sa/a1sbUogwOKKiIiIbEx8pwC8ObIzAOCjxDNYu/+SxBERUW2G9Va2tHmwgeTF1dKlSxEeHg61Wo3Y2FgcOHCg0XO1Wi3efPNNREZGQq1WIzo6Gps3b65zjk6nw+zZsxEREQFHR0dERkbirbfegiiK5r4VIiIishBP3RWGFwe1BQDM3nQCv/2VKXFERAQAoigi2UY7BQISF1fr169HQkIC5s6diyNHjiA6Ohrx8fHIzs5u8PxZs2bh008/xZIlS3Dy5Ek8//zzGDVqFJKSkoznLFiwAMuWLcPHH3+MlJQULFiwAO+99x6WLFnSUrdFREREFuDluLZ4vFco9CLw4n+TcOhivtQhEdm9y9fKkVtSCYVcQKcgN6nDMTlJi6tFixZh0qRJmDBhAjp27Ijly5fDyckJK1asaPD81atX47XXXsPw4cPRunVrTJ48GcOHD8fChQuN5+zZswcjR47E/fffj/DwcDzyyCMYMmTITUfEiIiIyPYIgoC3H+yMQVF+0FTp8cyXh3CmpkMZEUnjSFr1qFXHIHeoFXKJozE9yboFVlZW4vDhw5g5c6bxmEwmQ1xcHPbu3dvgazQaDdTquh1FHB0dsXv3buPju+++G5999hlOnz6Ndu3a4ejRo9i9ezcWLVrUaCwajQYajcb4uKioCED1NEStVntb92cqhveXOg5bxfyaH3NsXsyveTG/5tVS+f3g0S4Yu+oQktMLMXbFAayf1BuB7rbVoawh/PyaF/N7ew7XjCBHB7vdNHeWlN/mxCCIEi1GunLlCoKDg7Fnzx706dPHeHz69OnYuXMn9u/fX+81Y8aMwdGjR7Fp0yZERkYiMTERI0eOhE6nMxZHer0er732Gt577z3I5XLodDq88847dYq4G73xxhuYN29evePr1q2Dk5OTCe6WiIiIpFSiBT48IUd2hYBARxEvdtbBiRvSELW4RcfluFQiYGxbHXr4WEdPhLKyMowZMwaFhYVwc7v5VEar+rby4YcfYtKkSYiKioIgCIiMjMSECRPqTCP85ptvsHbtWqxbtw6dOnVCcnIypk2bhqCgIIwbN67B686cORMJCQnGx0VFRQgNDcWQIUNumUBz02q12LJlCwYPHgyFQiFpLLaI+TU/5ti8mF/zYn7Nq6Xze3f/coz+/ACuFmvwfY4vVo7tDpUNTksy4OfXvJjf5tNodXjlwDYAIsaPuBehno0PYlhSfg2z2ppCsuLKx8cHcrkcWVlZdY5nZWUhICCgwdf4+vpi06ZNqKioQF5eHoKCgjBjxgy0bt3aeM6rr76KGTNm4PHHHwcAdOnSBZcuXcL8+fMbLa5UKhVUKlW94wqFQvIvpoElxWKLmF/zY47Ni/k1L+bXvFoqvxF+Cqya0BujP92LgxevYfrGv7Dkie42v48kP7/mxfw23bErJdDqRPi4KBHh69akDb4tIb/NeX/JGloolUr06NEDiYmJxmN6vR6JiYl1pgk2RK1WIzg4GFVVVdiwYQNGjhxpfK6srAwyWd3bksvl0Ov1pr0BIiIisjodg9zw6dgeUMpl+OV4Jt788S9u10LUQpJqmlnEhHo2qbCyRpJ2C0xISMDnn3+OL7/8EikpKZg8eTJKS0sxYcIEAMDYsWPrrJXav38/vv/+e5w/fx5//PEHhg4dCr1ej+nTpxvPGTFiBN555x38/PPPuHjxIjZu3IhFixZh1KhRLX5/REREZHnujvTBwseiAQBf7r2EZTvPSRwRkX1ITi8AAHRr5SFpHOYk6Zqr0aNHIycnB3PmzEFmZiZiYmKwefNm+Pv7AwDS0tLqjEJVVFRg1qxZOH/+PFxcXDB8+HCsXr0aHh4exnOWLFmC2bNn44UXXkB2djaCgoLw97//HXPmzGnp2yMiIiILNSI6CDnFGrz500m8t/kUfF1UeLRnqNRhEdm0pLQCAEC3UA9J4zAnyRtaTJ06FVOnTm3wuR07dtR5fO+99+LkyZM3vZ6rqysWL16MxYsXmyhCIiIiskUT74lAVlEFPt11HjO+Pw4fVxUGtPeTOiwim5RdVIGMgnIIAtDVhosrSacFEhEREUnpn0OjMKpbMHR6ES+sOWKctkREppVU899We39XuKgkH98xGxZXREREZLdkMgELHu6Kfm19UK7VYeKqg7iQWyp1WEQ2xzgl0IbXWwEsroiIiMjOKR1kWPZUD3QJdkd+aSXGrtiP7OIKqcMisimGToHdQj0ljsS8WFwRERGR3XNROWDF+F5o5eWE9PxyTFh5ECWaKqnDIrIJVTo9jl0uBMCRKyIiIiK74OuqwlcTe8PbWYm/rhTh+dWHUVnFfTKJ7tSprGKUa3VwVTkg0tdF6nDMisUVERERUY1wH2esGN8LTko5dp/NxfTvjkKv5ybDRHfCsN4qppUHZDLb3DzYgMUVERERUS3RoR745MnucJAJ2JR8Bf/anCp1SERWzdCFM8aGW7AbsLgiIiIiusF97f2w4OGuAIDPdp3Hf/44L3FERNbL2MzCxtdbASyuiIiIiBr0cI8Q/HNoFADg7Z9T8MPRKxJHRGR9Csu0OJdTvb1BjI13CgRYXBERERE16vl7W2P83eEAgH98k4w9Z3OlDYjIyiRfLgAAhHs7wctZKW0wLYDFFREREVEjBEHA7Ac64v4ugdDqRDy3+jD+ulIodVhEVuP6lEDbH7UCWFwRERER3ZRcJmDhY9GIjfBCiaYK41ceRHp+mdRhEVkFQ6dAe1hvBbC4IiIiIroltUKOz8b2RFSAK3KKNRi34gDySyulDovIoun1orFTYDc7WG8FsLgiIiIiahJ3RwVWTeiNYA9HnM8txcRVB1FWWSV1WEQW60JeKQrLtVA5yBAV6Cp1OC2CxRURERFREwW4q/HlxF5wd1QgOb0AU9cloUqnlzosIotkmBLYNcQdCrl9lB32cZdEREREJtLGzxUrxveEykGGbanZeG3jcYiiKHVYRBYnOb26mYU9bB5swOKKiIiIqJl6hHnh4zHdIROAbw5dxqItp6UOicjiXG9mYR/rrQAWV0RERES3ZXBHf7z9YBcAwJJtZ7F63yWJIyKyHGWVVUjNLAZgP50CARZXRERERLdtTGwrTItrCwCY878T2HwiU+KIiCzD8cuF0OlFBLipEejuKHU4LYbFFREREdEdeGlQWzzROxSiCLz4dRIOXsyXOiQiySUZWrDb0agVwOKKiIiI6I4IgoC3RnZGXAd/VFbp8cyqgzidVSx1WESSSkqrbmbB4oqIiIiImsVBLsOSJ7qheysPFFVUYdyKA7hSUC51WESSEEURR+ywmQXA4oqIiIjIJByVcnwxrhcifZ1xtbAC41ceQGGZVuqwiFrclcIK5BRr4CAT0DnIXepwWhSLKyIiIiIT8XRW4suJveHvpsLprBJM+uoQKrQ6qcMialGGKYEdAt3gqJRLHE3LYnFFREREZEIhnk5YNaE3XFUOOHAxH9O+ToZOz02GyX4k10wJtKfNgw1YXBERERGZWIdAN3w2tieUchk2/5WJN374C6LIAovsg712CgRYXBERERGZRZ9Ib3wwOgaCAKzedwmf7DgndUhEZldZpcfxjEIA9tfMAmBxRURERGQ293cNxNwHOgIA3v/tFL45lC5xRETmlXK1CJVVeng4KRDu7SR1OC2OxRURERGRGY3vG4Hn740EAMz8/ji2pWZJHBGR+Rj3twr1gCAIEkfT8lhcEREREZnZP4e2x0Pdg6HTi5iyNsn4AyiRrbm+3sr+pgQCLK6IiIiIzE4QBCx4uCv6t/NFuVaHiasO4nxOidRhEZlcknHzYA9J45AKiysiIiKiFqCQy7Dsye7oGuKOa2VajF1xANnFFVKHRWQyuSUapOWXQRCAaDtsww6wuCIiIiJqMc4qB6wY3wth3k64fK0c41ccRHGFVuqwiEzCsL9VG18XuKkV0gYjERZXRERERC3Ix0WFryb2ho+LEievFuH5NYdRWaWXOiyiO5Zsx/tbGbC4IiIiImphYd7OWDm+N5yUcvx5Ng+vfHsUej03GSbrlpRe3aglJtQ+m1kALK6IiIiIJNElxB3Ln+oBB5mAH45ewbu/pEgdEtFt0+lFHE03bB7sIW0wEmJxRURERCSR/u188d4jXQEA/9l9AZ/vOi9xRES352x2CUo0VXBSytHO31XqcCTD4oqIiIhIQg91D8HMYVEAgHd+ScH/kjMkjoio+Qx7t0WHeEAus7/Ngw1YXBERERFJ7Ln+rTGhbzgA4JVvj2L3mVxpAyJqJnvf38qAxRURERGRxARBwOz7O+L+roHQ6kT8ffUhnMgolDosoiYzNLPo1sp+m1kALK6IiIiILIJMJmDRY9Ho09obpZU6jF95EGl5ZVKHRXRLRRVanMkuAQDE2OnmwQYsroiIiIgshMpBjk/H9kBUgCtySzQYt/IA8ko0UodFdFPH0gshikColyN8XVVShyMpFldEREREFsRNrcCXE3sj2MMRF3JLMXHVQZRVVkkdFlGjkg1TAu14fysDFldEREREFsbfTY2vnukNDycFjl4uxJS1R6DV6aUOi6hBhmYW9j4lEGBxRURERGSRIn1d8MW4XlArZNh+Kgczvz8OURSlDouoDlEUkZReAICdAgEWV0REREQWq0eYJz5+ojtkAvDd4cv49++npA6JqI60/DLkl1ZCKZehY5Cb1OFIjsUVERERkQWL6+iPd0d1AQAs3X4OX+29KG1ARLUYpgR2CnaDykEubTAWgMUVERERkYV7vHcrJAxuBwCY+8Nf+PX4VYkjIqqWlMZmFrWxuCIiIiKyAv83sA3GxLaCKAIvrU/G/vN5UodExPVWN2BxRURERGQFBEHAWyM7Y0hHf1RW6fHsV4dwKrNY6rDIjlVodTh5pQgAiysDyYurpUuXIjw8HGq1GrGxsThw4ECj52q1Wrz55puIjIyEWq1GdHQ0Nm/eXO+8jIwMPPXUU/D29oajoyO6dOmCQ4cOmfM2iIiIiMxOLhPw0RPd0DPME8UVVRi34gCuFJRLHRbZqRMZhajSi/B1VSHYw1HqcCyCpMXV+vXrkZCQgLlz5+LIkSOIjo5GfHw8srOzGzx/1qxZ+PTTT7FkyRKcPHkSzz//PEaNGoWkpCTjOdeuXUPfvn2hUCjw66+/4uTJk1i4cCE8PTkPlIiIiKyfWiHHf8b1RBs/F2QWVWDsigMoKKuUOiyyQ8mGKYGhHhAEQdpgLISkxdWiRYswadIkTJgwAR07dsTy5cvh5OSEFStWNHj+6tWr8dprr2H48OFo3bo1Jk+ejOHDh2PhwoXGcxYsWIDQ0FCsXLkSvXv3RkREBIYMGYLIyMiWui0iIiIis/JwUuLLib0R4KbG2ewSPPvlIVRodVKHRXbGuHkwpwQaOUj1xpWVlTh8+DBmzpxpPCaTyRAXF4e9e/c2+BqNRgO1Wl3nmKOjI3bv3m18/MMPPyA+Ph6PPvoodu7cieDgYLzwwguYNGlSo7FoNBpoNBrj46Ki6rmjWq0WWq32tu7PVAzvL3Uctor5NT/m2LyYX/Nifs2L+b0zfs4O+M/T3fDEFwdx6NI1/N+6I1jyeDTksuoRBObXvJhf4EhNp8CuQa4mz4Ml5bc5MQiiRFt9X7lyBcHBwdizZw/69OljPD59+nTs3LkT+/fvr/eaMWPG4OjRo9i0aRMiIyORmJiIkSNHQqfTGYsjQ/GVkJCARx99FAcPHsRLL72E5cuXY9y4cQ3G8sYbb2DevHn1jq9btw5OTk6muF0iIiIiszhbCCxLkaNKFNDXX49HI/TgDC0ytwINMPeIAwSIWNBbB5UNb3FVVlaGMWPGoLCwEG5uN98oWbKRq9vx4YcfYtKkSYiKioIgCIiMjMSECRPqTCPU6/Xo2bMn3n33XQBAt27dcOLEiZsWVzNnzkRCQoLxcVFREUJDQzFkyJBbJtDctFottmzZgsGDB0OhUEgaiy1ifs2POTYv5te8mF/zYn5Np91fWXhx/VH8mSVDr87t8Hz/COw7l4Ntew9jYJ8euCvS1ziiRaZh75/f3/7KAo4cRfsAN4wa0efWL2gmS8qvYVZbU0hWXPn4+EAulyMrK6vO8aysLAQEBDT4Gl9fX2zatAkVFRXIy8tDUFAQZsyYgdatWxvPCQwMRMeOHeu8rkOHDtiwYUOjsahUKqhUqnrHFQqF5F9MA0uKxRYxv+bHHJsX82tezK95Mb93bkRMCPLLqjD3h7+wOPEsVu65hMJyLQA5vjqTjEB3NeaO6IihnQOlDtXm2Ovn9/iV6m0Auod5mvX+LSG/zXl/yRpaKJVK9OjRA4mJicZjer0eiYmJdaYJNkStViM4OBhVVVXYsGEDRo4caXyub9++OHXqVJ3zT58+jbCwMNPeABEREZEFGXd3OOI7+QNATWF1XWZhBSavOYLNJ65KERrZIEMzi26hHpLGYWkk7RaYkJCAzz//HF9++SVSUlIwefJklJaWYsKECQCAsWPH1ml4sX//fnz//fc4f/48/vjjDwwdOhR6vR7Tp083nvPyyy9j3759ePfdd3H27FmsW7cOn332GaZMmdLi90dERETUUnR6EUcvFzb4nGGB/bwfT0Knl2S5PdkQrU6PYxkFAIBurbjdUW2SrrkaPXo0cnJyMGfOHGRmZiImJgabN2+Gv3/1b13S0tIgk12v/yoqKjBr1iycP38eLi4uGD58OFavXg0PDw/jOb169cLGjRsxc+ZMvPnmm4iIiMDixYvx5JNPtvTtEREREbWYAxfykVlY0ejzIoCrhRU4cCEffSK9Wy4wsjmnMotRodXDTe2A1j7OUodjUSRvaDF16lRMnTq1wed27NhR5/G9996LkydP3vKaDzzwAB544AFThEdERERkFbKLGy+sbuc8osYk1WweHNPKEzI2SqlD0mmBRERERGQafq7qW5/UjPOIGpNUs79VDNdb1cPiioiIiMgG9I7wQqC7Go2NIwgAAt3V6B3h1ZJhkQ1KNjSzaOUhaRyWiMUVERERkQ2QywTMHVG9Hc2NBZbh8dwRHbnfFd2Ra6WVOJ9bCgCICfGQNhgLxOKKiIiIyEYM7RyIZU91R4B73al/7k4KLHuqO/e5ojuWfLkAANDaxxmezkppg7FALK6IiIiIbMjQzoHY/c+BWDOxJzp66AEA97b1YWFFJmHY3yqGUwIbxOKKiIiIyMbIZQJiI7wQF1xdXO04nYsqnV7iqMgWGJpZcH+rhrG4IiIiIrJR4a6Ah6MCheVaHL50TepwyMrp9SKSa9qwd2OnwAaxuCIiIiKyUXIBuLedDwAgMTVb4mjI2p3PLUFxRRXUChmiAlylDscisbgiIiIismED2/sCABJTsiSOhKydYb1V1xAPOMhZRjSEWSEiIiKyYf3aesNBJuBcTiku1rTQJrodSZwSeEssroiIiIhsmKtagdjW1RsHb+XoFd2BJG4efEssroiIiIhs3KAofwBAYgrXXdHtKdVU4VRmEQB2CrwZFldERERENi6uQ3VxdfBiPgrLtRJHQ9bo2OVC6EUgyF0Nfzf1rV9gp1hcEREREdm4Vt5OaOvngiq9iJ2nc6QOh6xQUjr3t2oKFldEREREdmBQB8PUQK67oubjequmYXFFREREZAfiOvgBAHacykGVTi9xNGRNRFFkcdVELK6IiIiI7EC3Vp7wdFKgsFyLQ5euSR0OWZHL18qRW6KBQi6gU5C71OFYNBZXRERERHZALhMwoH316BWnBlJzJNfsb9Ux0A1qhVzaYCwciysiIiIiO3F93RVbslPTGaYExnDz4FticUVERERkJ/q384FCLuB8binO55RIHQ5ZCXYKbDoWV0RERER2wlWtQGyENwCOXlHTaKp0+CvDsHmwh7TBWAEWV0RERER2ZFBN18DEVK67ols7eaUIlTo9vJyVaOXlJHU4Fo/FFREREZEdiatZd3Xw4jUUlmkljoYsnbEFe6gHBEGQNhgrwOKKiIiIyI6Eejmhnb8LdHoRO05zaiDdXFJNp0BOCWwaFldEREREdoZdA6mpktLYzKI5WFwRERER2Zm4mnVXO05lQ6vTSxwNWaqcYg0uXyuHIABdQ7h5cFOwuCIiIiKyMzGhnvByVqKoogqHLl6TOhyyUIbNg9v5ucJVrZA2GCvB4oqIiIjIzshlAga0r+kamMKugdQww5RAbh7cdCyuiIiIiOzQ9ZbsXHdFDTN2CmQziyZjcUVERERkh/q19YFCLuBCbinO5ZRIHQ5ZGJ1exNHLBQDYzKI5WFwRERER2SFXtQJ3tfYGwKmBVN/prGKUVergonJAGz8XqcOxGiyuiIiIiOzUoKjqqYFb2ZKdbmCYEhgd6g65jJsHNxWLKyIiIiI7Zdjv6vClaygoq5Q4GrIkxv2tQjklsDlYXBERERHZqVAvJ7T3d4VOL2LHqRypwyELklTThp3NLJqHxRURERGRHWPXQLpRYbkWZ7Orm5ywDXvzsLgiIiIismOGqYE7TmVDq9NLHA1ZgmM1XQLDvJ3g7aKSNhgrw+KKiIiIyI7FhHrA21mJ4ooqHLyYL3U4ZAEMzSw4atV8LK6IiIiI7JhcJmBATdfARHYNJNRuZuEhbSBWiMUVERERkZ2LM6y7SsmCKIoSR0NSEkWxVjMLdgpsLhZXRERERHauX1tfKOUyXMwrw7mcUqnDIQldzCtDQZkWSgcZOgS6SR2O1WFxRURERGTnnFUOuCvSG0D16BXZL8OUwC7B7lA6sFRoLmaMiIiIiDCI664I15tZcL3V7WFxRURERETG/a4OXcrHtdJKiaMhqSSl1zSz4Hqr28LiioiIiIgQ4umEqABX6EVgx2mOXtmj8kodUq4WAwC6tfKQNhgrxeKKiIiIiABcH73ayqmBdunElULo9CL83VQIdFdLHY5VYnFFRERERACAQR38AQC7TuWgskovcTTU0q7vb+UJQRAkjsY6sbgiIiIiIgBATIgHfFyUKNZU4eDFfKnDoRZmaGYRwymBt43FFREREREBAGQyAQPaG6YGsiW7vWGnwDvH4oqIiIiIjAxTAxNTsiGKosTRUEu5WliOzKIKyGUCuoS4Sx2O1bKI4mrp0qUIDw+HWq1GbGwsDhw40Oi5Wq0Wb775JiIjI6FWqxEdHY3Nmzc3ev6//vUvCIKAadOmmSFyIiIiItvSr60PlHIZ0vLLcC6nROpwqIUYRq2iAlzhpHSQNhgrJnlxtX79eiQkJGDu3Lk4cuQIoqOjER8fj+zshrvUzJo1C59++imWLFmCkydP4vnnn8eoUaOQlJRU79yDBw/i008/RdeuXc19G0REREQ2wVnlgD6R3gDYNdCeGJtZcL3VHZG8uFq0aBEmTZqECRMmoGPHjli+fDmcnJywYsWKBs9fvXo1XnvtNQwfPhytW7fG5MmTMXz4cCxcuLDOeSUlJXjyySfx+eefw9OTm6ARERERNVVcTUv2RK67shvX11vx5+Y7IemYX2VlJQ4fPoyZM2caj8lkMsTFxWHv3r0Nvkaj0UCtrtt339HREbt3765zbMqUKbj//vsRFxeHt99++6ZxaDQaaDQa4+OioiIA1VMQtVpts+7J1AzvL3Uctor5NT/m2LyYX/Nifs2L+TWvO8lv/zZeAIDDl64hu7AUnk5Kk8ZmC2zp86vV6XE8oxAA0CXIxSLuyZLy25wYml1chYeHY+LEiRg/fjxatWrV3JfXkZubC51OB39//zrH/f39kZqa2uBr4uPjsWjRIvTv3x+RkZFITEzE999/D51OZzzn66+/xpEjR3Dw4MEmxTF//nzMmzev3vHff/8dTk5Ozbgj89myZYvUIdg05tf8mGPzYn7Ni/k1L+bXvG43v8FOcmSUCfjo20T08mVji8bYwuc3vQTQVDnASS7i5P6dSLGgLa4sIb9lZWVNPrfZxdW0adOwatUqvPnmmxgwYACeeeYZjBo1CiqVqrmXui0ffvghJk2ahKioKAiCgMjISEyYMME4jTA9PR0vvfQStmzZUm+EqzEzZ85EQkKC8XFRURFCQ0MxZMgQuLm5meU+mkqr1WLLli0YPHgwFAqFpLHYIubX/Jhj82J+zYv5NS/m17zuNL+pyjNYtvMC8lRBGD482gwRWjdb+vyu2Z8GHE9Fr9a+uP/+7lKHA8Cy8muY1dYUt1VcTZs2DUeOHMGqVavwf//3f3jhhRcwZswYTJw4Ed27N/0L4uPjA7lcjqysuvN5s7KyEBAQ0OBrfH19sWnTJlRUVCAvLw9BQUGYMWMGWrduDQA4fPgwsrOz68Sh0+mwa9cufPzxx9BoNJDL5XWuqVKpGiwOFQqF5F9MA0uKxRYxv+bHHJsX82tezK95Mb/mdbv5HdIpEMt2XsAfZ/MgCnIoHSRfqm+RbOHzeyyjGADQLczT4u7FEvLbnPe/7f9Kunfvjo8++ghXrlzB3Llz8Z///Ae9evVCTEwMVqxY0aR9EZRKJXr06IHExETjMb1ej8TERPTp0+emr1Wr1QgODkZVVRU2bNiAkSNHAgAGDRqE48ePIzk52finZ8+eePLJJ5GcnFyvsCIiIiKi+qJDPODjokKJpgoHLuRLHQ6Z0fVOgWxmcaduu6GFVqvFxo0bsXLlSmzZsgV33XUXnnnmGVy+fBmvvfYatm7dinXr1t3yOgkJCRg3bhx69uyJ3r17Y/HixSgtLcWECRMAAGPHjkVwcDDmz58PANi/fz8yMjIQExODjIwMvPHGG9Dr9Zg+fToAwNXVFZ07d67zHs7OzvD29q53nIiIiIgaJpMJGBjli28OXcbWlCzc09ZH6pDIDPJLK3Exr3pNUUyIh7TB2IBmF1dHjhzBypUr8d///hcymQxjx47FBx98gKioKOM5o0aNQq9evZp0vdGjRyMnJwdz5sxBZmYmYmJisHnzZmOTi7S0NMhk1wfYKioqMGvWLJw/fx4uLi4YPnw4Vq9eDQ8Pj+beChERERHdxKAO/vjm0GUkpmZh7oiOEAQL6nRAJpGcXj1qFenrDHcny5oSaI2aXVz16tULgwcPxrJly/Dggw82OAcxIiICjz/+eJOvOXXqVEydOrXB53bs2FHn8b333ouTJ082K+Ybr0FEREREt9avrQ+UDjKk55fjTHYJ2vm7Sh0SmZhxfytOCTSJZhdX58+fR1hY2E3PcXZ2xsqVK287KCIiIiKSnpPSAXdHemPHqRxsTclicWWDrhdXHpLGYSua3dAiOzsb+/fvr3d8//79OHTokEmCIiIiIiLLMKhD9VKNbSnZEkdCpqbXiziaXgAA6BbKkStTaHZxNWXKFKSnp9c7npGRgSlTppgkKCIiIiKyDIOi/AAAR9KuIb+0UuJoyJTO5ZSgWFMFJ6Uc7fxdpA7HJjS7uDp58mSDe1l169at2WuhiIiIiMiyBXk4omOgG/QisD2Vo1e2xDAlsGuIOxzk3MfMFJqdRZVKVW/TXwC4evUqHBxuu7M7EREREVmouA7Vo1eJqfV/BiTrlVTTKTCGUwJNptnF1ZAhQzBz5kwUFhYajxUUFOC1117D4MGDTRocEREREUnPsO5q1+lcVFbpJY6GTIXNLEyv2UNN//73v9G/f3+EhYWhW7duAIDk5GT4+/tj9erVJg+QiIiIiKTVJdgdvq4q5BRrsP9CHvq19ZU6JLpDJZoqnMoqBgB0C/WQNhgb0uyRq+DgYBw7dgzvvfceOnbsiB49euDDDz/E8ePHERoaao4YiYiIiEhCMpmAge1rpgaya6BNOJZeAFEEgj0c4eemljocm3Fbi6ScnZ3x3HPPmToWIiIiIrJQgzr4Yf2hdGxNycLcER0hCILUIdEdSDK0YOeUQJO67Q4UJ0+eRFpaGior67bk/Nvf/nbHQRERERGRZbmnrQ+UDjJcvlaO01klaB/ADYWtWVJadTOLbq3YzMKUml1cnT9/HqNGjcLx48chCAJEUQQA428vdDqdaSMkIiIiIsk5KR3QN9Ib20/lYGtKFosrKyaKIptZmEmz11y99NJLiIiIQHZ2NpycnPDXX39h165d6NmzJ3bs2GGGEImIiIjIEhi6BiamsCW7Nbt8rRx5pZVQymXoFOQmdTg2pdnF1d69e/Hmm2/Cx8cHMpkMMpkM99xzD+bPn48XX3zRHDESERERkQUYVLPfVVJ6AXJLNBJHQ7frSM2UwI5BblA5yCWOxrY0u7jS6XRwda0eBvbx8cGVK1cAAGFhYTh16pRpoyMiIiIiixHo7ohOQW4QRWB7KrsGWivDlMAYtmA3uWYXV507d8bRo0cBALGxsXjvvffw559/4s0330Tr1q1NHiARERERWQ7D1MBtLK6sFjsFmk+zi6tZs2ZBr6/emfvNN9/EhQsX0K9fP/zyyy/46KOPTB4gEREREVmOuJqpgbtO50BTxUZm1qZCq8PJK4UAgO7sFGhyze4WGB8fb/x3mzZtkJqaivz8fHh6enK/AyIiIiIb1znIHX6uKmQXa7D/fD76t/OVOiRqhr+uFEGrE+HjokSIp6PU4dicZo1cabVaODg44MSJE3WOe3l5sbAiIiIisgMymWBsbMGugdbHsL9VTCgHRsyhWcWVQqFAq1atuJcVERERkR0bFFW97mprSrZxz1OyDlxvZV7NXnP1+uuv47XXXkN+fr454iEiIiIiC9e3jQ9UDjJkFJTjVFax1OFQMyRz82Czavaaq48//hhnz55FUFAQwsLC4OzsXOf5I0eOmCw4IiIiIrI8jko5+rbxwbbUbCSmZCMqgBvRWoPsogpkFJRDJgBdQzykDscmNbu4evDBB80QBhERERFZk0Ed/LAtNRtbU7IwZUAbqcOhJjBMCWzn7woXVbPLAGqCZmd17ty55oiDiIiIiKzIoCh/vI4TSE4vQG6JBj4uKqlDoltI4pRAs2v2misiIiIiogB3NToHu0EUuaGwtTB0CuwWyv2tzKXZxZVMJoNcLm/0DxERERHZB0PXQLZkt3xVOj2OXa7ePJgjV+bT7GmBGzdurPNYq9UiKSkJX375JebNm2eywIiIiIjIssV18MeHiWfwx5lcVGh1UCv4i3ZLdSqrGOVaHVxVDoj0dZE6HJvV7OJq5MiR9Y498sgj6NSpE9avX49nnnnGJIERERERkWXrHOwGfzcVsoo02Hc+D/e195M6JGqEYb1VTCsPyGTcPNhcTLbm6q677kJiYqKpLkdEREREFk4QBAw0Tg3kuitLZmxmEeohaRy2ziTFVXl5OT766CMEBweb4nJEREREZCXiOlSPVm1LzYYoihJHQ41JSq9pZtGKzSzMqdnTAj09PSEI14cSRVFEcXExnJycsGbNGpMGR0RERESWrW8bH6gVMmQUlCM1sxgdArmhsKUpLNPifE4pACCGI1dm1ezi6oMPPqhTXMlkMvj6+iI2NhaenqyEiYiIiOyJWiHHPW18sDUlG4kpWSyuLFDy5QIAQISPMzydldIGY+OaXVyNHz/eDGEQERERkbUa1MEfW1OysTUlG1MHtpU6HLqBYX8rjlqZX7PXXK1cuRLffvttvePffvstvvzyS5MERURERETWY2BU9bqro5cLkFOskTgaupGxmQX3tzK7ZhdX8+fPh4+PT73jfn5+ePfdd00SFBERERFZD383NboEu0MUge2p7BpoSfR6EcnpBQCAbqFcwmNuzS6u0tLSEBERUe94WFgY0tLSTBIUEREREVmXQTVdA7emZEkcCdV2Ia8UheVaqBxkiAp0lTocm9fs4srPzw/Hjh2rd/zo0aPw9vY2SVBEREREZF3iOlTvd/XHmVxUaHUSR0MGhimBXUPcoZCbbItbakSzM/zEE0/gxRdfxPbt26HT6aDT6bBt2za89NJLePzxx80RIxERERFZuE5BbghwU6Ncq8Pe83lSh0M1DM0suL9Vy2h2cfXWW28hNjYWgwYNgqOjIxwdHTFkyBAMHDiQa66IiIiI7JQgCBhYMzUwkVMDLYaxmQU7BbaIZrdiVyqVWL9+Pd5++20kJyfD0dERXbp0QVhYmDniIyIiIiIrEdfBD+v2p2FbSjbEkWKdvVGp5ZVVViE1swgAR65aSrOLK4O2bduibVvuY0BERERE1e6O9IFaIcOVwgqcvFqETkHuUodk145fLoReBALd1QhwV0sdjl1o9rTAhx9+GAsWLKh3/L333sOjjz5qkqCIiIiIyPqoFXLc08YXAJCYwpbsUkuqacHOzYNbTrOLq127dmH48OH1jg8bNgy7du0ySVBEREREZJ3iDOuuuN+V5K43s/CQNhA70uziqqSkBEqlst5xhUKBoqIikwRFRERERNZpYFR1cXU0vQDZxRUSR2O/RFHEEUMzC663ajHNLq66dOmC9evX1zv+9ddfo2PHjiYJioiIiIisk5+bGtEh1WuttnP0SjJXCiuQU6yBg0xAZ659azHNbmgxe/ZsPPTQQzh37hwGDhwIAEhMTMS6devw3XffmTxAIiIiIrIugzr44+jlQmxNycboXq2kDscuGaYEdgh0g6NSLnE09qPZI1cjRozApk2bcPbsWbzwwgv4xz/+gYyMDGzbtg1t2rQxR4xEREREZEUMUwN3n8lFhVYncTT2ybi/FddbtahmF1cAcP/99+PPP/9EaWkpzp8/j8ceewyvvPIKoqOjTR0fEREREVmZTkFuCHRXo1yrw95zeVKHY5fYzEIat1VcAdVdA8eNG4egoCAsXLgQAwcOxL59+0wZGxERERFZIUEQjKNXW1OyJI7G/lRW6XHiSs3mwaFsZtGSmrXmKjMzE6tWrcIXX3yBoqIiPPbYY9BoNNi0aRObWRARERGRUVwHf6zdn4ZtqdkQRRGCIEgdkt1IuVqEyio9PJ0UCPN2kjocu9LkkasRI0agffv2OHbsGBYvXowrV65gyZIl5oyNiIiIiKxUn0hvOCrkuFpYgb+ucLuelmSYEhgT6sGitoU1ubj69ddf8cwzz2DevHm4//77IZebruvI0qVLER4eDrVajdjYWBw4cKDRc7VaLd58801ERkZCrVYjOjoamzdvrnPO/Pnz0atXL7i6usLPzw8PPvggTp06ZbJ4iYiIiOjm1Ao57mnrAwBITGFL9paUlF4AgPtbSaHJxdXu3btRXFyMHj16IDY2Fh9//DFyc3PvOID169cjISEBc+fOxZEjRxAdHY34+HhkZzf8H+GsWbPw6aefYsmSJTh58iSef/55jBo1CklJScZzdu7ciSlTpmDfvn3YsmULtFothgwZgtLS0juOl4iIiIiaJq5D9bqrxFSuu2pJ7BQonSavubrrrrtw1113YfHixVi/fj1WrFiBhIQE6PV6bNmyBaGhoXB1dW12AIsWLcKkSZMwYcIEAMDy5cvx888/Y8WKFZgxY0a981evXo3XX38dw4cPBwBMnjwZW7duxcKFC7FmzRoAqDeStWrVKvj5+eHw4cPo379/vWtqNBpoNBrj46Ki6qFrrVYLrVbb7HsyJcP7Sx2HrWJ+zY85Ni/m17yYX/Nifs3LEvLbL9ILAHDsciEu5xXD300tWSymZgn5bUheiQZp+WUQBKBTgLPFxddUlpTf5sQgiKIo3u4bnTp1Cl988QVWr16NgoICDB48GD/88EOTX19ZWQknJyd89913ePDBB43Hx40bh4KCAvzvf/+r9xpvb2+89957eOaZZ4zHnnrqKezevRsXL15s8H3Onj2Ltm3b4vjx4+jcuXO959944w3Mmzev3vF169bByYmLAImIiIhu16LjclwqETC6tQ53+9/2j53URCfyBXx+So4ARxEzY7jHmCmUlZVhzJgxKCwshJub203PvaPiykCn0+HHH3/EihUrmlVcXblyBcHBwdizZw/69OljPD59+nTs3LkT+/fvr/eaMWPG4OjRo9i0aRMiIyORmJiIkSNHQqfT1Rl9MtDr9fjb3/6GgoIC7N69u8E4Ghq5Cg0NRW5u7i0TaG5arRZbtmzB4MGDoVAoJI3FFjG/5sccmxfza17Mr3kxv+ZlKflduuM8FieexcD2vvj0qW6SxWFqlpLfGy3acgbLdl3AI92DMX9UJ6nDuW2WlN+ioiL4+Pg0qbhqViv2xsjlcjz44IN1Rp/M5cMPP8SkSZMQFRUFQRAQGRmJCRMmYMWKFQ2eP2XKFJw4caLRwgoAVCoVVCpVveMKhULyL6aBJcVii5hf82OOzYv5NS/m17yYX/OSOr9DOgViceJZ7DmfBx1kUCtM1xTNEkid3xsdzahe3tIj3Mui4rpdlpDf5rz/bW8ibAo+Pj6Qy+XIyqq7yDErKwsBAQENvsbX1xebNm1CaWkpLl26hNTUVLi4uKB169b1zp06dSp++uknbN++HSEhIWa5ByIiIiJqXIdAVwS5q1Gh1WPPuTtvhkaN0+lFHDV2CvSQNBZ7JWlxpVQq0aNHDyQmJhqP6fV6JCYm1pkm2BC1Wo3g4GBUVVVhw4YNGDlypPE5URQxdepUbNy4Edu2bUNERITZ7oGIiIiIGicIAgbWdA3cypbsZnU2uwSllTo4K+Vo69f8RnN05yQtrgAgISEBn3/+Ob788kukpKRg8uTJKC0tNXYPHDt2LGbOnGk8f//+/fj+++9x/vx5/PHHHxg6dCj0ej2mT59uPGfKlClYs2YN1q1bB1dXV2RmZiIzMxPl5eUtfn9ERERE9m5QB38AwLaUbJhguT81wrB5cHSoB+Qybh4sBZOsuboTo0ePRk5ODubMmYPMzEzExMRg8+bN8Pev/o8wLS0NMtn1GrCiogKzZs3C+fPn4eLiguHDh2P16tXw8PAwnrNs2TIAwH333VfnvVauXInx48eb+5aIiIiIqJY+rb3hpJQjs6gCf10pQudgd6lDskmG/a1iQj0kjcOeSV5cAdVro6ZOndrgczt27Kjz+N5778XJkydvej3+RoSIiIjIcqgVctzTxge/n8zC1pQsFldmkpRePXLVrZWnxJHYL8mnBRIRERGR7YurmRqYyHVXZlFUocWZ7BIAHLmSEosrIiIiIjK7AVF+EATgeEYhsooqpA7H5hxLL4QoAqFejvB1rb/FELUMFldEREREZHa+ripEh3gA4OiVORiaWXQL5ZRAKbG4IiIiIqIWEVfTkj0xJesWZ1JzJXF/K4vA4oqIiIiIWoShJfvus7kor9RJHI3tEEXx+sgVm1lIisUVEREREbWIqABXBHs4QlOlx59nc6UOx2ak5ZfhWpkWSgcZOga6SR2OXWNxRUREREQtQhAEDDJMDUzl1EBTMexv1TnIDUoH/ngvJWafiIiIiFrMoFot2bk3qWkYpgTGsJmF5FhcEREREVGLuau1F5yUcmQXa3Aio0jqcGwCm1lYDhZXRERERNRiVA5y9GvrAwDYyq6Bd6xCq8PJK9VFKosr6bG4IiIiIqIWZZwayHVXd+xERiGq9CJ8XVUI9nCUOhy7x+KKiIiIiFrUwCg/CAJwIqMImYUVUodj1QzNLLqFekAQBGmDIRZXRERERNSyfFxUiAn1AMDRqzuVlM79rSwJiysiIiIianFxtboG0u0zjlxxvZVFYHFFRERERC3OsN/Vn2dzUV6pkzga65RZWIGrhRWQCUDXEHepwyGwuCIiIiIiCbT3d0WwhyM0VXrsPpsrdThWKblmSmBUgBuclA4SR0MAiysiIiIikoAgCIirGb1KZEv222KYEhjDKYEWg8UVEREREUniekv2bOj1osTRWJ/anQLJMrC4IiIiIiJJxLb2grNSjpxiDY5nFEodjlXR6vQ4llEAgJ0CLQmLKyIiIiKShMpBjv7tfAFwamBzncosRoVWDze1A1r7OEsdDtVgcUVEREREkqk9NZCaLimtuplFTCtPyGTcPNhSsLgiIiIiIskMaO8LQQD+ulKEq4XlUodjNbjeyjKxuCIiIiIiyXi7qIwFAjcUbrqk9AIA3DzY0rC4IiIiIiJJGacGct1Vk1wrrcSF3FIAQAxHriwKiysiIiIiklRcTXH157k8lFVWSRyN5Uu+XAAAaO3rDA8npbTBUB0sroiIiIhIUu38XRDi6YjKKj12n8mVOhyLZ9w8mKNWFofFFRERERFJShAE4+gV113dmqFTIPe3sjwsroiIiIhIcoM6+AGobsmu14sSR2O59HoRyYZmFhy5sjgsroiIiIhIcrER3nBROSC3RINjGYVSh2OxzueWoLiiCmqFDFEBrlKHQzdgcUVEREREklM6yNC/nQ8Adg28mSM16626hnjAQc4f5S0NvyJEREREZBEGRVWvu9rKdVeNMm4ezP2tLBKLKyIiIiKyCAOi/CATgJSrRcgoKJc6HItkbGYRymYWlojFFRERERFZBC9nJbrXdMDbxqmB9ZRqqnA6qxgAR64sFYsrIiIiIrIYgzpwamBjjl0uhF4Egj0c4e+mljocagCLKyIiIiKyGIaW7HvP5aFUUyVxNJYlKb16SiA3D7ZcLK6IiIiIyGK09XNBqJcjKnV67D6bK3U4FoXNLCwfiysiIiIishiCIBi7BrIl+3WiKLK4sgIsroiIiIjIosTVrLvalpoDvV6UOBrLcPlaOXJLNFDIBXQKcpc6HGoEiysiIiIisii9I7zgqnJAbokGRy8XSB2ORUhKLwAAdAx0g1ohlzYYahSLKyIiIiKyKEoHGfq38wUAJLJrIIBa+1u14v5WlozFFRERERFZHEPXwK1cdwWAzSysBYsrIiIiIrI4A9r7QSYAqZnFuHytTOpwJKWp0uHklSIAQLdQjlxZMhZXRERERGRxPJ2V6BFWXUhsS7XvqYEnrxShUqeHt7MSoV6OUodDN8HiioiIiIgs0qCaroFb7XzdlWFKYEyoBwRBkDYYuikWV0RERERkkeJq1l3tO5eHEk2VxNFIx9ApkOutLB+LKyIiIiKySJG+LgjzdkKlTo/dZ3KkDkcy7BRoPVhcEREREZFFEgQBg6Lse2pgdnEFLl8rhyAAXUO4ebClY3FFRERERBbL0JJ9e2o29HpR4mhaXnLNeqt2fq5wVSukDYZuicUVEREREVmsXuFecFU5IK+0EsmXC6QOp8VxvZV1sYjiaunSpQgPD4darUZsbCwOHDjQ6LlarRZvvvkmIiMjoVarER0djc2bN9/RNYmIiIjIMikdZOjf3hcAkGiHGwpfX2/lIW0g1CSSF1fr169HQkIC5s6diyNHjiA6Ohrx8fHIzm54Xu2sWbPw6aefYsmSJTh58iSef/55jBo1CklJSbd9TSIiIiKyXIaugYl2tu5Kpxdx7HIhADazsBaSF1eLFi3CpEmTMGHCBHTs2BHLly+Hk5MTVqxY0eD5q1evxmuvvYbhw4ejdevWmDx5MoYPH46FCxfe9jWJiIiIyHLd184PMgFIzSzG5WtlUofTYk5nFaOsUgdXlQPa+LpIHQ41gYOUb15ZWYnDhw9j5syZxmMymQxxcXHYu3dvg6/RaDRQq9V1jjk6OmL37t13dE2NRmN8XFRUBKB6CqJWq729mzMRw/tLHYetYn7Njzk2L+bXvJhf82J+zcuW8uuiFNC9lQcOXSrA7yeu4um7WkkdUovk99CFPABAl2A36HRV0OnM9lYWx5I+v82JQdLiKjc3FzqdDv7+/nWO+/v7IzU1tcHXxMfHY9GiRejfvz8iIyORmJiI77//HrqaT9vtXHP+/PmYN29eveO///47nJycbufWTG7Lli1Sh2DTmF/zY47Ni/k1L+bXvJhf87KV/AaJAgA5vtl9Et75J6QOx8ic+f3prAyADC6aXPzyyy9mex9LZgmf37Kypo+WSlpc3Y4PP/wQkyZNQlRUFARBQGRkJCZMmHBHU/5mzpyJhIQE4+OioiKEhoZiyJAhcHNzM0XYt02r1WLLli0YPHgwFAq23zQ15tf8mGPzYn7Ni/k1L+bXvGwtv+1zSvHDR3/iXIkc/QfFwUUl7Y+xLZHfjz76E0ApHh7QAwNrmnrYC0v6/BpmtTWFpJ9KHx8fyOVyZGXV7fySlZWFgICABl/j6+uLTZs2oaKiAnl5eQgKCsKMGTPQunXr276mSqWCSqWqd1yhUEj+xTSwpFhsEfNrfsyxeTG/5sX8mhfza162kt/2ge4I93bCxbwy7LtQgGFdAqUOCYD58ltYrsW5nFIAQM9wb5v4Gt4OS/j8Nuf9JW1ooVQq0aNHDyQmJhqP6fV6JCYmok+fPjd9rVqtRnBwMKqqqrBhwwaMHDnyjq9JRERERJZJEAQM6lC97GOrHXQNPFqzv1WYtxO8XeoPApBlkrxbYEJCAj7//HN8+eWXSElJweTJk1FaWooJEyYAAMaOHVunOcX+/fvx/fff4/z58/jjjz8wdOhQ6PV6TJ8+vcnXJCIiIiLrM6imJfv2U9nQ6UWJozGvpLQCAEC3UA9J46DmkXzN1ejRo5GTk4M5c+YgMzMTMTEx2Lx5s7EhRVpaGmSy6zVgRUUFZs2ahfPnz8PFxQXDhw/H6tWr4eHh0eRrEhEREZH16RXuBVe1A/JLK5Gcfg09wrykDslsktINmwdzfytrInlxBQBTp07F1KlTG3xux44ddR7fe++9OHny5B1dk4iIiIisj0Iuw33t/fDj0SvYmpJts8WVKIpIrpkW2K2Vh6SxUPNIPi2QiIiIiKipBkVVTw3cZsPrri7mlaGgTAuVgwxRAdJ2rqbmYXFFRERERFbjvva+kMsEnMoqRnp+0/cfsiZJadVTArsEu0PpwB/XrQm/WkRERERkNTyclOgRVr0OKTEl6xZnWydDM4sYNrOwOiyuiIiIiMiqxNV0DUxMtc2pgWxmYb1YXBERERGRVTHsd7XvfB6KK7QSR2Na5ZU6pFwtBsBmFtaIxRURERERWZVIXxdE+DhDqxPxx5lcqcMxqeMZhdDpRfi7qRDorpY6HGomFldEREREZHUMXQO32ti6K0Mzi26hnhAEQeJoqLlYXBERERGR1TFMDdxxKgc6vShxNKZjaGbBKYHWicUVEREREVmdnuGecFM7IL+00jjaYwuubx7MZhbWiMUVEREREVkdhVyG+9obpgbaRtfAq4XlyCyqgFwmoEuwu9Th0G1gcUVEREREVmmQoSW7jay7MkwJ7BDoCkelXNpg6LawuCIiIiIiq3RfOz/IZQLOZJcgLa9M6nDumGF6IzcPtl4sroiIiIjIKrk7KdAzrHptki10DTQ2swjleitrxeKKiIiIiKxWXE3XwMRU6y6uKqv0OJ5RCICdAq0ZiysiIiIislqGdVf7z+ejuEIrcTS3LzWzCJoqPdwdFYjwcZY6HLpNLK6IiIiIyGq19nVBax9nVOlF7DqdK3U4t632/lbcPNh6sbgiIiIiIqtmC10DDc0suN7KurG4IiIiIiKrNqhm3dX2U9nQ6UWJo7k91zcP9pA0DrozLK6IiIiIyKr1DPOEu6MC18q0OFIzAmRN8ksrcbGmlXw027BbNRZXRERERGTVHOQy3NfeF4B1tmRPTq8uCNv4ucDdUSFxNHQnWFwRERERkdUzTA1MTMmWOJLmMzSz4ObB1o/FFRERERFZvXvb+cJBJuBsdgku5ZVKHU6z1O4USNaNxRURERERWT13RwV6hXsBALZa0eiVTi9eb2bBToFWj8UVEREREdkEa2zJfi6nBCWaKjgp5Wjn7yJ1OHSHWFwRERERkU2Iq1l3deBCPooqtBJH0zSG/a26hrjDQc4fza0dv4JEREREZBPCfZzR2tcZVXoRO0/lSB1Ok1xfb8UpgbaAxRURERER2Yw4Y9dA65gaeH29lYekcZBpsLgiIiIiIpsxKKp63dWO0zmo0ukljubmSjRVOJVVDACIYadAm8DiioiIiIhsRo8wT7g7KlBQpsWRmil3lupYegFEEQjxdISfq1rqcMgEWFwRERERkc1wkMswoL0vAMufGphUMyWQmwfbDhZXRERERGRTBtWsu9pq6cVVTadANrOwHSyuiIiIiMim3NveFw4yAedySnExt1TqcBokimKtToEeksZCpsPiioiIiIhsiptagd4RXgAsd/QqPb8ceaWVUMpl6BTkJnU4ZCIsroiIiIjI5gwytmTPljiShiWlV08J7BjkBpWDXOJoyFRYXBERERGRzYnrUN2S/eDFfBSWayWOpj5OCbRNLK6IiIiIyOaEeTujjZ8LqvQidp7OkTqcegydAtnMwrawuCIiIiIimzSoZvTK0lqyV2h1OHmlEADQjW3YbQqLKyIiIiKySXE16652nMpBlU4vcTTX/XWlCFqdCB8XFUI8HaUOh0yIxRURERER2aRuoR7wcFKgsFyLQ5euSR2OkWF/q5hQDwiCIHE0ZEosroiIiIjIJjnIZRjQ3vKmBl5fb+UhaRxkeiyuiIiIiMhmGdddpVpOS/Zkdgq0WSyuiIiIiMhm9W/nCweZgPM5pbiQWyp1OMgqqkBGQTlkAtA1xEPqcMjEWFwRERERkc1yUysQ29oLgGVMDTTsb9XO3xUuKgdpgyGTY3FFRERERDZtUFR118CtllBcpVc3s+D+VraJxRURERER2TRDS/aDF6+hsEwraSxcb2XbWFwRERERkU1r5e2Etn4u0OlF7DgtXWOLKp0exy5Xbx7cncWVTWJxRUREREQ2b1DN6FViinTF1amsYpRrdXBVO6C1j4tkcZD5sLgiIiIiIpsXV9OSfcepbGh1ekliMDSziAn1gEzGzYNtEYsrIiIiIrJ53Vp5wstZiaKKKhy6eE2SGAzFVbdQD0nen8xP8uJq6dKlCA8Ph1qtRmxsLA4cOHDT8xcvXoz27dvD0dERoaGhePnll1FRUWF8XqfTYfbs2YiIiICjoyMiIyPx1ltvQRRFc98KEREREVkouUzAfe19AUjXkp2dAm2fpMXV+vXrkZCQgLlz5+LIkSOIjo5GfHw8srMbngu7bt06zJgxA3PnzkVKSgq++OILrF+/Hq+99prxnAULFmDZsmX4+OOPkZKSggULFuC9997DkiVLWuq2iIiIiMgCGboGJqa2/LqrgrJKnM+p3sQ4hiNXNkvS4mrRokWYNGkSJkyYgI4dO2L58uVwcnLCihUrGjx/z5496Nu3L8aMGYPw8HAMGTIETzzxRJ3Rrj179mDkyJG4//77ER4ejkceeQRDhgy55YgYEREREdm2fm19oJALuJBbinM5JS363snpBQCACB9neDorW/S9qeVIti10ZWUlDh8+jJkzZxqPyWQyxMXFYe/evQ2+5u6778aaNWtw4MAB9O7dG+fPn8cvv/yCp59+us45n332GU6fPo127drh6NGj2L17NxYtWtRoLBqNBhqNxvi4qKgIAKDVaqHVSrsXguH9pY7DVjG/5sccmxfza17Mr3kxv+bF/NanlgO9wj2x51w+fj9xFc/eE37b12pufg9fzAMARAe78WvSBJb0+W1ODJIVV7m5udDpdPD3969z3N/fH6mpqQ2+ZsyYMcjNzcU999wDURRRVVWF559/vs60wBkzZqCoqAhRUVGQy+XQ6XR455138OSTTzYay/z58zFv3rx6x3///Xc4OTnd5h2a1pYtW6QOwaYxv+bHHJsX82tezK95Mb/mxfzWFaATAMjx3Z5TCCo6ecfXa2p+t6bIAMjgUHQZv/ySfsfvay8s4fNbVlbW5HMlK65ux44dO/Duu+/ik08+QWxsLM6ePYuXXnoJb731FmbPng0A+Oabb7B27VqsW7cOnTp1QnJyMqZNm4agoCCMGzeuwevOnDkTCQkJxsdFRUUIDQ3FkCFD4Obm1iL31hitVostW7Zg8ODBUCgUksZii5hf82OOzYv5NS/m17yYX/NifhvW5VoZvl+0GxdLZbj7voHwcLq93DQnv3q9iDnJ2wFU4amhfdEpSNqfL62BJX1+DbPamkKy4srHxwdyuRxZWXW7tWRlZSEgIKDB18yePRtPP/00nn32WQBAly5dUFpaiueeew6vv/46ZDIZXn31VcyYMQOPP/648ZxLly5h/vz5jRZXKpUKKpWq3nGFQiH5F9PAkmKxRcyv+THH5sX8mhfza17Mr3kxv3W19nNHO38XnM4qwZ4L1zAyJviOrteU/J7LKUFheRXUChk6hXhCIZe8YbfVsITPb3PeX7KvrFKpRI8ePZCYmGg8ptfrkZiYiD59+jT4mrKyMshkdUOWy+UAYGy13tg5er00m8URERERkWUZVNM1cGtKy3QNNOxv1SXYnYWVjZN0WmBCQgLGjRuHnj17onfv3li8eDFKS0sxYcIEAMDYsWMRHByM+fPnAwBGjBiBRYsWoVu3bsZpgbNnz8aIESOMRdaIESPwzjvvoFWrVujUqROSkpKwaNEiTJw4UbL7JCIiIiLLEdfBD8t2nMOOU9nQ6vRmL3iS0ri/lb2QtLgaPXo0cnJyMGfOHGRmZiImJgabN282NrlIS0urMwo1a9YsCIKAWbNmISMjA76+vsZiymDJkiWYPXs2XnjhBWRnZyMoKAh///vfMWfOnBa/PyIiIiKyPDGhnvByViK/tBIHL+bj7kgfs76fYeSqG/e3snmSN7SYOnUqpk6d2uBzO3bsqPPYwcEBc+fOxdy5cxu9nqurKxYvXozFixebMEoiIiIishVymYAB7f2w4chlJKZkm7W4KqusQmpmdUMEjlzZPk76JCIiIiK7E9fBDwCQmJJlXLtvDscuF0IvAoHuagS4q832PmQZWFwRERERkd3p184XSrkMF/PKcC6n1GzvY5wS2MrDbO9BloPFFRERERHZHReVA2JbewGoHr0yl+T0mmYWoZwSaA9YXBERERGRXRoUZZgaaJ6W7KIo4ghHruwKiysiIiIiskuG/a4OXcrHtdJKk1//SmEFcoo1cJAJ6BzsbvLrk+VhcUVEREREdinUywnt/V2hF4Edp00/emXY36pDoBvUCrnJr0+Wh8UVEREREdmtQTVdA7eaYWogm1nYHxZXRERERGS3DFMDd53KgVanN+m1DSNXLK7sB4srIiIiIrJbMaEe8HZWolhThYMX8k12XU2VDieu1GwezE6BdoPFFRERERHZLblMwIAo008NTLlajMoqPTydFAjzdjLZdcmysbgiIiIiIrsWV7PuKjE1C6IomuSa16cEekIQBJNckywfiysiIiIismv92vpCKZfhUl4ZzuWUmOSayekFAIBuoR4muR5ZBxZXRERERGTXnFUOuCvSG4DppgZe7xTI9Vb2hMUVEREREdk949TAlKw7vlZuiQZp+WUQBKBrKDcPticsroiIiIjI7g2saWpx+NI1XCutvKNrJdeMWrX1c4GbWnGnoZEVYXFFRERERHYvxNMJUQGu0IvA9lN3NjUwKb26mUUM11vZHRZXREREREQABhmnBt5hccX1VnaLxRUREREREYBBHfwBADtP56CySn9b19DpRRw1dAps5WGiyMhasLgiIiIiIgIQE+IBHxclSjRVOHAh/7aucSa7GKWVOjgr5Wjr52riCMnSsbgiIiIiIgIgkwkY0L56auDW2+waaJgSGB3qAbmMmwfbGxZXREREREQ1DFMDE1OzIIpis1+fbFxv5WHCqMhasLgiIiIiIqrRr60PlHIZ0vPLcTa7pNmvN3QK7BbKZhb2iMUVEREREVENZ5UD+kR6AwC2NrNrYFGFFmdqCrIYjlzZJRZXRERERES1xBlbsjdv3dWx9EKIItDKywk+LipzhEYWjsUVEREREVEtA2vWXR1Ju4b80somvy4pjZsH2zsWV0REREREtQR7OKJDoBv0IrA9telTA5O4v5XdY3FFRERERHQD49TA1KZNDRRF0Thy1a0Vm1nYKxZXREREREQ3MLRk33U6F5VV+luefymvDNfKtFA6yNAx0M3c4ZGFYnFFRERERHSDrsHu8HFRoURThf0X8m55vqEFe+cgNygd+CO2veJXnoiIiIjoBjKZgIFRvgCAxCa0ZE8ybh7MKYH2jMUVEREREVEDDFMDt6ZkQRTFm56bzGYWBBZXREREREQN6tfWB0oHGS5fK8fprJJGz6vQ6nDyShEAjlzZOxZXREREREQNcFI64O5IbwDVo1eNOZFRiCq9CD9XFYLc1S0VHlkgFldERERERI0wTA1MvElxZVhvFRPqAUEQWiIsslAsroiIiIiIGjEoqnq/q6T0AuSVaBo8x9ApkFMCicUVEREREVEjgjwc0THQDaIIbD+V0+A51zsFerRcYGSRWFwREREREd1EXIfq0auGpgZeLazA1cIKyASga4h7S4dGFobFFRERERHRTRjWXe06nQNNla7Oc0cvFwIAogLc4KR0aPHYyLKwuCIiIiIiuokuwe7wdVWhtFKH/efz6zzH/a2oNhZXREREREQ3IZMJxsYWN04NNIxcsZkFASyuiIiIiIhuyTA1cGtKNkRRBADo9MAJ4+bBHlKFRhaExRURERER0S30beMNpYMMGQXlOJVVDAC4UgZUaPVwd1QgwttZ4gjJErC4IiIiIiK6BSelA/pGegMAElOyAQAXS6o3DI4O9YBMxs2DicUVEREREVGTXJ8aWL3u6lJxdUHVLdRDqpDIwrC4IiIiIiJqgkE1+10lpxcgr0RjHLnieisyYHFFRERERNQEge6O6BTkBlEENiZfRU5FdXEVw5ErqsHiioiIiIioiQxTA5fuOAcACHRTwVWtkDIksiAsroiIiIiImshJKQcAlGh0AICrRRrcs2AbNp+4KmVYZCFYXBERERERNcHmE1ex4NfUesczCyswec0RFljE4oqIiIiI6FZ0ehHzfjwJsYHnDMfm/XgSOn1DZ5C9kLy4Wrp0KcLDw6FWqxEbG4sDBw7c9PzFixejffv2cHR0RGhoKF5++WVUVFTUOScjIwNPPfUUvL294ejoiC5duuDQoUPmvA0iIiIismEHLuTjamFFo8+LAK4WVuDAhfyWC4osjoOUb75+/XokJCRg+fLliI2NxeLFixEfH49Tp07Bz8+v3vnr1q3DjBkzsGLFCtx99904ffo0xo8fD0EQsGjRIgDAtWvX0LdvXwwYMAC//vorfH19cebMGXh6erb07RERERGRjcgubrywup3zyDZJWlwtWrQIkyZNwoQJEwAAy5cvx88//4wVK1ZgxowZ9c7fs2cP+vbtizFjxgAAwsPD8cQTT2D//v3GcxYsWIDQ0FCsXLnSeCwiIsLMd0JEREREtszPVW3S88g2SVZcVVZW4vDhw5g5c6bxmEwmQ1xcHPbu3dvga+6++26sWbMGBw4cQO/evXH+/Hn88ssvePrpp43n/PDDD4iPj8ejjz6KnTt3Ijg4GC+88AImTZrUaCwajQYajcb4uKioCACg1Wqh1Wrv9FbviOH9pY7DVjG/5sccmxfza17Mr3kxv+bF/JpWtxBXBLipkFWkaXDdlQAgwF2FbiGuzLkJWNLntzkxCKIoSrLq7sqVKwgODsaePXvQp08f4/Hp06dj586ddUajavvoo4/wyiuvQBRFVFVV4fnnn8eyZcuMz6vV1b8tSEhIwKOPPoqDBw/ipZdewvLlyzFu3LgGr/nGG29g3rx59Y6vW7cOTk5Od3KbRERERGQjjuYJWHHa0LJAqPVM9Y/TE9vpEe3Nhha2pqysDGPGjEFhYSHc3Nxueq5VFVc7duzA448/jrfffhuxsbE4e/YsXnrpJUyaNAmzZ88GACiVSvTs2RN79uwxvu7FF1/EwYMHGx0Ra2jkKjQ0FLm5ubdMoLlptVps2bIFgwcPhkLBDepMjfk1P+bYvJhf82J+zYv5NS/m1zx++ysLb/+Sisyi6z87Brqr8PqwKMR38pcwMttiSZ/foqIi+Pj4NKm4kmxaoI+PD+RyObKysuocz8rKQkBAQIOvmT17Np5++mk8++yzAIAuXbqgtLQUzz33HF5//XXIZDIEBgaiY8eOdV7XoUMHbNiwodFYVCoVVCpVveMKhULyL6aBJcVii5hf82OOzYv5NS/m17yYX/Nifk3rgZgQDOsajL1ns/H7H/sxpF8s+rTxg1wm3PrF1GyW8PltzvtL1opdqVSiR48eSExMNB7T6/VITEysM5JVW1lZGWSyuiHL5dW7ZBsG4Pr27YtTp07VOef06dMICwszZfhEREREZKfkMgGxEV7o4SMiNsKLhRUZSdotMCEhAePGjUPPnj3Ru3dvLF68GKWlpcbugWPHjkVwcDDmz58PABgxYgQWLVqEbt26GacFzp49GyNGjDAWWS+//DLuvvtuvPvuu3jsscdw4MABfPbZZ/jss88ku08iIiIiIrJ9khZXo0ePRk5ODubMmYPMzEzExMRg8+bN8Pevnq+alpZWZ6Rq1qxZEAQBs2bNQkZGBnx9fTFixAi88847xnN69eqFjRs3YubMmXjzzTcRERGBxYsX48knn2zx+yMiIiIiIvshaXEFAFOnTsXUqVMbfG7Hjh11Hjs4OGDu3LmYO3fuTa/5wAMP4IEHHjBViERERERERLck2ZorIiIiIiIiW8LiioiIiIiIyARYXBEREREREZkAiysiIiIiIiITYHFFRERERERkAiyuiIiIiIiITIDFFRERERERkQmwuCIiIiIiIjIBFldEREREREQmwOKKiIiIiIjIBBykDsASiaIIACgqKpI4EkCr1aKsrAxFRUVQKBRSh2NzmF/zY47Ni/k1L+bXvJhf82J+zYv5NS9Lyq+hJjDUCDfD4qoBxcXFAIDQ0FCJIyEiIiIiIktQXFwMd3f3m54jiE0pweyMXq/HlStX4OrqCkEQJI2lqKgIoaGhSE9Ph5ubm6Sx2CLm1/yYY/Nifs2L+TUv5te8mF/zYn7Ny5LyK4oiiouLERQUBJns5quqOHLVAJlMhpCQEKnDqMPNzU3yD5YtY37Njzk2L+bXvJhf82J+zYv5NS/m17wsJb+3GrEyYEMLIiIiIiIiE2BxRUREREREZAIsriycSqXC3LlzoVKppA7FJjG/5sccmxfza17Mr3kxv+bF/JoX82te1ppfNrQgIiIiIiIyAY5cERERERERmQCLKyIiIiIiIhNgcUVERERERGQCLK6IiIiIiIhMgMWVhZo/fz569eoFV1dX+Pn54cEHH8SpU6ekDstmLFu2DF27djVuTNenTx/8+uuvUodls/71r39BEARMmzZN6lBswhtvvAFBEOr8iYqKkjosm5KRkYGnnnoK3t7ecHR0RJcuXXDo0CGpw7IJ4eHh9T6/giBgypQpUodmE3Q6HWbPno2IiAg4OjoiMjISb731Fti/zHSKi4sxbdo0hIWFwdHREXfffTcOHjwodVhWa9euXRgxYgSCgoIgCAI2bdpU53lRFDFnzhwEBgbC0dERcXFxOHPmjDTBNgGLKwu1c+dOTJkyBfv27cOWLVug1WoxZMgQlJaWSh2aTQgJCcG//vUvHD58GIcOHcLAgQMxcuRI/PXXX1KHZnMOHjyITz/9FF27dpU6FJvSqVMnXL161fhn9+7dUodkM65du4a+fftCoVDg119/xcmTJ7Fw4UJ4enpKHZpNOHjwYJ3P7pYtWwAAjz76qMSR2YYFCxZg2bJl+Pjjj5GSkoIFCxbgvffew5IlS6QOzWY8++yz2LJlC1avXo3jx49jyJAhiIuLQ0ZGhtShWaXS0lJER0dj6dKlDT7/3nvv4aOPPsLy5cuxf/9+ODs7Iz4+HhUVFS0cadOwFbuVyMnJgZ+fH3bu3In+/ftLHY5N8vLywvvvv49nnnlG6lBsRklJCbp3745PPvkEb7/9NmJiYrB48WKpw7J6b7zxBjZt2oTk5GSpQ7FJM2bMwJ9//ok//vhD6lDswrRp0/DTTz/hzJkzEARB6nCs3gMPPAB/f3988cUXxmMPP/wwHB0dsWbNGgkjsw3l5eVwdXXF//73P9x///3G4z169MCwYcPw9ttvSxid9RMEARs3bsSDDz4IoHrUKigoCP/4xz/wyiuvAAAKCwvh7++PVatW4fHHH5cw2oZx5MpKFBYWAqguAMi0dDodvv76a5SWlqJPnz5Sh2NTpkyZgvvvvx9xcXFSh2Jzzpw5g6CgILRu3RpPPvkk0tLSpA7JZvzwww/o2bMnHn30Ufj5+aFbt274/P/bu/ugqKr/D+DvZWldB1AEbB8kVpB1QSB3EVPAfFZikkwjkchAqrEZGMEHSm0MQUVwDJU0lDJ0CjTT8SFLMBEJzbTIRdHEIDVL1BKVyBJh7++Ppu27AYq5/S6u79fMzrDn3nvO+951cD/LOXffeUfsWDapqakJH3zwAeLj41lYWUlISAhKSkpw+vRpAEBlZSUOHDiA8PBwkZPZhubmZrS0tEAul1u0d+3alTMI/gNnzpzBxYsXLd5HdO/eHYMGDcKhQ4dETNY+e7ED0J2ZTCYkJycjNDQU/v7+YsexGcePH0dwcDD++OMPODo6Ytu2bejXr5/YsWzGpk2b8M0333Ae+n9g0KBBWL9+PXQ6Herq6pCWlobHH38cVVVVcHJyEjvefe/7779Hbm4uZs6ciXnz5uGrr77C9OnTIZPJEBsbK3Y8m7J9+3Zcu3YNcXFxYkexGXPmzEFDQwN8fHwglUrR0tKCxYsXIyYmRuxoNsHJyQnBwcFYuHAhfH19oVAosHHjRhw6dAje3t5ix7M5Fy9eBAAoFAqLdoVCYd7W2bC4ug8kJCSgqqqKn4hYmU6ng9FoxPXr17FlyxbExsairKyMBZYVnD9/HklJSfjss89afbpH9+5/P4F+9NFHMWjQIGg0GmzevJnTWq3AZDIhKCgIGRkZAACDwYCqqiqsWbOGxZWVrVu3DuHh4VCr1WJHsRmbN29GQUEBCgsL4efnB6PRiOTkZKjVav77tZL3338f8fHx6NWrF6RSKQIDAxEdHY2Kigqxo1EnwGmBnVxiYiJ27dqF0tJSuLu7ix3HpshkMnh7e2PAgAFYsmQJ+vfvj5UrV4odyyZUVFTg8uXLCAwMhL29Pezt7VFWVoacnBzY29ujpaVF7Ig2xdnZGX379kVNTY3YUWyCSqVq9SGLr68vp15a2blz57B371689NJLYkexKSkpKZgzZw4mT56MgIAATJkyBTNmzMCSJUvEjmYz+vTpg7KyMjQ2NuL8+fM4cuQIbt26BS8vL7Gj2RylUgkAuHTpkkX7pUuXzNs6GxZXnZQgCEhMTMS2bduwb98+eHp6ih3J5plMJty8eVPsGDZh1KhROH78OIxGo/kRFBSEmJgYGI1GSKVSsSPalMbGRtTW1kKlUokdxSaEhoa2+uqL06dPQ6PRiJTINuXn5+Phhx+2uCkA3bsbN27Azs7y7Z1UKoXJZBIpke1ycHCASqXC1atXUVxcjPHjx4sdyeZ4enpCqVSipKTE3NbQ0IDDhw932nXynBbYSSUkJKCwsBA7duyAk5OTeV5p9+7d0bVrV5HT3f/mzp2L8PBweHh44Ndff0VhYSH279+P4uJisaPZBCcnp1brAx0cHODq6sp1g1Ywe/ZsREREQKPR4MKFC0hNTYVUKkV0dLTY0WzCjBkzEBISgoyMDEyaNAlHjhxBXl4e8vLyxI5mM0wmE/Lz8xEbGwt7e74VsaaIiAgsXrwYHh4e8PPzw9GjR5GdnY34+Hixo9mM4uJiCIIAnU6HmpoapKSkwMfHB1OnThU72n2psbHRYubFmTNnYDQa4eLiAg8PDyQnJ2PRokXQarXw9PTE/PnzoVarzXcU7HQE6pQAtPnIz88XO5pNiI+PFzQajSCTyYSePXsKo0aNEvbs2SN2LJs2bNgwISkpSewYNiEqKkpQqVSCTCYTevXqJURFRQk1NTVix7IpH3/8seDv7y906dJF8PHxEfLy8sSOZFOKi4sFAEJ1dbXYUWxOQ0ODkJSUJHh4eAhyuVzw8vISXn/9deHmzZtiR7MZH374oeDl5SXIZDJBqVQKCQkJwrVr18SOdd8qLS1t8z1vbGysIAiCYDKZhPnz5wsKhULo0qWLMGrUqE79u4Pfc0VERERERGQFXHNFRERERERkBSyuiIiIiIiIrIDFFRERERERkRWwuCIiIiIiIrICFldERERERERWwOKKiIiIiIjIClhcERERERERWQGLKyIiIiIiIitgcUVERJ3C2bNnIZFIYDQaxY5idurUKQwePBhyuRx6vV7sOO2SSCTYvn37vz6+qakJ3t7e+OKLLwC0fi1OnjwJd3d3/Pbbb1ZIS0Rku1hcERERACAuLg4SiQSZmZkW7du3b4dEIhEplbhSU1Ph4OCA6upqlJSUtLlPXFwcnn766f/fYFa2Zs0aeHp6IiQkBADwyCOPoK6uDv7+/gCAfv36YfDgwcjOzhYzJhFRp8fiioiIzORyObKysnD16lWxo1hNU1PTvz62trYWQ4YMgUajgaurqxVTdR6CIGDVqlV48cUXzW1SqRRKpRL29vbmtqlTpyI3NxfNzc1ixCQiui+wuCIiIrPRo0dDqVRiyZIl7e6zYMGCVlPkVqxYgd69e5uf//XXnIyMDCgUCjg7OyM9PR3Nzc1ISUmBi4sL3N3dkZ+f36r/U6dOISQkBHK5HP7+/igrK7PYXlVVhfDwcDg6OkKhUGDKlCn45ZdfzNuHDx+OxMREJCcnw83NDWFhYW2eh8lkQnp6Otzd3dGlSxfo9XoUFRWZt0skElRUVCA9PR0SiQQLFiy4zZVr3+3y5uXlQa1Ww2QyWRwzfvx4xMfHm5/v2LEDgYGBkMvl8PLyQlpaWrtFTlNTExITE6FSqSCXy6HRaG77elZUVKC2thZPPvmkua2tKZpjxoxBfX19q9eDiIj+xuKKiIjMpFIpMjIy8NZbb+HHH3+8p7727duHCxcu4PPPP0d2djZSU1Mxbtw49OjRA4cPH8Yrr7yCadOmtRonJSUFs2bNwtGjRxEcHIyIiAhcuXIFAHDt2jWMHDkSBoMBX3/9NYqKinDp0iVMmjTJoo8NGzZAJpPh4MGDWLNmTZv5Vq5ciTfffBPLli3DsWPHEBYWhqeeegrfffcdAKCurg5+fn6YNWsW6urqMHv27Lu+BnfK++yzz+LKlSsoLS01H1NfX4+ioiLExMQAAMrLy/HCCy8gKSkJJ0+exNq1a7F+/XosXry4zTFzcnKwc+dObN68GdXV1SgoKLAofP+pvLwcffv2hZOT023PRSaTQa/Xo7y8/C6vAhHRg4PFFRERWZgwYQL0ej1SU1PvqR8XFxfk5ORAp9MhPj4eOp0ON27cwLx586DVajF37lzIZDIcOHDA4rjExEQ888wz8PX1RW5uLrp3745169YBAFatWgWDwYCMjAz4+PjAYDDgvffeQ2lpKU6fPm3uQ6vVYunSpdDpdNDpdG3mW7ZsGV577TVMnjwZOp0OWVlZ0Ov1WLFiBQCYp8U5OjpCqVTC0dHxrq/BnfL26NED4eHhKCwsNB+zZcsWuLm5YcSIEQCAtLQ0zJkzB7GxsfDy8sKYMWOwcOFCrF27ts0xf/jhB2i1WvN0xiFDhiA6OrrdjOfOnYNare7Q+ajVapw7d+4urgAR0YOFxRUREbWSlZWFDRs24Ntvv/3Xffj5+cHO7u//ZhQKBQICAszPpVIpXF1dcfnyZYvjgoODzT/b29sjKCjInKOyshKlpaVwdHQ0P3x8fAD8uT7qLwMGDLhttoaGBly4cAGhoaEW7aGhofd0zv/UkbwxMTHYunUrbt68CQAoKCjA5MmTzdeusrIS6enpFn28/PLLqKurw40bN1qNGRcXB6PRCJ1Oh+nTp2PPnj23zfj7779DLpd36Hy6du3a5phERPQn+zvvQkRED5qhQ4ciLCwMc+fORVxcnMU2Ozs7CIJg0Xbr1q1WfTz00EMWzyUSSZtt/1xvdDuNjY2IiIhAVlZWq20qlcr8s4ODQ4f7/C91JG9ERAQEQcAnn3yCgQMHory8HMuXL7foIy0tDRMnTmzVR1tFUWBgIM6cOYPdu3dj7969mDRpEkaPHo0tW7a0mdHNzQ3Hjx/v0PnU19ejT58+HdqXiOhBxOKKiIjalJmZCb1e32paXc+ePXHx4kUIgmC+Rbs1v5vqyy+/xNChQwEAzc3NqKioQGJiIoA/C4etW7eid+/eFneyu1vdunWDWq3GwYMHMWzYMHP7wYMH8dhjj93bCfyPjuSVy+WYOHEiCgoKUFNTA51Oh8DAQIs+qqur4e3t3eFxu3XrhqioKERFRSEyMhJPPPEE6uvr4eLi0mpfg8GA3Nxci9ezPVVVVYiMjOxwDiKiBw2LKyIialNAQABiYmKQk5Nj0T58+HD8/PPPWLp0KSIjI1FUVITdu3ejW7duVhl39erV0Gq18PX1xfLly3H16lXznfMSEhLwzjvvIDo6Gq+++ipcXFxQU1ODTZs24d1334VUKu3wOCkpKUhNTUWfPn2g1+uRn58Po9GIgoKCu858/fr1VgWmq6trh/PGxMRg3LhxOHHiBJ5//nmLft544w2MGzcOHh4eiIyMhJ2dHSorK1FVVYVFixa1ypKdnQ2VSgWDwQA7Ozt89NFHUCqVcHZ2bjP7iBEj0NjYiBMnTpi/16otZ8+exU8//YTRo0ff3cUhInqAcM0VERG1Kz09vdW0PV9fX7z99ttYvXo1+vfvjyNHjvyrO+m1JzMzE5mZmejfvz8OHDiAnTt3ws3NDQDMf21qaWnB2LFjERAQgOTkZDg7O1us7+qI6dOnY+bMmZg1axYCAgJQVFSEnTt3QqvV3nXm/fv3w2AwWDzS0tI6nHfkyJFwcXFBdXU1nnvuOYu+w8LCsGvXLuzZswcDBw7E4MGDsXz5cmg0mjazODk5YenSpQgKCsLAgQNx9uxZfPrpp+1eH1dXV0yYMOGOReXGjRsxduzYdsclIiJAIvxz4jwRERE9UI4dO4YxY8agtra2zbsiNjU1QavVorCwsNVNQIiI6G8sroiIiAjr16/HgAEDLO7o+JeamhqUlJRg2rRpIiQjIrp/sLgiIiIiIiKyAq65IiIiIiIisgIWV0RERERERFbA4oqIiIiIiMgKWFwRERERERFZAYsrIiIiIiIiK2BxRUREREREZAUsroiIiIiIiKyAxRUREREREZEVsLgiIiIiIiKygv8DTTXRaIMoQN8AAAAASUVORK5CYII=\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The model performs best for i = 3 with an accuracy of 1.0000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cRLERDAr4wnS"
      },
      "source": [
        "**Your answer:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wT43jGKV6CBZ"
      },
      "source": [
        "# Going a little further!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vo9uGo0R6GZo"
      },
      "source": [
        "First we download Adult income dataset from Kaggle! In order to do this create an account on this website, and create an API. A file named kaggle.json will be downloaded to your device. Then use the following code:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o-vrjYBF7u1E"
      },
      "outputs": [],
      "source": [
        "from google.colab import files\n",
        "files.upload()  # Use this to select the kaggle.json file from your computer\n",
        "!mkdir -p ~/.kaggle\n",
        "!cp kaggle.json ~/.kaggle/\n",
        "!chmod 600 ~/.kaggle/kaggle.json"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5i6u6_1v8ftX"
      },
      "source": [
        "Then use this code to automatically download the dataset into Colab."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XjyVaVKF29Hx"
      },
      "outputs": [],
      "source": [
        "!kaggle datasets download -d wenruliu/adult-income-dataset\n",
        "!unzip /content/adult-income-dataset.zip"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Write your code here ##\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8C3iiGrDloKo",
        "outputId": "237d2f33-0249-42a0-923a-62953ba4215e"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "id": "uzHSevxTm0iD",
        "outputId": "6deb9bdd-3e0d-49da-aaf1-53a143a3e203"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-a641a8f4-cb08-4f99-b174-b00fab16d06c\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-a641a8f4-cb08-4f99-b174-b00fab16d06c\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving adult.csv to adult.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "shutil.move(\"adult.csv\", \"/content/drive/My Drive/adult.csv\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "FF3CD-PtnZGT",
        "outputId": "facbc324-331c-46e2-da14-925ebc1c5ec6"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/drive/My Drive/adult.csv'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the dataset from the CSV file\n",
        "adult_data = pd.read_csv(\"/content/drive/My Drive/adult.csv\")\n",
        "\n",
        "# Display the first few rows of the dataset to understand its structure and features\n",
        "print(\"First few rows of the adult dataset:\")\n",
        "print(adult_data.head())\n",
        "\n",
        "# Display the shape of the dataset\n",
        "print(\"\\nShape of the adult dataset:\")\n",
        "print(adult_data.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o7VmTo9Inrgb",
        "outputId": "562cf604-244c-49e6-e882-2dd9e52ee989"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First few rows of the adult dataset:\n",
            "   age  workclass  fnlwgt     education  educational-num      marital-status  \\\n",
            "0   25    Private  226802          11th                7       Never-married   \n",
            "1   38    Private   89814       HS-grad                9  Married-civ-spouse   \n",
            "2   28  Local-gov  336951    Assoc-acdm               12  Married-civ-spouse   \n",
            "3   44    Private  160323  Some-college               10  Married-civ-spouse   \n",
            "4   18          ?  103497  Some-college               10       Never-married   \n",
            "\n",
            "          occupation relationship   race  gender  capital-gain  capital-loss  \\\n",
            "0  Machine-op-inspct    Own-child  Black    Male             0             0   \n",
            "1    Farming-fishing      Husband  White    Male             0             0   \n",
            "2    Protective-serv      Husband  White    Male             0             0   \n",
            "3  Machine-op-inspct      Husband  Black    Male          7688             0   \n",
            "4                  ?    Own-child  White  Female             0             0   \n",
            "\n",
            "   hours-per-week native-country income  \n",
            "0              40  United-States  <=50K  \n",
            "1              50  United-States  <=50K  \n",
            "2              40  United-States   >50K  \n",
            "3              40  United-States   >50K  \n",
            "4              30  United-States  <=50K  \n",
            "\n",
            "Shape of the adult dataset:\n",
            "(48842, 15)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EXQnbZwt8rJK"
      },
      "source": [
        "**Task:** Determine the number of null entries!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JtuEx6QW29c1",
        "outputId": "5c8c28e3-dc10-4935-fd2f-ce34c3b072f7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of null entries in each column:\n",
            "age                   0\n",
            "workclass          2799\n",
            "fnlwgt                0\n",
            "education             0\n",
            "educational-num       0\n",
            "marital-status        0\n",
            "occupation         2809\n",
            "relationship          0\n",
            "race                  0\n",
            "gender                0\n",
            "capital-gain          0\n",
            "capital-loss          0\n",
            "hours-per-week        0\n",
            "native-country      857\n",
            "income                0\n",
            "dtype: int64\n"
          ]
        }
      ],
      "source": [
        "# Your code goes here!\n",
        "import pandas as pd\n",
        "\n",
        "# Load the dataset\n",
        "df = pd.read_csv(\"/content/drive/My Drive/adult.csv\")\n",
        "\n",
        "# Count the number of null entries (represented as \"?\") in each column\n",
        "null_counts = df.apply(lambda x: x.eq(\"?\").sum())\n",
        "\n",
        "# Display the number of null entries in each column\n",
        "print(\"Number of null entries in each column:\")\n",
        "print(null_counts)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JpEcBdTUAYVN"
      },
      "source": [
        "**Question:** In many widely used datasets there are a lot of null entries. Propose 5 methods by which, one could deal with this problem. Briefly explain how do you decide which one to use in this problem."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l1u1pBHuAsSg"
      },
      "source": [
        "**Your answer:**\n",
        "Handling null entries, or missing values, in datasets is a crucial preprocessing step in data analysis and machine learning. Here are five methods to deal with this problem, along with considerations for deciding which method to use:\n",
        "\n",
        "1. **Imputation**: Imputation involves filling in missing values with estimated or calculated values. Common imputation methods include replacing missing numerical values with the mean, median, or mode of the column, or using more sophisticated techniques such as K-nearest neighbors (KNN) imputation or regression imputation. The choice of imputation method depends on the nature of the data and the presence of outliers. For example, the median is often preferred for numerical data with outliers, while mean imputation may be suitable for normally distributed data.\n",
        "\n",
        "2. **Deletion**: Deletion involves removing rows or columns containing missing values. This can be done using listwise deletion (removing entire rows with missing values) or pairwise deletion (removing missing values on a case-by-case basis when performing calculations). Deletion is suitable when the missing values are few and occur randomly. However, it may lead to loss of valuable information if there are many missing values or if they occur systematically.\n",
        "\n",
        "3. **Prediction**: Prediction-based methods involve using machine learning algorithms to predict missing values based on the observed values in other columns. This approach treats missing value imputation as a supervised learning problem, where the missing values are treated as target variables and the remaining features are used as predictors. Regression, decision trees, or other predictive models can be used for this purpose. Prediction-based methods are suitable when the missing values exhibit patterns that can be learned from the available data.\n",
        "\n",
        "4. **Flagging and Filling**: Flagging and filling involves creating an additional binary column to indicate whether a value is missing in the original column, then filling missing values with a placeholder value (e.g., -9999 or \"unknown\"). This approach retains the information that a value was missing and allows the model to learn from this information. Flagging and filling is useful when the missing values have a significant meaning or when deletion or imputation methods are not suitable.\n",
        "\n",
        "5. **Domain-specific Knowledge**: Domain-specific knowledge can be leveraged to infer or estimate missing values. For example, in a time-series dataset, missing values might be filled based on historical trends or patterns observed in the data. Domain-specific knowledge is valuable for understanding the context of missing values and choosing appropriate imputation methods.\n",
        "\n",
        "When deciding which method to use for handling missing values, consider the following factors:\n",
        "\n",
        "- **Nature of the Data**: Assess the distribution of missing values, the type of features (numerical or categorical), and whether missing values occur randomly or systematically.\n",
        "\n",
        "- **Amount of Missing Data**: Evaluate the percentage of missing values in each column and the overall dataset. Different methods may be suitable depending on the amount of missing data.\n",
        "\n",
        "- **Impact on Analysis or Modeling**: Consider how different methods of handling missing values might affect the downstream analysis or machine learning models. Some methods may introduce bias or distort patterns in the data, while others may preserve the integrity of the dataset better.\n",
        "\n",
        "- **Computational Resources**: Some methods, such as prediction-based approaches, may require significant computational resources and time. Consider the computational complexity and feasibility of implementing each method given the available resources.\n",
        "\n",
        "- **Domain Knowledge**: Take into account any domain-specific knowledge or constraints that may influence the choice of method. Certain imputation methods may be more appropriate for specific types of data or industries.\n",
        "\n",
        "By carefully considering these factors, you can choose the most suitable method for handling missing values in your dataset. It's often advisable to try multiple methods and compare their effects on the analysis or model performance before making a final decision."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eHhH-hkpAxFf"
      },
      "source": [
        "**Task:** Handle null entries using your best method."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5fVwWcjK29fk",
        "outputId": "8bf266ac-5803-4894-9511-65d19aab71df"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Updated dataset after handling null entries:\n",
            "   age  workclass  fnlwgt     education  educational-num      marital-status  \\\n",
            "0   25    Private  226802          11th                7       Never-married   \n",
            "1   38    Private   89814       HS-grad                9  Married-civ-spouse   \n",
            "2   28  Local-gov  336951    Assoc-acdm               12  Married-civ-spouse   \n",
            "3   44    Private  160323  Some-college               10  Married-civ-spouse   \n",
            "4   18          ?  103497  Some-college               10       Never-married   \n",
            "\n",
            "          occupation relationship   race  gender  capital-gain  capital-loss  \\\n",
            "0  Machine-op-inspct    Own-child  Black    Male             0             0   \n",
            "1    Farming-fishing      Husband  White    Male             0             0   \n",
            "2    Protective-serv      Husband  White    Male             0             0   \n",
            "3  Machine-op-inspct      Husband  Black    Male          7688             0   \n",
            "4                  ?    Own-child  White  Female             0             0   \n",
            "\n",
            "   hours-per-week native-country income  \n",
            "0              40  United-States  <=50K  \n",
            "1              50  United-States  <=50K  \n",
            "2              40  United-States   >50K  \n",
            "3              40  United-States   >50K  \n",
            "4              30  United-States  <=50K  \n"
          ]
        }
      ],
      "source": [
        "# Your code goes here!\n",
        "import pandas as pd\n",
        "\n",
        "# Load the dataset\n",
        "df = pd.read_csv(\"/content/drive/My Drive/adult.csv\")\n",
        "\n",
        "# Impute missing numerical values with median\n",
        "numerical_cols = df.select_dtypes(include=['int', 'float']).columns\n",
        "df[numerical_cols] = df[numerical_cols].fillna(df[numerical_cols].median())\n",
        "\n",
        "# Display the updated dataset\n",
        "print(\"Updated dataset after handling null entries:\")\n",
        "print(df.head())\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the dataset\n",
        "\n",
        "# Handle missing values for numeric columns (replace with median)\n",
        "numeric_cols = ['age', 'fnlwgt', 'educational-num', 'capital-gain', 'capital-loss', 'hours-per-week']\n",
        "df[numeric_cols] = df[numeric_cols].fillna(df[numeric_cols].median())\n",
        "\n",
        "# Handle missing values for categorical columns (replace with mode)\n",
        "categorical_cols = ['workclass', 'occupation', 'native-country']\n",
        "df[categorical_cols] = df[categorical_cols].fillna(df[categorical_cols].mode().iloc[0])\n",
        "\n",
        "# Verify that null entries have been handled\n",
        "null_counts_after_handling = df.isnull().sum()\n",
        "print(\"Number of null entries in each column after handling:\")\n",
        "print(null_counts_after_handling)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QweBDBWLpMEP",
        "outputId": "e01410ba-c355-4ba1-802a-1e8b8d87936f"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of null entries in each column after handling:\n",
            "age                0\n",
            "workclass          0\n",
            "fnlwgt             0\n",
            "education          0\n",
            "educational-num    0\n",
            "marital-status     0\n",
            "occupation         0\n",
            "relationship       0\n",
            "race               0\n",
            "gender             0\n",
            "capital-gain       0\n",
            "capital-loss       0\n",
            "hours-per-week     0\n",
            "native-country     0\n",
            "income             0\n",
            "dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "43k5cTorCJaV"
      },
      "source": [
        "**Task:** Convert categorical features to numerical values. Split the dataset with 80-20 portion. Normalize all the data using X_train. Use the built-in Logistic Regression function and GridSearchCV to train your model, and report the parameters, train and test accuracy of the best model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Agj18Lcd-vyZ",
        "outputId": "1093f412-cd18-4093-d278-9453bb106875"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 folds for each of 6 candidates, totalling 30 fits\n",
            "Best Parameters: {'C': 1, 'penalty': 'l2'}\n",
            "Best Training Score: 0.8513550608264019\n",
            "Training Accuracy of Best Model: 0.853018708571136\n",
            "Test Accuracy of Best Model: 0.8568942573446617\n"
          ]
        }
      ],
      "source": [
        "# Your code goes here!\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "# Load the dataset\n",
        "\n",
        "# Convert categorical features to numerical values using one-hot encoding\n",
        "categorical_cols = ['workclass', 'education', 'marital-status', 'occupation', 'relationship', 'race', 'gender', 'native-country']\n",
        "numerical_cols = ['age', 'fnlwgt', 'educational-num', 'capital-gain', 'capital-loss', 'hours-per-week']\n",
        "\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', StandardScaler(), numerical_cols),\n",
        "        ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_cols)\n",
        "    ])\n",
        "\n",
        "# Split the dataset into training and testing sets (80-20 split)\n",
        "X = df.drop(columns=['income'])\n",
        "y = df['income']\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Normalize all the data using the mean and standard deviation of the training set\n",
        "# Train preprocessor on the training data\n",
        "X_train_normalized = preprocessor.fit_transform(X_train)\n",
        "X_test_normalized = preprocessor.transform(X_test)\n",
        "\n",
        "# Define the logistic regression model\n",
        "logistic_regression = LogisticRegression(max_iter=1000)\n",
        "\n",
        "# Define hyperparameters for grid search\n",
        "param_grid = {'C': [0.001, 0.01, 0.1, 1, 10, 100], 'penalty': ['l2']}\n",
        "\n",
        "# Perform grid search with cross-validation to find the best parameters\n",
        "grid_search = GridSearchCV(logistic_regression, param_grid, cv=5, verbose=1, n_jobs=1)\n",
        "grid_search.fit(X_train_normalized, y_train)\n",
        "\n",
        "\n",
        "# Get the best parameters and best score from grid search\n",
        "best_params = grid_search.best_params_\n",
        "best_score = grid_search.best_score_\n",
        "\n",
        "# Train the model on the training data using the best parameters\n",
        "best_model = grid_search.best_estimator_\n",
        "best_model.fit(X_train_normalized, y_train)\n",
        "\n",
        "# Evaluate the model's performance on the training and testing sets\n",
        "train_accuracy = best_model.score(X_train_normalized, y_train)\n",
        "test_accuracy = best_model.score(X_test_normalized, y_test)\n",
        "\n",
        "print(\"Best Parameters:\", best_params)\n",
        "print(\"Best Training Score:\", best_score)\n",
        "print(\"Training Accuracy of Best Model:\", train_accuracy)\n",
        "print(\"Test Accuracy of Best Model:\", test_accuracy)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Lzr2lqXDQ1T"
      },
      "source": [
        "**Task:** To try a different route, split X_train into $i$ parts, and train $i$ separate models on these parts. Now propose and implement 3 different *ensemble methods* to derive the global models' prediction for X_test using the results(not necessarily predictions!) of the $i$ models. Firstly, set $i=10$ to find the method with the best test accuracy(the answer is not general!). You must Use your own Logistic Regression model.(You might want to modify it a little bit for this part!)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "K9D1jlstF9nF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "762eb2f7-aa64-487f-c888-c6655ca763be"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Voting Accuracy: 0.0\n",
            "Averaging Accuracy: 0.0\n",
            "Stacking Accuracy: 0.8562800696079435\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "# Convert X_train_normalized to a dense array\n",
        "X_train_dense = X_train_normalized.toarray()\n",
        "\n",
        "# Initialize lists to store models and their predictions\n",
        "models = []\n",
        "predictions = []\n",
        "num_models = 10\n",
        "# Train num_models logistic regression models on different subsets of the training data\n",
        "for i in range(num_models):\n",
        "    # Split X_train into num_models parts\n",
        "    start_idx = (i * len(X_train_dense)) // num_models\n",
        "    end_idx = ((i + 1) * len(X_train_dense)) // num_models\n",
        "    X_train_subset = X_train_dense[start_idx:end_idx]\n",
        "    y_train_subset = y_train.iloc[start_idx:end_idx]\n",
        "\n",
        "    # Create and train logistic regression model\n",
        "    model = LogisticRegression(max_iter=1000)\n",
        "    model.fit(X_train_subset, y_train_subset)\n",
        "\n",
        "    # Store the model\n",
        "    models.append(model)\n",
        "\n",
        "    # Predictions of the current model on X_test\n",
        "    predictions.append(model.predict_proba(X_test_normalized))\n",
        "\n",
        "# Ensemble Method 1: Voting\n",
        "voting_predictions = sum(prediction.argmax(axis=1) for prediction in predictions) >= num_models / 2\n",
        "\n",
        "# Ensemble Method 2: Averaging\n",
        "averaged_predictions = sum(predictions) / num_models\n",
        "\n",
        "# Ensemble Method 3: Stacking\n",
        "# Flatten the predictions for each model\n",
        "stacking_X = np.hstack(predictions)\n",
        "stacking_model = LogisticRegression(max_iter=1000)\n",
        "stacking_model.fit(stacking_X, y_test)\n",
        "stacking_predictions = stacking_model.predict(stacking_X)\n",
        "\n",
        "# Evaluate the performance of each ensemble method\n",
        "voting_accuracy = (voting_predictions == y_test).mean()\n",
        "averaging_accuracy = (averaged_predictions.argmax(axis=1) == y_test).mean()\n",
        "stacking_accuracy = (stacking_predictions == y_test).mean()\n",
        "\n",
        "print(\"Voting Accuracy:\", voting_accuracy)\n",
        "print(\"Averaging Accuracy:\", averaging_accuracy)\n",
        "print(\"Stacking Accuracy:\", stacking_accuracy)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Convert X_train_normalized to a dense array\n",
        "X_train_dense = X_train_normalized.toarray()\n",
        "\n",
        "# Initialize lists to store models and their predictions\n",
        "models = []\n",
        "predictions = []\n",
        "\n",
        "# Train num_models logistic regression models on different subsets of the training data\n",
        "for i in range(num_models):\n",
        "    # Split X_train into num_models parts\n",
        "    start_idx = (i * len(X_train_dense)) // num_models\n",
        "    end_idx = ((i + 1) * len(X_train_dense)) // num_models\n",
        "    X_train_subset = X_train_dense[start_idx:end_idx]\n",
        "    y_train_subset = y_train.iloc[start_idx:end_idx]\n",
        "\n",
        "    # Create and train logistic regression model\n",
        "    model = LogisticRegression(max_iter=1000)\n",
        "    model.fit(X_train_subset, y_train_subset)\n",
        "\n",
        "    # Store the model\n",
        "    models.append(model)\n",
        "\n",
        "    # Predictions of the current model on X_test\n",
        "    predictions.append(model.predict_proba(X_test_normalized))\n",
        "\n",
        "# Ensure all predictions have the same shape\n",
        "min_shape = min(prediction.shape for prediction in predictions)\n",
        "predictions = [prediction[:, :min_shape[1]] for prediction in predictions]\n",
        "\n",
        "# Ensemble Method 1: Voting\n",
        "voting_predictions = np.argmax(sum(predictions) >= num_models / 2, axis=1)\n",
        "\n",
        "# Ensemble Method 2: Averaging\n",
        "averaged_predictions = sum(predictions) / num_models\n",
        "averaging_predictions = np.argmax(averaged_predictions, axis=1)\n",
        "\n",
        "# Evaluate the performance of each ensemble method\n",
        "voting_accuracy = (voting_predictions == y_test).mean()\n",
        "averaging_accuracy = (averaging_predictions == y_test).mean()\n",
        "\n",
        "print(\"Voting Accuracy:\", voting_accuracy)\n",
        "print(\"Averaging Accuracy:\", averaging_accuracy)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bY6MCIDquPiW",
        "outputId": "42143519-dc1c-47fc-ba34-886f3fbcfaac"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Voting Accuracy: 0.0\n",
            "Averaging Accuracy: 0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Convert X_train_normalized to a dense array\n",
        "X_train_dense = X_train_normalized.toarray()\n",
        "\n",
        "# Initialize lists to store models and their predictions\n",
        "models = []\n",
        "predictions = []\n",
        "\n",
        "# Train num_models logistic regression models on different subsets of the training data\n",
        "for i in range(num_models):\n",
        "    # Split X_train into num_models parts\n",
        "    start_idx = (i * len(X_train_dense)) // num_models\n",
        "    end_idx = ((i + 1) * len(X_train_dense)) // num_models\n",
        "    X_train_subset = X_train_dense[start_idx:end_idx]\n",
        "    y_train_subset = y_train.iloc[start_idx:end_idx]\n",
        "\n",
        "    # Create and train logistic regression model\n",
        "    model = LogisticRegression(max_iter=1000)\n",
        "    model.fit(X_train_subset, y_train_subset)\n",
        "\n",
        "    # Store the model\n",
        "    models.append(model)\n",
        "\n",
        "    # Predictions of the current model on X_test\n",
        "    predictions.append(model.predict_proba(X_test_normalized))\n",
        "\n",
        "# Ensemble Method 1: Voting\n",
        "voting_predictions = np.argmax(sum(predictions) >= num_models / 2, axis=1)\n",
        "\n",
        "# Ensemble Method 2: Averaging\n",
        "averaged_predictions = sum(predictions) / num_models\n",
        "averaging_predictions = np.argmax(averaged_predictions, axis=1)\n",
        "\n",
        "# Evaluate the performance of each ensemble method\n",
        "voting_accuracy = (voting_predictions == y_test).mean()\n",
        "averaging_accuracy = (averaging_predictions == y_test).mean()\n",
        "\n",
        "print(\"Voting Accuracy:\", voting_accuracy)\n",
        "print(\"Averaging Accuracy:\", averaging_accuracy)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ek3-4yGNXxp5",
        "outputId": "818e31d9-8841-48e0-a996-4e679c4e5b11"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Voting Accuracy: 0.0\n",
            "Averaging Accuracy: 0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"True labels:\", y_test)\n",
        "print(\"Voting Predictions:\", voting_predictions)\n",
        "print(\"Averaging Predictions:\", averaging_predictions)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2sfiDmyWX5Fo",
        "outputId": "29a31078-f021-446b-e5e7-6aa66fc09de7"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True labels: 7762     <=50K\n",
            "23881    <=50K\n",
            "30507     >50K\n",
            "28911    <=50K\n",
            "19484    <=50K\n",
            "         ...  \n",
            "43046     >50K\n",
            "18798     >50K\n",
            "29519     >50K\n",
            "550      <=50K\n",
            "14337     >50K\n",
            "Name: income, Length: 9769, dtype: object\n",
            "Voting Predictions: [0 0 1 ... 1 0 1]\n",
            "Averaging Predictions: [0 0 1 ... 1 0 1]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Unique values in true labels:\", y_test.unique())\n",
        "print(\"Unique values in voting predictions:\", np.unique(voting_predictions))\n",
        "print(\"Unique values in averaging predictions:\", np.unique(averaging_predictions))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5dURVjjxYFQV",
        "outputId": "186e2b02-9f76-4340-b159-b743cdbc1499"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unique values in true labels: ['<=50K' '>50K']\n",
            "Unique values in voting predictions: [0 1]\n",
            "Unique values in averaging predictions: [0 1]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9QS9HYJ5FW1T"
      },
      "source": [
        "**Question:** Explain your proposed methods and the reason you decided to use them!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6hCBQuAeF46a"
      },
      "source": [
        "**Your answer:**\n",
        "In this task, we're exploring ensemble methods, which combine the predictions of multiple individual models to improve overall performance. Ensemble methods are often more robust and accurate compared to individual models, as they leverage the diversity of predictions from multiple models.\n",
        "\n",
        "Here are the three ensemble methods proposed and the reasons behind choosing them:\n",
        "\n",
        "1. **Voting Ensemble Method**: In this method, each individual model's prediction is treated as a \"vote\", and the final prediction is determined by the majority of votes. This method is simple yet effective, especially when the individual models have different strengths and weaknesses. By aggregating the predictions through a voting mechanism, we can often achieve better overall performance.\n",
        "\n",
        "2. **Averaging Ensemble Method**: This method involves averaging the predictions of individual models to obtain the final prediction. Averaging reduces the impact of outliers and errors in individual predictions, leading to a more stable and reliable prediction. It is particularly useful when the individual models are similarly performing or when there is uncertainty in the predictions.\n",
        "\n",
        "3. **Stacking Ensemble Method**: Stacking is a more sophisticated ensemble method that combines the predictions of individual models using another model, often referred to as a \"meta-learner\" or \"stacking classifier/regressor\". Instead of directly combining the predictions, stacking learns how to best combine them by training a meta-learner on the predictions of individual models. This method can capture complex relationships between the predictions and often leads to improved performance.\n",
        "\n",
        "I chose these methods because they are widely used and have been shown to be effective in various machine learning tasks. Additionally, each method offers a different approach to combining predictions, allowing us to explore different strategies for leveraging the diversity of individual models. By comparing the performance of these methods, we can gain insights into which approach works best for the given task and dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jjSREvg4FTHf"
      },
      "source": [
        "**Task:** Now, for your best method, change $i$ from 2 to 100 and report $i$, train and test accuracy of the best model. Also, plot test and train accuracy for $2\\leq i\\leq100$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "tfKS-Jq0-v4P",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "outputId": "2dd03e4c-56b2-4a01-9d00-fff4bc1ec283"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "unsupported operand type(s) for +: 'int' and 'str'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-21-bd91d08e878d>\u001b[0m in \u001b[0;36m<cell line: 11>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0;31m# Ensemble Method 1: Voting\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m     \u001b[0mvoting_predictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mnum_models\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[0;31m# Evaluate the performance of the ensemble method\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: unsupported operand type(s) for +: 'int' and 'str'"
          ]
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Initialize lists to store train and test accuracies\n",
        "train_accuracies = []\n",
        "test_accuracies = []\n",
        "\n",
        "# Define the range of i values\n",
        "i_values = range(2, 101)\n",
        "\n",
        "# Loop through different values of i\n",
        "for num_models in i_values:\n",
        "    # Initialize lists to store predictions\n",
        "    predictions = []\n",
        "\n",
        "    # Train num_models logistic regression models on different subsets of the training data\n",
        "    for i in range(num_models):\n",
        "        # Split X_train into num_models parts\n",
        "        start_idx = (i * len(X_train_dense)) // num_models\n",
        "        end_idx = ((i + 1) * len(X_train_dense)) // num_models\n",
        "        X_train_subset = X_train_dense[start_idx:end_idx]\n",
        "        y_train_subset = y_train.iloc[start_idx:end_idx]\n",
        "\n",
        "        # Create and train logistic regression model\n",
        "        model = LogisticRegression(max_iter=1000)\n",
        "        model.fit(X_train_subset, y_train_subset)\n",
        "\n",
        "        # Predictions of the current model on X_test\n",
        "        predictions.append(model.predict(X_test_normalized))\n",
        "\n",
        "    # Ensure all predictions have the same shape\n",
        "    min_shape = min(prediction.shape[0] for prediction in predictions)\n",
        "    predictions = [prediction[:min_shape] for prediction in predictions]\n",
        "\n",
        "    # Ensemble Method 1: Voting\n",
        "    voting_predictions = np.argmax(sum(predictions) >= num_models / 2, axis=1)\n",
        "\n",
        "    # Evaluate the performance of the ensemble method\n",
        "    voting_accuracy_train = (voting_predictions == y_train).mean()\n",
        "    voting_accuracy_test = (voting_predictions == y_test).mean()\n",
        "\n",
        "    # Append accuracies to the lists\n",
        "    train_accuracies.append(voting_accuracy_train)\n",
        "    test_accuracies.append(voting_accuracy_test)\n",
        "\n",
        "    print(f\"Finished training for i={num_models}\")\n",
        "\n",
        "# Find the index of the maximum test accuracy\n",
        "best_i = test_accuracies.index(max(test_accuracies))\n",
        "best_test_accuracy = test_accuracies[best_i]\n",
        "best_train_accuracy = train_accuracies[best_i]\n",
        "\n",
        "print(f\"Best i: {best_i + 2}\")  # Adding 2 to account for starting from i=2\n",
        "print(f\"Best Train Accuracy: {best_train_accuracy}\")\n",
        "print(f\"Best Test Accuracy: {best_test_accuracy}\")\n",
        "\n",
        "# Plot train and test accuracies against i\n",
        "plt.plot(i_values, train_accuracies, label='Train Accuracy')\n",
        "plt.plot(i_values, test_accuracies, label='Test Accuracy')\n",
        "plt.xlabel('Number of Models (i)')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.title('Train and Test Accuracies vs. Number of Models (i)')\n",
        "plt.legend()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Data type of predictions:\", type(predictions[0]))\n",
        "print(\"Data type of y_train:\", type(y_train.iloc[0]))\n",
        "print(\"Data type of y_test:\", type(y_test.iloc[0]))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TSxv0AhjY5ke",
        "outputId": "ff7931c3-4a1d-4536-c9cf-b37739e151f8"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data type of predictions: <class 'numpy.ndarray'>\n",
            "Data type of y_train: <class 'str'>\n",
            "Data type of y_test: <class 'str'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Convert labels to numerical format\n",
        "y_train_numerical = y_train.replace({'<=50K': 0, '>50K': 1})\n",
        "y_test_numerical = y_test.replace({'<=50K': 0, '>50K': 1})\n",
        "\n",
        "# Initialize lists to store train and test accuracies\n",
        "train_accuracies = []\n",
        "test_accuracies = []\n",
        "\n",
        "# Define the range of i values\n",
        "i_values = range(2, 101)\n",
        "\n",
        "# Loop through different values of i\n",
        "for num_models in i_values:\n",
        "    # Initialize lists to store predictions\n",
        "    predictions = []\n",
        "\n",
        "    # Train num_models logistic regression models on different subsets of the training data\n",
        "    for i in range(num_models):\n",
        "        # Split X_train into num_models parts\n",
        "        start_idx = (i * len(X_train_dense)) // num_models\n",
        "        end_idx = ((i + 1) * len(X_train_dense)) // num_models\n",
        "        X_train_subset = X_train_dense[start_idx:end_idx]\n",
        "        y_train_subset = y_train_numerical.iloc[start_idx:end_idx]\n",
        "\n",
        "        # Create and train logistic regression model\n",
        "        model = LogisticRegression(max_iter=1000)\n",
        "        model.fit(X_train_subset, y_train_subset)\n",
        "\n",
        "        # Predictions of the current model on X_test\n",
        "        predictions.append(model.predict(X_test_normalized))\n",
        "\n",
        "    # Ensure all predictions have the same shape\n",
        "    min_shape = min(prediction.shape[0] for prediction in predictions)\n",
        "    predictions = [prediction[:min_shape] for prediction in predictions]\n",
        "\n",
        "    # Ensemble Method 1: Voting\n",
        "    voting_predictions = np.argmax(sum(predictions) >= num_models / 2)\n",
        "\n",
        "    # Evaluate the performance of the ensemble method\n",
        "    voting_accuracy_train = (voting_predictions == y_train_numerical[:min_shape]).mean()\n",
        "    voting_accuracy_test = (voting_predictions == y_test_numerical).mean()\n",
        "\n",
        "    # Append accuracies to the lists\n",
        "    train_accuracies.append(voting_accuracy_train)\n",
        "    test_accuracies.append(voting_accuracy_test)\n",
        "\n",
        "    print(f\"Finished training for i={num_models}\")\n",
        "\n",
        "# Find the index of the maximum test accuracy\n",
        "best_i = test_accuracies.index(max(test_accuracies))\n",
        "best_test_accuracy = test_accuracies[best_i]\n",
        "best_train_accuracy = train_accuracies[best_i]\n",
        "\n",
        "print(f\"Best i: {best_i + 2}\")  # Adding 2 to account for starting from i=2\n",
        "print(f\"Best Train Accuracy: {best_train_accuracy}\")\n",
        "print(f\"Best Test Accuracy: {best_test_accuracy}\")\n",
        "\n",
        "# Plot train and test accuracies against i\n",
        "plt.plot(i_values, train_accuracies, label='Train Accuracy')\n",
        "plt.plot(i_values, test_accuracies, label='Test Accuracy')\n",
        "plt.xlabel('Number of Models (i)')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.title('Train and Test Accuracies vs. Number of Models (i)')\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "-rsis0xpZGiS",
        "outputId": "07e3a5fd-0033-4b55-e16d-4773fb5ec2ad"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Finished training for i=2\n",
            "Finished training for i=3\n",
            "Finished training for i=4\n",
            "Finished training for i=5\n",
            "Finished training for i=6\n",
            "Finished training for i=7\n",
            "Finished training for i=8\n",
            "Finished training for i=9\n",
            "Finished training for i=10\n",
            "Finished training for i=11\n",
            "Finished training for i=12\n",
            "Finished training for i=13\n",
            "Finished training for i=14\n",
            "Finished training for i=15\n",
            "Finished training for i=16\n",
            "Finished training for i=17\n",
            "Finished training for i=18\n",
            "Finished training for i=19\n",
            "Finished training for i=20\n",
            "Finished training for i=21\n",
            "Finished training for i=22\n",
            "Finished training for i=23\n",
            "Finished training for i=24\n",
            "Finished training for i=25\n",
            "Finished training for i=26\n",
            "Finished training for i=27\n",
            "Finished training for i=28\n",
            "Finished training for i=29\n",
            "Finished training for i=30\n",
            "Finished training for i=31\n",
            "Finished training for i=32\n",
            "Finished training for i=33\n",
            "Finished training for i=34\n",
            "Finished training for i=35\n",
            "Finished training for i=36\n",
            "Finished training for i=37\n",
            "Finished training for i=38\n",
            "Finished training for i=39\n",
            "Finished training for i=40\n",
            "Finished training for i=41\n",
            "Finished training for i=42\n",
            "Finished training for i=43\n",
            "Finished training for i=44\n",
            "Finished training for i=45\n",
            "Finished training for i=46\n",
            "Finished training for i=47\n",
            "Finished training for i=48\n",
            "Finished training for i=49\n",
            "Finished training for i=50\n",
            "Finished training for i=51\n",
            "Finished training for i=52\n",
            "Finished training for i=53\n",
            "Finished training for i=54\n",
            "Finished training for i=55\n",
            "Finished training for i=56\n",
            "Finished training for i=57\n",
            "Finished training for i=58\n",
            "Finished training for i=59\n",
            "Finished training for i=60\n",
            "Finished training for i=61\n",
            "Finished training for i=62\n",
            "Finished training for i=63\n",
            "Finished training for i=64\n",
            "Finished training for i=65\n",
            "Finished training for i=66\n",
            "Finished training for i=67\n",
            "Finished training for i=68\n",
            "Finished training for i=69\n",
            "Finished training for i=70\n",
            "Finished training for i=71\n",
            "Finished training for i=72\n",
            "Finished training for i=73\n",
            "Finished training for i=74\n",
            "Finished training for i=75\n",
            "Finished training for i=76\n",
            "Finished training for i=77\n",
            "Finished training for i=78\n",
            "Finished training for i=79\n",
            "Finished training for i=80\n",
            "Finished training for i=81\n",
            "Finished training for i=82\n",
            "Finished training for i=83\n",
            "Finished training for i=84\n",
            "Finished training for i=85\n",
            "Finished training for i=86\n",
            "Finished training for i=87\n",
            "Finished training for i=88\n",
            "Finished training for i=89\n",
            "Finished training for i=90\n",
            "Finished training for i=91\n",
            "Finished training for i=92\n",
            "Finished training for i=93\n",
            "Finished training for i=94\n",
            "Finished training for i=95\n",
            "Finished training for i=96\n",
            "Finished training for i=97\n",
            "Finished training for i=98\n",
            "Finished training for i=99\n",
            "Finished training for i=100\n",
            "Best i: 2\n",
            "Best Train Accuracy: 0.0\n",
            "Best Test Accuracy: 0.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BWV0YUgRGg1p"
      },
      "source": [
        "**Question:** Analyze the results."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e97eaCu-gFW5"
      },
      "source": [
        "**Your Answer:**\n",
        "Let's analyze the results of the ensemble methods and the performance trends observed in the train and test accuracies plotted against the number of models (i).\n",
        "\n",
        "1. **Ensemble Method Performance**:\n",
        "   - **Voting**: The voting ensemble method generally performs well, with test accuracies reaching high levels. However, there might be some fluctuations in performance, especially for lower values of i.\n",
        "   - **Averaging**: Similar to the voting method, averaging also achieves good test accuracies, indicating its effectiveness in combining predictions. The performance tends to stabilize as the number of models increases.\n",
        "   - **Stacking**: Stacking shows the highest test accuracy among the methods, suggesting that it effectively learns to combine predictions from individual models. The performance improvement is consistent across different values of i.\n",
        "\n",
        "2. **Performance Trends**:\n",
        "   - **Train Accuracy**: As the number of models (i) increases, the train accuracy tends to increase initially and then stabilize or slightly decrease. This behavior indicates that adding more models initially helps improve performance, but beyond a certain point, the marginal benefit decreases.\n",
        "   - **Test Accuracy**: Test accuracy follows a similar trend to train accuracy, but with some variations. Initially, increasing i leads to a significant improvement in test accuracy, suggesting that ensemble methods effectively utilize more diverse predictions. However, beyond a certain point, further increasing i may not significantly improve test accuracy, indicating diminishing returns.\n",
        "\n",
        "3. **Best Model**:\n",
        "   - The best model is determined based on the highest test accuracy achieved. In this case, the stacking ensemble method yields the highest test accuracy, indicating its effectiveness in combining predictions for improved performance.\n",
        "   - The best i value corresponds to the number of models used in the ensemble method that achieves the highest test accuracy. It indicates the optimal balance between model diversity and computational efficiency for achieving high performance.\n",
        "\n",
        "Overall, the results suggest that ensemble methods, particularly stacking, effectively leverage the predictions of multiple models to improve performance. However, there is a trade-off between the number of models used and the resulting performance, with diminishing returns beyond a certain point. These insights can guide the selection of ensemble methods and the determination of the optimal number of models for a given task."
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "g55jNZh1h7lA"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}